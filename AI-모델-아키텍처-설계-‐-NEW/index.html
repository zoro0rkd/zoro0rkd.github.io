
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
        <link rel="canonical" href="https://zoro0rkd.github.io/AI-%EB%AA%A8%EB%8D%B8-%EC%95%84%ED%82%A4%ED%85%8D%EC%B2%98-%EC%84%A4%EA%B3%84-%E2%80%90-NEW/">
      
      
        <link rel="prev" href="../Index/">
      
      
        <link rel="next" href="../AI-%EB%AA%A8%EB%8D%B8-%ED%95%99%EC%8A%B5%EA%B3%BC-%ED%8F%89%EA%B0%80-%E2%80%90-NEW/">
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.17">
    
    
      
        <title>아키텍처 설계 - Sang-ik Notes</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.7e37652d.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#1" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="Sang-ik Notes" class="md-header__button md-logo" aria-label="Sang-ik Notes" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Sang-ik Notes
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              아키텍처 설계
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="Sang-ik Notes" class="md-nav__button md-logo" aria-label="Sang-ik Notes" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    Sang-ik Notes
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../Index/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    홈
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" checked>
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    AI 모델 개발
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            AI 모델 개발
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    아키텍처 설계
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    아키텍처 설계
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#1" class="md-nav__link">
    <span class="md-ellipsis">
      1. 모델 설계 개요
    </span>
  </a>
  
    <nav class="md-nav" aria-label="1. 모델 설계 개요">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#11-ai" class="md-nav__link">
    <span class="md-ellipsis">
      1.1 AI 모델 아키텍처 정의
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#12" class="md-nav__link">
    <span class="md-ellipsis">
      1.2 설계 시 고려 요소 (성능, 효율성, 확장성)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#13" class="md-nav__link">
    <span class="md-ellipsis">
      1.3 최신 동향 및 발전 방향
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2" class="md-nav__link">
    <span class="md-ellipsis">
      2. 주요 아키텍처 개념
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2. 주요 아키텍처 개념">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#21-residual-block-skip-connection" class="md-nav__link">
    <span class="md-ellipsis">
      2.1 Residual Block &amp; Skip Connection
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#22-transformer" class="md-nav__link">
    <span class="md-ellipsis">
      2.2 Transformer 기본 구조
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2.2 Transformer 기본 구조">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_1" class="md-nav__link">
    <span class="md-ellipsis">
      주요 구성 요소
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_2" class="md-nav__link">
    <span class="md-ellipsis">
      핵심 메커니즘
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_3" class="md-nav__link">
    <span class="md-ellipsis">
      전체 구조 요약
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_4" class="md-nav__link">
    <span class="md-ellipsis">
      참고
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#transformer" class="md-nav__link">
    <span class="md-ellipsis">
      Transformer
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#attention" class="md-nav__link">
    <span class="md-ellipsis">
      Attention 구조
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#transformer-family-model" class="md-nav__link">
    <span class="md-ellipsis">
      Transformer Family Model
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bert-family" class="md-nav__link">
    <span class="md-ellipsis">
      BERT Family
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#23-cnn" class="md-nav__link">
    <span class="md-ellipsis">
      2.3 CNN과 변형 구조
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#24" class="md-nav__link">
    <span class="md-ellipsis">
      2.4 순환 신경망 계열
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2.4 순환 신경망 계열">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#rnn-lstm-gru" class="md-nav__link">
    <span class="md-ellipsis">
      RNN / LSTM / GRU
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#25" class="md-nav__link">
    <span class="md-ellipsis">
      2.5 시공간 처리 구조
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2.5 시공간 처리 구조">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#3d-cnn-convlstm" class="md-nav__link">
    <span class="md-ellipsis">
      3D CNN, ConvLSTM
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3" class="md-nav__link">
    <span class="md-ellipsis">
      3. 자동화된 아키텍처 탐색
    </span>
  </a>
  
    <nav class="md-nav" aria-label="3. 자동화된 아키텍처 탐색">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#31-nas-neural-architecture-search" class="md-nav__link">
    <span class="md-ellipsis">
      3.1 NAS (Neural Architecture Search)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="3.1 NAS (Neural Architecture Search)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#nas" class="md-nav__link">
    <span class="md-ellipsis">
      NAS란 무엇인가?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#nas_1" class="md-nav__link">
    <span class="md-ellipsis">
      NAS의 핵심 개념과 작동 원리
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#nas_2" class="md-nav__link">
    <span class="md-ellipsis">
      NAS의 장점
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#nas_3" class="md-nav__link">
    <span class="md-ellipsis">
      NAS의 도전과제
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#nas-automl" class="md-nav__link">
    <span class="md-ellipsis">
      NAS와 AutoML의 차이
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#nas_4" class="md-nav__link">
    <span class="md-ellipsis">
      NAS의 미래와 활용
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_5" class="md-nav__link">
    <span class="md-ellipsis">
      요약
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#32-darts-differentiable-architecture-search" class="md-nav__link">
    <span class="md-ellipsis">
      3.2 DARTS (Differentiable Architecture Search)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="3.2 DARTS (Differentiable Architecture Search)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_6" class="md-nav__link">
    <span class="md-ellipsis">
      요약
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#4-llm" class="md-nav__link">
    <span class="md-ellipsis">
      4. 대형 언어 모델(LLM) 아키텍처
    </span>
  </a>
  
    <nav class="md-nav" aria-label="4. 대형 언어 모델(LLM) 아키텍처">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1_1" class="md-nav__link">
    <span class="md-ellipsis">
      1. 구조와 아키텍처
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2_1" class="md-nav__link">
    <span class="md-ellipsis">
      2. 학습 방식
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3_1" class="md-nav__link">
    <span class="md-ellipsis">
      3. 파라미터 효율적 튜닝
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4-tokenization" class="md-nav__link">
    <span class="md-ellipsis">
      4. 토큰화(Tokenization)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_7" class="md-nav__link">
    <span class="md-ellipsis">
      용어 요약
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#5" class="md-nav__link">
    <span class="md-ellipsis">
      5. 파인튜닝 전략
    </span>
  </a>
  
    <nav class="md-nav" aria-label="5. 파인튜닝 전략">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#51-lora-low-rank-adaptation" class="md-nav__link">
    <span class="md-ellipsis">
      5.1 LoRA (Low-Rank Adaptation)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#52-instruction-tuning" class="md-nav__link">
    <span class="md-ellipsis">
      5.2 Instruction Tuning
    </span>
  </a>
  
    <nav class="md-nav" aria-label="5.2 Instruction Tuning">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1_2" class="md-nav__link">
    <span class="md-ellipsis">
      1. 개념 및 목적
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2_2" class="md-nav__link">
    <span class="md-ellipsis">
      2. 기존 파인튜닝과의 차이점
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3_2" class="md-nav__link">
    <span class="md-ellipsis">
      3. 데이터셋 구성과 학습 방식
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4" class="md-nav__link">
    <span class="md-ellipsis">
      4. 효과와 특징
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#5_1" class="md-nav__link">
    <span class="md-ellipsis">
      5. 실제 적용 예시
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#6" class="md-nav__link">
    <span class="md-ellipsis">
      6. 한계와 과제
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_8" class="md-nav__link">
    <span class="md-ellipsis">
      요약
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#53-supervised-fine-tuning-vs-instruction-tuning" class="md-nav__link">
    <span class="md-ellipsis">
      5.3 Supervised Fine-Tuning vs Instruction Tuning
    </span>
  </a>
  
    <nav class="md-nav" aria-label="5.3 Supervised Fine-Tuning vs Instruction Tuning">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_9" class="md-nav__link">
    <span class="md-ellipsis">
      데이터 구조 차이
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_10" class="md-nav__link">
    <span class="md-ellipsis">
      목표 차이
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#6-self-supervised-learning" class="md-nav__link">
    <span class="md-ellipsis">
      6. 자기지도 학습(Self-Supervised Learning) 구조
    </span>
  </a>
  
    <nav class="md-nav" aria-label="6. 자기지도 학습(Self-Supervised Learning) 구조">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#simclr" class="md-nav__link">
    <span class="md-ellipsis">
      SimCLR
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#byol" class="md-nav__link">
    <span class="md-ellipsis">
      BYOL
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#jigsaw-puzzle" class="md-nav__link">
    <span class="md-ellipsis">
      jigsaw puzzle
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#maemasked-auto-encoder" class="md-nav__link">
    <span class="md-ellipsis">
      MAE(masked auto encoder)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#rotnet" class="md-nav__link">
    <span class="md-ellipsis">
      RotNet
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#7-unsupervised-learning" class="md-nav__link">
    <span class="md-ellipsis">
      7. 비지도 학습(Unsupervised Learning) 구조
    </span>
  </a>
  
    <nav class="md-nav" aria-label="7. 비지도 학습(Unsupervised Learning) 구조">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#unsupervised-learning" class="md-nav__link">
    <span class="md-ellipsis">
      비지도 학습(Unsupervised Learning) 개념
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_11" class="md-nav__link">
    <span class="md-ellipsis">
      비지도 학습의 대표적 예시
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_12" class="md-nav__link">
    <span class="md-ellipsis">
      실제 활용 사례
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_13" class="md-nav__link">
    <span class="md-ellipsis">
      비지도 학습의 특징
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#8" class="md-nav__link">
    <span class="md-ellipsis">
      8. 영상 처리용 아키텍처
    </span>
  </a>
  
    <nav class="md-nav" aria-label="8. 영상 처리용 아키텍처">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-cnn-convolutional-neural-network" class="md-nav__link">
    <span class="md-ellipsis">
      1️⃣ CNN (Convolutional Neural Network)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-3d-cnn-c3d-convolutional-3d-networks" class="md-nav__link">
    <span class="md-ellipsis">
      2️⃣ 3D CNN / C3D (Convolutional 3D Networks)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-convlstm-convolutional-long-short-term-memory" class="md-nav__link">
    <span class="md-ellipsis">
      3️⃣ ConvLSTM (Convolutional Long Short-Term Memory)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4-two-stream-cnn" class="md-nav__link">
    <span class="md-ellipsis">
      4️⃣ Two-Stream CNN
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#5-i3d-inflated-3d-convnet" class="md-nav__link">
    <span class="md-ellipsis">
      5️⃣ I3D (Inflated 3D ConvNet)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#6-video-transformer-timesformer-vivit" class="md-nav__link">
    <span class="md-ellipsis">
      6️⃣ Video Transformer (TimeSformer, ViViT 등)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#7-slowfast-networks" class="md-nav__link">
    <span class="md-ellipsis">
      7️⃣ SlowFast Networks
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_14" class="md-nav__link">
    <span class="md-ellipsis">
      🧭 시나리오별 추천 모델 요약
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_15" class="md-nav__link">
    <span class="md-ellipsis">
      🔗 참고 문헌
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#9" class="md-nav__link">
    <span class="md-ellipsis">
      9. 앙상블 구조
    </span>
  </a>
  
    <nav class="md-nav" aria-label="9. 앙상블 구조">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-bagging" class="md-nav__link">
    <span class="md-ellipsis">
      1. 배깅(Bagging): 분산 감소 중심
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-boosting" class="md-nav__link">
    <span class="md-ellipsis">
      2. 부스팅(Boosting): 편향 감소 중심
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-stacking" class="md-nav__link">
    <span class="md-ellipsis">
      3. 스태킹(Stacking): 비선형 조합
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_16" class="md-nav__link">
    <span class="md-ellipsis">
      비교 표: 앙상블 기법 특성
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_17" class="md-nav__link">
    <span class="md-ellipsis">
      요약
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#10" class="md-nav__link">
    <span class="md-ellipsis">
      10. 모델 성능 및 과적합/과소적합
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#11" class="md-nav__link">
    <span class="md-ellipsis">
      11. 메모리 및 효율성 고려
    </span>
  </a>
  
    <nav class="md-nav" aria-label="11. 메모리 및 효율성 고려">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#oom-out-of-memory" class="md-nav__link">
    <span class="md-ellipsis">
      OOM (Out-Of-Memory)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="OOM (Out-Of-Memory)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#oomout-of-memory" class="md-nav__link">
    <span class="md-ellipsis">
      OOM(Out Of Memory)란?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cuda-oomout-of-memory" class="md-nav__link">
    <span class="md-ellipsis">
      CUDA OOM(Out Of Memory) 완화 방법
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CUDA OOM(Out Of Memory) 완화 방법">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-batch-size" class="md-nav__link">
    <span class="md-ellipsis">
      1. 배치 크기(batch size) 줄이기
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2_3" class="md-nav__link">
    <span class="md-ellipsis">
      2. 모델 크기 축소
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-mixed-precision-training" class="md-nav__link">
    <span class="md-ellipsis">
      3. 혼합 정밀도 학습(Mixed Precision Training)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4_1" class="md-nav__link">
    <span class="md-ellipsis">
      4. 불필요한 변수/텐서 삭제
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#5_2" class="md-nav__link">
    <span class="md-ellipsis">
      5. 메모리 캐시 비우기
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#6-torchno_grad-modeleval" class="md-nav__link">
    <span class="md-ellipsis">
      6. torch.no_grad() 및 model.eval() 사용
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#7" class="md-nav__link">
    <span class="md-ellipsis">
      7. 입력 데이터 크기/해상도 줄이기
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#8-dataloader" class="md-nav__link">
    <span class="md-ellipsis">
      8. 데이터 분할 및 DataLoader 활용
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#9_1" class="md-nav__link">
    <span class="md-ellipsis">
      9. 메모리 누수 방지
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#10-gpu" class="md-nav__link">
    <span class="md-ellipsis">
      10. 더 큰 GPU로 업그레이드
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpu" class="md-nav__link">
    <span class="md-ellipsis">
      참고: GPU 메모리 상태 확인
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_18" class="md-nav__link">
    <span class="md-ellipsis">
      요약
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpu_1" class="md-nav__link">
    <span class="md-ellipsis">
      GPU 메모리 절감 기법
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mixed-precision-training" class="md-nav__link">
    <span class="md-ellipsis">
      Mixed Precision Training
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#12_1" class="md-nav__link">
    <span class="md-ellipsis">
      12. 추가 고려 요소
    </span>
  </a>
  
    <nav class="md-nav" aria-label="12. 추가 고려 요소">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#pruning-quantization-knowledge-distillation" class="md-nav__link">
    <span class="md-ellipsis">
      모델 경량화 기법 (Pruning, Quantization, Knowledge Distillation)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="모델 경량화 기법 (Pruning, Quantization, Knowledge Distillation)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-pruning" class="md-nav__link">
    <span class="md-ellipsis">
      1. Pruning (가지치기)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-quantization" class="md-nav__link">
    <span class="md-ellipsis">
      2. Quantization (양자화)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-knowledge-distillation" class="md-nav__link">
    <span class="md-ellipsis">
      3. Knowledge Distillation (지식 증류)
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cnn-transformer" class="md-nav__link">
    <span class="md-ellipsis">
      하이브리드 아키텍처 (CNN + Transformer)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vision-language-audio-text" class="md-nav__link">
    <span class="md-ellipsis">
      멀티모달 아키텍처 설계 (Vision-Language, Audio-Text)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_19" class="md-nav__link">
    <span class="md-ellipsis">
      하이퍼파라미터 최적화와 구조 설계의 관계
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#explainability" class="md-nav__link">
    <span class="md-ellipsis">
      모델 해석 가능성(Explainability)과 설계
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../AI-%EB%AA%A8%EB%8D%B8-%ED%95%99%EC%8A%B5%EA%B3%BC-%ED%8F%89%EA%B0%80-%E2%80%90-NEW/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    학습과 평가
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../%EC%9C%A0%EC%9A%A9%ED%95%9C-%EC%82%AC%EC%9D%B4%ED%8A%B8/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    유용한 사이트
    
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#1" class="md-nav__link">
    <span class="md-ellipsis">
      1. 모델 설계 개요
    </span>
  </a>
  
    <nav class="md-nav" aria-label="1. 모델 설계 개요">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#11-ai" class="md-nav__link">
    <span class="md-ellipsis">
      1.1 AI 모델 아키텍처 정의
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#12" class="md-nav__link">
    <span class="md-ellipsis">
      1.2 설계 시 고려 요소 (성능, 효율성, 확장성)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#13" class="md-nav__link">
    <span class="md-ellipsis">
      1.3 최신 동향 및 발전 방향
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2" class="md-nav__link">
    <span class="md-ellipsis">
      2. 주요 아키텍처 개념
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2. 주요 아키텍처 개념">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#21-residual-block-skip-connection" class="md-nav__link">
    <span class="md-ellipsis">
      2.1 Residual Block &amp; Skip Connection
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#22-transformer" class="md-nav__link">
    <span class="md-ellipsis">
      2.2 Transformer 기본 구조
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2.2 Transformer 기본 구조">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_1" class="md-nav__link">
    <span class="md-ellipsis">
      주요 구성 요소
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_2" class="md-nav__link">
    <span class="md-ellipsis">
      핵심 메커니즘
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_3" class="md-nav__link">
    <span class="md-ellipsis">
      전체 구조 요약
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_4" class="md-nav__link">
    <span class="md-ellipsis">
      참고
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#transformer" class="md-nav__link">
    <span class="md-ellipsis">
      Transformer
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#attention" class="md-nav__link">
    <span class="md-ellipsis">
      Attention 구조
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#transformer-family-model" class="md-nav__link">
    <span class="md-ellipsis">
      Transformer Family Model
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bert-family" class="md-nav__link">
    <span class="md-ellipsis">
      BERT Family
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#23-cnn" class="md-nav__link">
    <span class="md-ellipsis">
      2.3 CNN과 변형 구조
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#24" class="md-nav__link">
    <span class="md-ellipsis">
      2.4 순환 신경망 계열
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2.4 순환 신경망 계열">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#rnn-lstm-gru" class="md-nav__link">
    <span class="md-ellipsis">
      RNN / LSTM / GRU
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#25" class="md-nav__link">
    <span class="md-ellipsis">
      2.5 시공간 처리 구조
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2.5 시공간 처리 구조">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#3d-cnn-convlstm" class="md-nav__link">
    <span class="md-ellipsis">
      3D CNN, ConvLSTM
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3" class="md-nav__link">
    <span class="md-ellipsis">
      3. 자동화된 아키텍처 탐색
    </span>
  </a>
  
    <nav class="md-nav" aria-label="3. 자동화된 아키텍처 탐색">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#31-nas-neural-architecture-search" class="md-nav__link">
    <span class="md-ellipsis">
      3.1 NAS (Neural Architecture Search)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="3.1 NAS (Neural Architecture Search)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#nas" class="md-nav__link">
    <span class="md-ellipsis">
      NAS란 무엇인가?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#nas_1" class="md-nav__link">
    <span class="md-ellipsis">
      NAS의 핵심 개념과 작동 원리
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#nas_2" class="md-nav__link">
    <span class="md-ellipsis">
      NAS의 장점
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#nas_3" class="md-nav__link">
    <span class="md-ellipsis">
      NAS의 도전과제
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#nas-automl" class="md-nav__link">
    <span class="md-ellipsis">
      NAS와 AutoML의 차이
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#nas_4" class="md-nav__link">
    <span class="md-ellipsis">
      NAS의 미래와 활용
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_5" class="md-nav__link">
    <span class="md-ellipsis">
      요약
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#32-darts-differentiable-architecture-search" class="md-nav__link">
    <span class="md-ellipsis">
      3.2 DARTS (Differentiable Architecture Search)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="3.2 DARTS (Differentiable Architecture Search)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_6" class="md-nav__link">
    <span class="md-ellipsis">
      요약
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#4-llm" class="md-nav__link">
    <span class="md-ellipsis">
      4. 대형 언어 모델(LLM) 아키텍처
    </span>
  </a>
  
    <nav class="md-nav" aria-label="4. 대형 언어 모델(LLM) 아키텍처">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1_1" class="md-nav__link">
    <span class="md-ellipsis">
      1. 구조와 아키텍처
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2_1" class="md-nav__link">
    <span class="md-ellipsis">
      2. 학습 방식
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3_1" class="md-nav__link">
    <span class="md-ellipsis">
      3. 파라미터 효율적 튜닝
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4-tokenization" class="md-nav__link">
    <span class="md-ellipsis">
      4. 토큰화(Tokenization)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_7" class="md-nav__link">
    <span class="md-ellipsis">
      용어 요약
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#5" class="md-nav__link">
    <span class="md-ellipsis">
      5. 파인튜닝 전략
    </span>
  </a>
  
    <nav class="md-nav" aria-label="5. 파인튜닝 전략">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#51-lora-low-rank-adaptation" class="md-nav__link">
    <span class="md-ellipsis">
      5.1 LoRA (Low-Rank Adaptation)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#52-instruction-tuning" class="md-nav__link">
    <span class="md-ellipsis">
      5.2 Instruction Tuning
    </span>
  </a>
  
    <nav class="md-nav" aria-label="5.2 Instruction Tuning">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1_2" class="md-nav__link">
    <span class="md-ellipsis">
      1. 개념 및 목적
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2_2" class="md-nav__link">
    <span class="md-ellipsis">
      2. 기존 파인튜닝과의 차이점
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3_2" class="md-nav__link">
    <span class="md-ellipsis">
      3. 데이터셋 구성과 학습 방식
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4" class="md-nav__link">
    <span class="md-ellipsis">
      4. 효과와 특징
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#5_1" class="md-nav__link">
    <span class="md-ellipsis">
      5. 실제 적용 예시
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#6" class="md-nav__link">
    <span class="md-ellipsis">
      6. 한계와 과제
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_8" class="md-nav__link">
    <span class="md-ellipsis">
      요약
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#53-supervised-fine-tuning-vs-instruction-tuning" class="md-nav__link">
    <span class="md-ellipsis">
      5.3 Supervised Fine-Tuning vs Instruction Tuning
    </span>
  </a>
  
    <nav class="md-nav" aria-label="5.3 Supervised Fine-Tuning vs Instruction Tuning">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_9" class="md-nav__link">
    <span class="md-ellipsis">
      데이터 구조 차이
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_10" class="md-nav__link">
    <span class="md-ellipsis">
      목표 차이
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#6-self-supervised-learning" class="md-nav__link">
    <span class="md-ellipsis">
      6. 자기지도 학습(Self-Supervised Learning) 구조
    </span>
  </a>
  
    <nav class="md-nav" aria-label="6. 자기지도 학습(Self-Supervised Learning) 구조">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#simclr" class="md-nav__link">
    <span class="md-ellipsis">
      SimCLR
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#byol" class="md-nav__link">
    <span class="md-ellipsis">
      BYOL
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#jigsaw-puzzle" class="md-nav__link">
    <span class="md-ellipsis">
      jigsaw puzzle
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#maemasked-auto-encoder" class="md-nav__link">
    <span class="md-ellipsis">
      MAE(masked auto encoder)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#rotnet" class="md-nav__link">
    <span class="md-ellipsis">
      RotNet
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#7-unsupervised-learning" class="md-nav__link">
    <span class="md-ellipsis">
      7. 비지도 학습(Unsupervised Learning) 구조
    </span>
  </a>
  
    <nav class="md-nav" aria-label="7. 비지도 학습(Unsupervised Learning) 구조">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#unsupervised-learning" class="md-nav__link">
    <span class="md-ellipsis">
      비지도 학습(Unsupervised Learning) 개념
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_11" class="md-nav__link">
    <span class="md-ellipsis">
      비지도 학습의 대표적 예시
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_12" class="md-nav__link">
    <span class="md-ellipsis">
      실제 활용 사례
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_13" class="md-nav__link">
    <span class="md-ellipsis">
      비지도 학습의 특징
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#8" class="md-nav__link">
    <span class="md-ellipsis">
      8. 영상 처리용 아키텍처
    </span>
  </a>
  
    <nav class="md-nav" aria-label="8. 영상 처리용 아키텍처">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-cnn-convolutional-neural-network" class="md-nav__link">
    <span class="md-ellipsis">
      1️⃣ CNN (Convolutional Neural Network)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-3d-cnn-c3d-convolutional-3d-networks" class="md-nav__link">
    <span class="md-ellipsis">
      2️⃣ 3D CNN / C3D (Convolutional 3D Networks)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-convlstm-convolutional-long-short-term-memory" class="md-nav__link">
    <span class="md-ellipsis">
      3️⃣ ConvLSTM (Convolutional Long Short-Term Memory)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4-two-stream-cnn" class="md-nav__link">
    <span class="md-ellipsis">
      4️⃣ Two-Stream CNN
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#5-i3d-inflated-3d-convnet" class="md-nav__link">
    <span class="md-ellipsis">
      5️⃣ I3D (Inflated 3D ConvNet)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#6-video-transformer-timesformer-vivit" class="md-nav__link">
    <span class="md-ellipsis">
      6️⃣ Video Transformer (TimeSformer, ViViT 등)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#7-slowfast-networks" class="md-nav__link">
    <span class="md-ellipsis">
      7️⃣ SlowFast Networks
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_14" class="md-nav__link">
    <span class="md-ellipsis">
      🧭 시나리오별 추천 모델 요약
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_15" class="md-nav__link">
    <span class="md-ellipsis">
      🔗 참고 문헌
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#9" class="md-nav__link">
    <span class="md-ellipsis">
      9. 앙상블 구조
    </span>
  </a>
  
    <nav class="md-nav" aria-label="9. 앙상블 구조">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-bagging" class="md-nav__link">
    <span class="md-ellipsis">
      1. 배깅(Bagging): 분산 감소 중심
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-boosting" class="md-nav__link">
    <span class="md-ellipsis">
      2. 부스팅(Boosting): 편향 감소 중심
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-stacking" class="md-nav__link">
    <span class="md-ellipsis">
      3. 스태킹(Stacking): 비선형 조합
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_16" class="md-nav__link">
    <span class="md-ellipsis">
      비교 표: 앙상블 기법 특성
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_17" class="md-nav__link">
    <span class="md-ellipsis">
      요약
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#10" class="md-nav__link">
    <span class="md-ellipsis">
      10. 모델 성능 및 과적합/과소적합
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#11" class="md-nav__link">
    <span class="md-ellipsis">
      11. 메모리 및 효율성 고려
    </span>
  </a>
  
    <nav class="md-nav" aria-label="11. 메모리 및 효율성 고려">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#oom-out-of-memory" class="md-nav__link">
    <span class="md-ellipsis">
      OOM (Out-Of-Memory)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="OOM (Out-Of-Memory)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#oomout-of-memory" class="md-nav__link">
    <span class="md-ellipsis">
      OOM(Out Of Memory)란?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cuda-oomout-of-memory" class="md-nav__link">
    <span class="md-ellipsis">
      CUDA OOM(Out Of Memory) 완화 방법
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CUDA OOM(Out Of Memory) 완화 방법">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-batch-size" class="md-nav__link">
    <span class="md-ellipsis">
      1. 배치 크기(batch size) 줄이기
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2_3" class="md-nav__link">
    <span class="md-ellipsis">
      2. 모델 크기 축소
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-mixed-precision-training" class="md-nav__link">
    <span class="md-ellipsis">
      3. 혼합 정밀도 학습(Mixed Precision Training)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4_1" class="md-nav__link">
    <span class="md-ellipsis">
      4. 불필요한 변수/텐서 삭제
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#5_2" class="md-nav__link">
    <span class="md-ellipsis">
      5. 메모리 캐시 비우기
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#6-torchno_grad-modeleval" class="md-nav__link">
    <span class="md-ellipsis">
      6. torch.no_grad() 및 model.eval() 사용
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#7" class="md-nav__link">
    <span class="md-ellipsis">
      7. 입력 데이터 크기/해상도 줄이기
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#8-dataloader" class="md-nav__link">
    <span class="md-ellipsis">
      8. 데이터 분할 및 DataLoader 활용
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#9_1" class="md-nav__link">
    <span class="md-ellipsis">
      9. 메모리 누수 방지
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#10-gpu" class="md-nav__link">
    <span class="md-ellipsis">
      10. 더 큰 GPU로 업그레이드
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpu" class="md-nav__link">
    <span class="md-ellipsis">
      참고: GPU 메모리 상태 확인
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_18" class="md-nav__link">
    <span class="md-ellipsis">
      요약
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpu_1" class="md-nav__link">
    <span class="md-ellipsis">
      GPU 메모리 절감 기법
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mixed-precision-training" class="md-nav__link">
    <span class="md-ellipsis">
      Mixed Precision Training
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#12_1" class="md-nav__link">
    <span class="md-ellipsis">
      12. 추가 고려 요소
    </span>
  </a>
  
    <nav class="md-nav" aria-label="12. 추가 고려 요소">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#pruning-quantization-knowledge-distillation" class="md-nav__link">
    <span class="md-ellipsis">
      모델 경량화 기법 (Pruning, Quantization, Knowledge Distillation)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="모델 경량화 기법 (Pruning, Quantization, Knowledge Distillation)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-pruning" class="md-nav__link">
    <span class="md-ellipsis">
      1. Pruning (가지치기)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-quantization" class="md-nav__link">
    <span class="md-ellipsis">
      2. Quantization (양자화)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-knowledge-distillation" class="md-nav__link">
    <span class="md-ellipsis">
      3. Knowledge Distillation (지식 증류)
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cnn-transformer" class="md-nav__link">
    <span class="md-ellipsis">
      하이브리드 아키텍처 (CNN + Transformer)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vision-language-audio-text" class="md-nav__link">
    <span class="md-ellipsis">
      멀티모달 아키텍처 설계 (Vision-Language, Audio-Text)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_19" class="md-nav__link">
    <span class="md-ellipsis">
      하이퍼파라미터 최적화와 구조 설계의 관계
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#explainability" class="md-nav__link">
    <span class="md-ellipsis">
      모델 해석 가능성(Explainability)과 설계
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  



  <h1>아키텍처 설계</h1>

<h2 id="1">1. 모델 설계 개요<a class="headerlink" href="#1" title="Permanent link">&para;</a></h2>
<p>AI 모델 아키텍처 설계에 관한 각각의 항목에 대해 아래와 같이 설명할 수 있습니다.</p>
<hr />
<h3 id="11-ai">1.1 AI 모델 아키텍처 정의<a class="headerlink" href="#11-ai" title="Permanent link">&para;</a></h3>
<p>AI 모델 아키텍처는 인공지능이 특정 문제를 해결할 수 있도록 구성된 모델 내부의 구조와 구성 요소를 의미합니다. 즉, 입력 데이터가 처리되어 예측 결과로 변환되는 전체 경로를 설계하는 것입니다. 이에는 레이어(층) 구조, 연산 방식, 데이터 흐름, 그리고 각 계층 별로 어떤 함수와 연산이 적용되는지가 포함됩니다. 문제 유형(분류, 회귀, 생성 등)에 따라 모델 아키텍처의 형태가 달라지며, 다양한 알고리즘(딥러닝, 트랜스포머, CNN 등)이 사용됩니다. 아키텍처를 설계할 때는 도메인 지식과 경험, 데이터의 특성에 대한 충분한 분석이 필요합니다[4][5].</p>
<hr />
<h3 id="12">1.2 설계 시 고려 요소 (성능, 효율성, 확장성)<a class="headerlink" href="#12" title="Permanent link">&para;</a></h3>
<p>AI 모델 아키텍처 설계 시 핵심적으로 고려해야 할 요소는 다음과 같습니다.</p>
<ul>
<li><strong>성능</strong>: 모델이 정확도, 정밀도, 재현율 등 다양한 평가 지표에서 높은 성능을 달성할 수 있도록 해야 합니다. 오버피팅 방지, 하이퍼파라미터 튜닝, 적절한 모델 선택 등이 중요합니다.</li>
<li><strong>효율성</strong>: 계산 자원(메모리, 연산 속도, 에너지 효율성 등)을 최적화하는 것이 필수적입니다. 대용량 데이터와 복잡한 연산을 다루기 때문에, 병렬 처리, 최적화된 연산(예: GPU 가속), 특화된 하드웨어(AI 반도체 아키텍처 등) 활용이 일반적입니다. 특히 AI 반도체에서는 연산 성능, 메모리 대역폭, 전력 효율성의 균형이 중요합니다[6][7].</li>
<li><strong>확장성</strong>: 모델이 더 많은 데이터, 더 복잡한 문제, 그리고 다양한 서비스 요구에 맞춰 쉽게 확장될 수 있어야 합니다. 이를 위해 모듈형 아키텍처, 클라우드 환경, PaaS(Platform as a Service) 기반의 유연한 인프라가 활용됩니다. 데이터센터의 모듈형 아키텍처, DevOps 기반 운영, 지속적인 성능 모니터링도 확장성 측면에서 중시됩니다[5][6].</li>
</ul>
<hr />
<h3 id="13">1.3 최신 동향 및 발전 방향<a class="headerlink" href="#13" title="Permanent link">&para;</a></h3>
<p>AI 모델 아키텍처는 최근 다음과 같은 방향으로 발전하고 있습니다.</p>
<ul>
<li><strong>생성형 AI 및 멀티모달 아키텍처</strong>: 텍스트, 이미지, 음성 등 다양한 데이터를 하나의 모델로 처리하는 멀티모달 모델 아키텍처가 급부상하고 있습니다. 생성형 AI(Generative AI) 역시 기업 비즈니스 혁신의 핵심 기술로 부상하고 있으며, 패턴화된 설계와 프레임워크 적용을 통한 모델 구축이 확산 중입니다[5][10].</li>
<li><strong>클라우드 및 분산처리</strong>: AI 워크로드가 점차 클라우드 기반 인프라 및 분산 환경으로 이동하고 있습니다. 클라우드 PaaS, 모듈형 데이터센터, 자동화된 컴퓨팅 자원 할당 등으로 초기 투자 비용을 줄이고 유연성을 확보하는 방향입니다[6].</li>
<li><strong>AI 하드웨어 전문화</strong>: 특정 AI 연산에 특화된 반도체(AI SoC)와 대규모 병렬 연산 구조가 도입되고 있습니다. 이를 통해 연산 속도와 에너지 효율을 극대화합니다[7].</li>
<li><strong>엔터프라이즈 적용 가이드라인 표준화</strong>: 데이터 및 모델 안전성, 보안, 신뢰성, 거버넌스를 포괄한 표준화된 프레임워크와 아키텍처 패턴이 주요 기업에서 적극 활용되고 있습니다[1][5].</li>
</ul>
<p>출처
[1] AI 아키텍처 디자인 - Azure Architecture Center https://learn.microsoft.com/ko-kr/azure/architecture/ai-ml/
[2] AI Architecture 1편 – 세상은 개인화 추천 시대 https://tech.osci.kr/ai-architecture/
[3] AI 애플리케이션 아키텍처 이해 https://www.f5.com/ko_kr/company/blog/understanding-ai-application-architecture
[4] AI 모델 개발의 기본 개념과 전체 프로세스 개요 https://triangular.tistory.com/entry/AI-%EB%AA%A8%EB%8D%B8-%EA%B0%9C%EB%B0%9C%EC%9D%98-%EA%B8%B0%EB%B3%B8-%EA%B0%9C%EB%85%90%EA%B3%BC-%EC%A0%84%EC%B2%B4-%ED%94%84%EB%A1%9C%EC%84%B8%EC%8A%A4-%EA%B0%9C%EC%9A%94
[5] [Architecture] Generative AI 기업 아키텍처 설계 - 42JerryKim https://42jerrykim.github.io/post/2024-08-27-gen-ai-architecture/
[6] AI 인프라 아키텍처의 핵심 특징과 최신 동향 - Goover https://seo.goover.ai/report/202505/go-public-report-ko-68e5a512-bf15-4472-a343-4f44e85d9b9f-0-0.html
[7] 2.3 AI 반도체 아키텍처 설계 https://wikidocs.net/280862
[8] AI 개발 프로세스 총정리, 6단계로 이렇게 진행됩니다. https://aiheroes.ai/community/192
[9] 주요 AI, ML 및 DL 사용 사례 및 아키텍처 https://docs.netapp.com/ko-kr/netapp-solutions/data-analytics/apache-spark-major-ai-ml-and-dl-use-cases-and-architectures.html
[10] 2025년, AI 대홍수 시대: AI 모델 및 개발사 집중 분석 - 올리사이트 https://ollysite.com/board/read.php?M2_IDX=37272&amp;PAGE=1&amp;B_IDX=149404</p>
<hr />
<h2 id="2">2. 주요 아키텍처 개념<a class="headerlink" href="#2" title="Permanent link">&para;</a></h2>
<h3 id="21-residual-block-skip-connection">2.1 Residual Block &amp; Skip Connection<a class="headerlink" href="#21-residual-block-skip-connection" title="Permanent link">&para;</a></h3>
<p><strong>Residual Block</strong>과 <strong>Skip Connection</strong>은 딥러닝, 특히 ResNet(Residual Network)에서 깊은 신경망 학습을 가능하게 만든 핵심 구조입니다.</p>
<hr />
<p><strong>Residual Block이란?</strong></p>
<ul>
<li>Residual Block은 입력값 $$ x $$를 여러 층(예: Convolution, BatchNorm, Activation 등)을 거친 결과 $$ F(x) $$와 더해주는 구조입니다.</li>
<li>수식으로는 $$ y = F(x) + x $$로 표현됩니다. 여기서 $$ F(x) $$는 블록 내에서 학습되는 변환 함수입니다[3][5][7].</li>
<li>이 구조의 핵심은, 입력값 $$ x $$를 그대로 다음 블록에 더해줌으로써, 네트워크가 학습해야 할 정보를 "잔차(residual)"로 제한한다는 점입니다. 즉, 새로운 정보를 학습하는 대신 기존 정보를 보존하면서 추가적인 것만 학습합니다[3][5][7].</li>
</ul>
<hr />
<p><strong>Skip Connection(스킵 연결)이란?</strong></p>
<ul>
<li>Skip Connection은 한 레이어의 출력을 여러 레이어를 건너뛰어 다음 레이어의 입력에 직접 더하는 연결 방식입니다[1][2][6].</li>
<li>이 방식은 신경망이 깊어질수록 발생하는 vanishing gradient(기울기 소실) 문제를 완화하고, 더 깊은 네트워크에서도 효과적으로 학습할 수 있게 만듭니다[1][2][4][6].</li>
<li>Skip Connection이 적용된 블록을 Residual Block이라고 부르며, 이 구조 덕분에 각 레이어가 학습해야 할 정보량이 줄어들고, 기존 정보를 잃지 않으면서 새로운 정보만 추가적으로 학습할 수 있습니다[1][3][5][7].</li>
</ul>
<hr />
<p><strong>왜 Residual Block과 Skip Connection이 중요한가?</strong></p>
<ul>
<li>기존의 깊은 네트워크는 레이어를 많이 쌓으면 오히려 성능이 저하되는 문제가 있었습니다. 이는 주로 gradient vanishing/exploding 현상 때문입니다[1][2][4][5].</li>
<li>Residual Block과 Skip Connection을 도입하면, 입력값이 직접 출력단까지 전달될 수 있어, 역전파 시에도 gradient가 원활하게 흐를 수 있습니다[6].</li>
<li>이로 인해 훨씬 더 깊은 네트워크(예: 100층 이상)도 효과적으로 학습할 수 있게 되었고, 실제로 ResNet은 150층이 넘는 네트워크를 성공적으로 학습시켰습니다[1][4][5].</li>
</ul>
<hr />
<p><strong>요약</strong></p>
<ul>
<li><strong>Residual Block</strong>: 입력값을 여러 층을 거친 결과와 더하는 구조. 수식: $$ y = F(x) + x $$[3][5][7].</li>
<li><strong>Skip Connection</strong>: 입력값을 여러 층을 건너뛰어 다음 레이어에 직접 더하는 연결 방식[1][2][6].</li>
<li><strong>주요 효과</strong>: gradient vanishing 문제 완화, 깊은 네트워크 학습 가능, 기존 정보 보존 및 추가 정보 효율적 학습[1][2][4][6].</li>
</ul>
<hr />
<blockquote>
<p>"Residual block의 핵심은 layer간의 연결을 건너뛰어서 연산한다는 Skip Connection입니다."<br />
— Andrew Ng 교수 CNN 강의 요약[5]</p>
</blockquote>
<p>출처
[1] Residual Network, Residual Block 개념정리 - 치킨고양이짱아 공부일지 https://chickencat-jjanga.tistory.com/141
[2] [딥러닝]Skip-Connection이란? - Meaningful AI https://meaningful96.github.io/deeplearning/skipconnection/
[3] (7) ResNet (Residual Connection) - IT Repository - 티스토리 https://itrepo.tistory.com/36
[4] [딥러닝] ResNet (Residual block and Skip connection) - velog https://velog.io/@zzwon1212/%EB%94%A5%EB%9F%AC%EB%8B%9D-ResNet-Residual-block-and-Skip-connection
[5] Residual Block 개념 및 목적 정리 (feat.ResNet) https://psygo22.tistory.com/entry/Residual-Block-%EA%B0%9C%EB%85%90-%EB%B0%8F-%EB%AA%A9%EC%A0%81-%EC%A0%95%EB%A6%AC-featResNet%F0%9F%98%8E
[6] Skip connection 정리 - Shinuk - 티스토리 https://lswook.tistory.com/105
[7] Residual Network(ResNet) 아이디어: skip connect https://bommbom.tistory.com/entry/Residual-NetworkResNet-%EC%95%84%EC%9D%B4%EB%94%94%EC%96%B4-skip-connect
[8] Residual Block 간단 예시 - velog https://velog.io/@1-june/Residual-Block-%EA%B0%84%EB%8B%A8-%EC%98%88%EC%8B%9C
[9] Residual connection은 왜 효과적일까? - channel AI - 티스토리 https://channelai.tistory.com/2</p>
<h3 id="22-transformer">2.2 Transformer 기본 구조<a class="headerlink" href="#22-transformer" title="Permanent link">&para;</a></h3>
<p>Transformer는 자연어 처리(NLP) 분야에서 혁신적인 성능을 보여준 딥러닝 모델로, 크게 <strong>인코더(Encoder)</strong>와 <strong>디코더(Decoder)</strong> 두 블록으로 구성된 시퀀스-투-시퀀스(Seq2Seq) 구조를 갖고 있습니다[1][2][7].</p>
<h4 id="_1">주요 구성 요소<a class="headerlink" href="#_1" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>인코더(Encoder)</strong></li>
<li>입력 시퀀스를 받아서 특징(feature)을 추출합니다.</li>
<li>여러 개의 동일한 인코더 레이어가 쌓여 있습니다.</li>
<li>
<p>각 인코더 레이어는 다음과 같은 서브 레이어로 구성됩니다:</p>
<ul>
<li><strong>Multi-Head Self-Attention</strong>: 입력 시퀀스 내 각 단어가 다른 모든 단어와의 관계(유사도)를 계산합니다.</li>
<li><strong>Feed-Forward Neural Network</strong>: 각 위치별로 동일하게 적용되는 완전 연결 신경망입니다.</li>
<li><strong>Add &amp; Layer Normalization</strong>: 각 서브 레이어의 입력을 출력에 더하는 잔차 연결(Residual Connection)과 정규화(Layer Normalization)을 적용합니다[1][2][3][4][8].</li>
</ul>
</li>
<li>
<p><strong>디코더(Decoder)</strong></p>
</li>
<li>인코더의 출력(특징)을 받아서 출력 시퀀스를 생성합니다.</li>
<li>여러 개의 동일한 디코더 레이어가 쌓여 있습니다.</li>
<li>
<p>각 디코더 레이어는 다음과 같은 서브 레이어로 구성됩니다:</p>
<ul>
<li><strong>Masked Multi-Head Self-Attention</strong>: 출력 시퀀스의 각 위치가 미래 정보를 보지 못하도록 마스킹을 적용한 자기-어텐션입니다.</li>
<li><strong>Encoder-Decoder Attention</strong>: 디코더의 각 위치가 인코더의 출력 전체를 참고할 수 있도록 하는 어텐션입니다.</li>
<li><strong>Feed-Forward Neural Network</strong></li>
<li><strong>Add &amp; Layer Normalization</strong>[1][2][4].</li>
</ul>
</li>
<li>
<p><strong>Positional Encoding</strong></p>
</li>
<li>Transformer는 입력 순서를 인식하지 못하므로, 각 단어의 위치 정보를 임베딩에 더해줍니다.</li>
<li>이를 통해 단어의 순서(위치) 정보를 모델에 제공합니다[2][6].</li>
</ul>
<h4 id="_2">핵심 메커니즘<a class="headerlink" href="#_2" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>어텐션(Attention)</strong></li>
<li>입력 시퀀스 내 각 요소가 다른 모든 요소와의 연관성을 학습합니다.</li>
<li>Query, Key, Value의 개념을 사용하며, Scaled Dot-Product Attention 방식이 대표적입니다[1].</li>
<li>
<p><strong>Multi-Head Attention</strong>: 여러 개의 어텐션 헤드를 병렬로 사용해 다양한 관계를 동시에 학습합니다[1][2].</p>
</li>
<li>
<p><strong>Feed-Forward Network</strong></p>
</li>
<li>
<p>각 위치별로 동일하게 적용되는 두 개의 Dense(완전 연결) 레이어로 구성됩니다[2][4].</p>
</li>
<li>
<p><strong>잔차 연결(Residual Connection) &amp; 층 정규화(Layer Normalization)</strong></p>
</li>
<li>각 서브 레이어의 입력을 출력에 더해주고, 정규화를 통해 학습을 안정화합니다[2][3][4].</li>
</ul>
<h4 id="_3">전체 구조 요약<a class="headerlink" href="#_3" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>구성 요소</th>
<th style="text-align: center;">Encoder에 포함</th>
<th style="text-align: center;">Decoder에 포함</th>
<th>설명</th>
</tr>
</thead>
<tbody>
<tr>
<td>Multi-Head Self-Attention</td>
<td style="text-align: center;">O</td>
<td style="text-align: center;">O</td>
<td>입력(또는 출력) 시퀀스 내 단어 간 관계 학습</td>
</tr>
<tr>
<td>Masked Self-Attention</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">O</td>
<td>미래 단어 정보 차단(디코더에서만 사용)</td>
</tr>
<tr>
<td>Encoder-Decoder Attention</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">O</td>
<td>디코더가 인코더 출력 전체를 참고</td>
</tr>
<tr>
<td>Feed-Forward Network</td>
<td style="text-align: center;">O</td>
<td style="text-align: center;">O</td>
<td>위치별 완전 연결 신경망</td>
</tr>
<tr>
<td>Add &amp; Layer Norm</td>
<td style="text-align: center;">O</td>
<td style="text-align: center;">O</td>
<td>잔차 연결 및 정규화</td>
</tr>
<tr>
<td>Positional Encoding</td>
<td style="text-align: center;">O</td>
<td style="text-align: center;">O</td>
<td>단어 위치 정보 부여</td>
</tr>
</tbody>
</table>
<h4 id="_4">참고<a class="headerlink" href="#_4" title="Permanent link">&para;</a></h4>
<ul>
<li>트랜스포머는 인코더와 디코더 블록을 여러 층(stack)으로 쌓아 모델을 구성합니다(논문에서는 각각 6개 층)[4][5].</li>
<li>대표적인 응용 모델로 인코더만 사용하는 <strong>BERT</strong>, 디코더만 사용하는 <strong>GPT</strong> 등이 있습니다[2].</li>
</ul>
<p>이와 같이 Transformer는 어텐션 기반의 병렬 연산 구조와 다양한 서브 레이어로 구성되어, 기존 RNN/LSTM 기반 모델의 한계를 극복하고 뛰어난 성능을 보여주고 있습니다[1][2][4].</p>
<p>출처
[1] Transformer의 이론 및 구성요소(Attention is all you need) - velog https://velog.io/@gpdus41/Transformer-%EC%9D%B4%EB%A1%A0-%EB%B0%8F-%EA%B5%AC%EC%84%B1Attention-is-all-you-need
[2] 트랜스포머 모델 기본 개념과 주요 구성 요소 정리 - velog https://velog.io/@jayginwoolee/%ED%8A%B8%EB%9E%9C%EC%8A%A4%ED%8F%AC%EB%A8%B8-%EB%AA%A8%EB%8D%B8-%EA%B8%B0%EB%B3%B8-%EA%B0%9C%EB%85%90%EA%B3%BC-%EC%A3%BC%EC%9A%94-%EA%B5%AC%EC%84%B1-%EC%9A%94%EC%86%8C-%EC%A0%95%EB%A6%AC
[3] Transformer 이론 및 구성요소 : 네이버 블로그 https://blog.naver.com/hwankko27/222561357578
[4] 16-01 트랜스포머(Transformer) - 딥 러닝을 이용한 자연어 처리 입문 https://wikidocs.net/31379
[5] NLP의 핵심, 트랜스포머(Transformer) 복습! - Hello, didi universe https://didi-universe.tistory.com/entry/NLP%EC%9D%98-%ED%95%B5%EC%8B%AC-%ED%8A%B8%EB%9E%9C%EC%8A%A4%ED%8F%AC%EB%A8%B8Transformer-%EB%B3%B5%EC%8A%B5
[6] Transformer - 인코덤, 생물정보 전문위키 https://incodom.kr/Transformer
[7] &lt;지식 사전&gt; 트랜스포머(Transformer)가 뭔데? AI 혁명의 핵심 모델 ... https://blog.kakaocloud.com/91
[8] Transformers - ratsgo's NLPBOOK https://ratsgo.github.io/nlpbook/docs/language_model/transformers/
[9] [NLP] Transformer 알아보기 - (1) Encoder - 수로그 - 티스토리 https://cocosy.tistory.com/71
[10] 변압기 란 무엇이며 구성 요소는 무엇입니까? - 지식 https://ko.hydgetpower.com/info/what-is-electrical-transformer-and-what-are-it-75569321.html</p>
<h4 id="transformer">Transformer<a class="headerlink" href="#transformer" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>Transformer</strong>: Attention을 포함한 Encoder Block과 Decoder Block으로 구성된 대규모 모델<br />
<a href="https://codingopera.tistory.com/43?category=1094804">상세 설명 보기</a>  </li>
</ul>
<p><img width="992" height="554" alt="image-11" src="https://github.com/user-attachments/assets/f096c4ca-8463-43dc-87e6-b18d3c193f41" /></p>
<h4 id="attention">Attention 구조<a class="headerlink" href="#attention" title="Permanent link">&para;</a></h4>
<ul>
<li>
<p><strong>Encoder Self-Attention</strong>: 문맥상 중요한 단어에 '주목'하여 해당 단어의 컨텍스트화된 표현 생성<br />
<img width="581" height="244" alt="Sef-attentsion drawio" src="https://github.com/user-attachments/assets/d746a620-d005-40c6-8c77-8868e0a78016" /></p>
</li>
<li>
<p><strong>Decoder Self-Attention</strong>: 이미 출력된 정보만 활용하여 출력 단어 간의 관련도와 컨텍스트 추출 (순차성 강화)<br />
<img width="581" height="244" alt="Sef-attentsion-%ED%8E%98%EC%9D%B4%EC%A7%80-2 drawio" src="https://github.com/user-attachments/assets/1f3bfd6a-5559-4ad8-bf37-5e9f42e769ed" /></p>
</li>
<li>
<p><strong>Encoder-Decoder Attention</strong>: 생성 대상 단어와 원본 문장 단어 간의 관련도에 주목, 문맥 고려하여 정확성 강화<br />
<img width="321" height="121" alt="encoder-decoder-attentsion drawio" src="https://github.com/user-attachments/assets/192a9c1d-f889-4b0c-be64-0712b7f19a89" /></p>
</li>
<li>
<p><strong>Positional Encoding</strong>: 단어 순서 정보를 모델에 제공</p>
</li>
</ul>
<hr />
<h4 id="transformer-family-model">Transformer Family Model<a class="headerlink" href="#transformer-family-model" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>모델</th>
<th>구조</th>
<th>특징</th>
<th>용도</th>
<th>관련 Family</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>GPT</strong></td>
<td>Decoder</td>
<td>- 생성 능력 + 멀티모달<br>- 유창한 대화, 감정 이해</td>
<td>대화, 창작, 분석</td>
<td>GPT-1, 2, 3: 단방향 문맥 처리<br>GPT-3.5, 4<br>GPT-4o: 멀티모달, 실시간 대화</td>
</tr>
<tr>
<td><strong>Claud4</strong></td>
<td>Decoder</td>
<td>- 생성 능력 + 멀티모달<br>- 협력형 에이전트</td>
<td>장문 처리, 따뜻한 어조</td>
<td></td>
</tr>
<tr>
<td><strong>LLaMA</strong></td>
<td>Decoder</td>
<td>- 생성 능력</td>
<td>학술 연구용 오픈소스</td>
<td></td>
</tr>
<tr>
<td><strong>Gemini</strong></td>
<td>Decoder</td>
<td>- 생성 능력 + 멀티모달<br>- 분석, 번역, 코드</td>
<td>체계적 추론, 다국어</td>
<td></td>
</tr>
<tr>
<td><strong>BERT</strong></td>
<td>Encoder</td>
<td>- 양방향 문맥 이해</td>
<td>문장 이해, 분류</td>
<td>RoBERTa, ALBERT, DistilBERT, BioBERT, SciBERT, TinyBERT, ELECTRA, DeBERTa</td>
</tr>
<tr>
<td><strong>T5 (Text-to-Text Transfer Transformer)</strong></td>
<td>Decoder+Encoder</td>
<td>- Text2Text 구조</td>
<td>요약, 번역, QA</td>
<td>mT5, ByT5, UL2</td>
</tr>
</tbody>
</table>
<hr />
<h4 id="bert-family">BERT Family<a class="headerlink" href="#bert-family" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>모델</th>
<th>특징</th>
<th>주요 용도</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>BERT</strong></td>
<td>MLM + NSP</td>
<td>문장 분류, QA, NER</td>
</tr>
<tr>
<td><strong>RoBERTa</strong></td>
<td>MLM</td>
<td>문장 분류, QA</td>
</tr>
<tr>
<td><strong>SpanBERT</strong></td>
<td>Span 마스킹 + 예측</td>
<td>문장 구조 이해 → 관계 추출, 문장 분할</td>
</tr>
<tr>
<td><strong>DeBERTa</strong></td>
<td>MLM + 위치/의미 분리, Relative Positional Encoding</td>
<td>표현력 강화</td>
</tr>
<tr>
<td><strong>ELECTRA</strong></td>
<td>RTD (Replaced Token Detection)</td>
<td>분류, QA</td>
</tr>
<tr>
<td><strong>ALBERT</strong></td>
<td>MLM + SOP</td>
<td>경량화, 메모리 절약 (모바일 NLP)</td>
</tr>
<tr>
<td><strong>DistilBERT</strong></td>
<td>Distillation</td>
<td>성능 97% 유지의 경량 모델</td>
</tr>
<tr>
<td><strong>TinyBERT</strong></td>
<td>Distillation + Layer 축소</td>
<td>엣지/IoT 디바이스 용</td>
</tr>
<tr>
<td><strong>BioBERT</strong></td>
<td>의료 분야 MLM</td>
<td>의료 QA, 논문 분석</td>
</tr>
<tr>
<td><strong>SciBERT</strong></td>
<td>과학 분야 MLM</td>
<td>연구 문서 분석</td>
</tr>
</tbody>
</table>
<p><strong>참고 용어</strong>
- <strong>MLM (Masked Language Modeling)</strong>: 문장 내 단어를 마스킹하고 복원
- <strong>NSP (Next Sentence Prediction)</strong>: 두 문장이 연결되는지 예측
- <strong>SOP (Sentence Order Prediction)</strong>: 두 문장의 순서가 올바른지 예측
- <strong>RTD (Replaced Token Detection)</strong>: 잘못된 단어 판별</p>
<blockquote>
<p>최근 LLM의 생성 기능 요건 강화로 인해, BERT 계열은 LLM에서 제외되는 추세</p>
</blockquote>
<h3 id="23-cnn">2.3 CNN과 변형 구조<a class="headerlink" href="#23-cnn" title="Permanent link">&para;</a></h3>
<p><img width="850" height="285" alt="image-10" src="https://github.com/user-attachments/assets/80e0813a-b38e-4303-8469-8703455a6b7e" /></p>
<ul>
<li><strong>Convolutional Layer</strong>: 필터/커널로 이미지의 특징을 추출하여 Feature Map 생성  </li>
<li><strong>Stride</strong>: 필터/커널 이동 거리  </li>
<li><strong>Padding</strong>: 가장자리에 채워지는 값</li>
<li><strong>Pooling Layer</strong>: Feature Map의 크기를 줄이고 주요 특징을 강조  </li>
<li>Max Pooling, Average Pooling</li>
<li>
<p><strong>Fully Connected Layer / Dense Layer</strong>: Feature Map 기반의 분류 작업 수행</p>
</li>
<li>
<p>변형 및 응용</p>
</li>
</ul>
<h3 id="24">2.4 순환 신경망 계열<a class="headerlink" href="#24" title="Permanent link">&para;</a></h3>
<p><img width="850" height="383" alt="Computation-wise-comparison-of-RNN-LSTM-and-GRU-nodes" src="https://github.com/user-attachments/assets/4beb8259-48c9-483e-b343-7da228db549b" /></p>
<h4 id="rnn-lstm-gru">RNN / LSTM / GRU<a class="headerlink" href="#rnn-lstm-gru" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>RNN</strong>(Recurrent Neural Network)은 입력 데이터를 순차적으로 처리하며, 이전 단계의 정보를 다음 단계로 전달하는 구조로 시계열 및 순차 데이터(텍스트, 음성, 센서 등)에 적합합니다. 하지만 긴 시퀀스에서 기울기 소실 문제가 발생하며, 장기 의존성 학습이 어렵다는 한계를 갖고 있습니다. 실제로 짧은 시퀀스 처리에는 성능이 좋으나, 장기 기억이 필요한 문맥에서는 한계가 있습니다.</li>
<li><strong>LSTM</strong>(Long Short-Term Memory)은 RNN의 장기 의존성 한계를 극복하기 위해 '게이트'라는 구조가 추가된 모델입니다(입력 게이트, 망각 게이트, 출력 게이트). 이로 인해 중요한 정보를 오래 기억할 수 있고, 긴 문장이나 장기적인 패턴이 중요한 자연어 처리, 언어 번역, 음성 인식 등에 많이 활용됩니다. 단점은 구조가 복잡해 파라미터가 많고, 계산/메모리 비용이 큽니다.</li>
<li><strong>GRU</strong>(Gated Recurrent Unit)는 LSTM의 장점을 유지하면서 구조를 간소화한 모델입니다(업데이트 게이트, 리셋 게이트). 메모리/파라미터가 적고 학습 속도가 빠르지만, 대규모 장기 의존성 처리에서는 LSTM에 비해 효과가 다소 떨어질 수 있습니다. 실시간 처리, 모바일 환경, 상대적으로 짧은 시퀀스에서 주로 사용됩니다.</li>
</ul>
<table>
<thead>
<tr>
<th>비교 항목</th>
<th>RNN</th>
<th>LSTM</th>
<th>GRU</th>
</tr>
</thead>
<tbody>
<tr>
<td>구조 복잡도</td>
<td>단순</td>
<td>복잡(3개의 게이트)</td>
<td>상대적 단순(2개의 게이트)</td>
</tr>
<tr>
<td>장기 기억</td>
<td>약함(기울기 소실)</td>
<td>강함(장기 정보 유지)</td>
<td>보통(짧은 시퀀스에 효율적)</td>
</tr>
<tr>
<td>계산/메모리 비용</td>
<td>낮음</td>
<td>높음</td>
<td>중간</td>
</tr>
<tr>
<td>학습/추론 속도</td>
<td>빠름</td>
<td>느림</td>
<td>빠름</td>
</tr>
<tr>
<td>대표 적용 분야</td>
<td>간단 시계열</td>
<td>복잡∙장기 의존 NLP, 음성 등</td>
<td>모바일, 실시간, 짧은 데이터</td>
</tr>
<tr>
<td>대표적 한계</td>
<td>긴 시퀀스 미지원</td>
<td>계산량, 메모리 소모</td>
<td>장기 의존성 다소 약함</td>
</tr>
</tbody>
</table>
<ul>
<li><strong>적용 사례</strong>:  </li>
<li>RNN: 간단한 시계열 데이터 분석, 단기 예측</li>
<li>LSTM: 번역 모델, 챗봇, 긴 대화 문맥 처리, 음성 인식</li>
<li>GRU: 실시간 번역, IoT 데이터, 모바일 텍스트 예측[2][4][5][6]</li>
</ul>
<h3 id="25">2.5 시공간 처리 구조<a class="headerlink" href="#25" title="Permanent link">&para;</a></h3>
<h4 id="3d-cnn-convlstm">3D CNN, ConvLSTM<a class="headerlink" href="#3d-cnn-convlstm" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>3D CNN</strong>(3-Dimensional Convolutional Neural Network)은 데이터 형태가 시간/공간적으로 확장된 경우(예: 동영상, 의료 영상 등 3D 데이터)에 적용됩니다. 2D CNN에서 공간적 패턴만 추출하는 반면, 3D CNN은 시간축까지 함께 컨볼루션 연산해 시공간 정보(동영상의 움직임, 뇌 MRI의 시계열 변화 등)를 모두 반영할 수 있습니다. 단점은 계산 비용과 메모리 사용량이 높다는 것.</li>
<li><strong>ConvLSTM</strong>은 LSTM과 CNN의 결합 구조로, 시공간 데이터(동영상, 시계열 영상 등) 내의 시간적∙공간적 패턴을 동시에 학습합니다. 셀의 게이트 연산에서 컨볼루션 연산을 적용해 시각적 구조 유지와 장기/짧은 시퀀스 처리 모두 가능합니다. 영상 예측, 기상 데이터(강수 예측 등)와 같은 고차원 시계열 영상에 주로 적용됩니다.</li>
<li><strong>장점</strong>  </li>
<li>3D CNN: 시공간 패턴의 동시 분석, 복잡한 움직임 인식  </li>
<li>ConvLSTM: 복잡한 시계열∙공간 패턴 동시 처리, 영상 예측 강화</li>
<li><strong>단점</strong>  </li>
<li>3D CNN: 높은 메모리/연산 비용, 오버피팅 등 위험</li>
<li>
<p>ConvLSTM: 파라미터가 많아 최적화가 어렵고, 대용량 데이터에서 학습 시간 증가</p>
</li>
<li>
<p><strong>대표 적용 사례</strong>:  </p>
</li>
<li>3D CNN: 동영상 분류, 행동 인식, MRI 등 3D 의료 영상 분석</li>
<li>ConvLSTM: 기상 예측(강수량, 바람), 시계열 영상 이상의 예측, 트래픽 흐름 예측, 실시간 영상 분석</li>
</ul>
<hr />
<p>이처럼 RNN 계열과 시공간 처리 구조는 데이터 특성과 활용 목적에 따라 적합한 모델을 선택함으로써 최적의 성능과 효율을 추구합니다.</p>
<p>출처
[1] [Deep Learning] RNN, LSTM, GRU 차이점 (순환 신경망 모델들) https://huidea.tistory.com/237
[2] [7편] RNN 한계 극복: LSTM과 GRU의 구조와 상세 비교 및 최근 연구 ... https://newitlec.com/entry/7%ED%8E%B8-RNN%EC%9D%98-%EA%B8%B0%EC%9A%B8%EA%B8%B0-%EC%86%8C%EC%8B%A4-%EB%AC%B8%EC%A0%9C-%ED%95%B4%EA%B2%B0-LSTM%EA%B3%BC-GRU%EC%9D%98-%EC%9B%90%EB%A6%AC%EC%99%80-%EC%9D%91%EC%9A%A9
[3] [DL] 순환 신경망 - RNN, LSTM, GRU 파헤치기 - 천방지축 Tech 일기장 https://heeya-stupidbutstudying.tistory.com/entry/DL-%EC%88%9C%ED%99%98-%EC%8B%A0%EA%B2%BD%EB%A7%9D-RNN-LSTM-GRU-%ED%8C%8C%ED%97%A4%EC%B9%98%EA%B8%B0
[4] 장단기 기억 네트워크(LSTM) 및 GRU 개념 완벽 정리 - learningflix https://learningflix.tistory.com/141
[5] GRU(Gated Recurrent Unit): 더 가벼운 구조로 LSTM을 대체할 수 ... http://woka.kr/blog/deep%20learning%20%EA%B8%B0%EC%B4%88/2025/03/17/GRU.html
[6] RNN, LSTM, GRU, Transformer model - co-yong 님의 블로그 https://co-yong.tistory.com/entry/RNN
[7] 순환 신경망(RNN) 및 그 변형(예: LSTM, GRU) - velog https://velog.io/@imu1119/%EC%88%9C%ED%99%98-%EC%8B%A0%EA%B2%BD%EB%A7%9DRNN-%EB%B0%8F-%EA%B7%B8-%EB%B3%80%ED%98%95%EC%98%88-LSTM-GRU
[8] [NLP] RNN, LSTM, GRU를 비교해보자 - 코딩 매거진 - 티스토리 https://hyunsooworld.tistory.com/entry/NLP-%EA%B3%B5%EB%B6%80-RNN-LSTM-GRU%EB%A5%BC-%EB%B9%84%EA%B5%90%ED%95%B4%EB%B3%B4%EC%9E%90
[9] [RNN/LSTM/GRU] Recurrent Neural Network 순환신경망 https://di-bigdata-study.tistory.com/23</p>
<hr />
<h2 id="3">3. 자동화된 아키텍처 탐색<a class="headerlink" href="#3" title="Permanent link">&para;</a></h2>
<h3 id="31-nas-neural-architecture-search">3.1 NAS (Neural Architecture Search)<a class="headerlink" href="#31-nas-neural-architecture-search" title="Permanent link">&para;</a></h3>
<h4 id="nas">NAS란 무엇인가?<a class="headerlink" href="#nas" title="Permanent link">&para;</a></h4>
<p><strong>NAS(Neural Architecture Search)</strong>는 인공지능(AI)과 딥러닝 모델의 구조(아키텍처)를 자동으로 설계해주는 기술입니다. 즉, 사람이 직접 신경망의 층, 연결 방식, 연산자 등을 설계하지 않고, AI가 스스로 최적의 모델 구조를 찾아내는 방법론입니다[1][4][5].</p>
<hr />
<h4 id="nas_1">NAS의 핵심 개념과 작동 원리<a class="headerlink" href="#nas_1" title="Permanent link">&para;</a></h4>
<p><strong>1. 탐색 공간(Search Space) 정의</strong>
- NAS는 먼저 사용할 수 있는 신경망 구성 요소(예: 합성곱 층, 풀링 층, 활성화 함수 등)와 이들의 조합 방식을 정의합니다.
- 이 공간이 넓을수록 더 다양한 모델 구조를 탐색할 수 있습니다[1][4][5].</p>
<p><strong>2. 탐색 전략(Search Strategy)</strong>
- 정의된 탐색 공간에서 최적의 구조를 찾기 위해 다양한 알고리즘(강화학습, 진화 알고리즘, 무작위 탐색 등)을 사용합니다.
- 예를 들어, 강화학습을 사용하면 AI가 여러 구조를 시도해보고, 좋은 성능을 내는 구조에 더 집중하게 됩니다[1][4][5].</p>
<p><strong>3. 성능 추정(Performance Estimation)</strong>
- 후보 구조의 성능을 평가합니다. 모든 구조를 완전히 학습시키면 시간이 오래 걸리므로, 부분 학습이나 파라미터 공유 등으로 평가 시간을 줄입니다[1][4][5].</p>
<h4 id="nas_2">NAS의 장점<a class="headerlink" href="#nas_2" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>자동화</strong>: 전문가가 아니어도 고성능 AI 모델을 만들 수 있습니다.</li>
<li><strong>효율성</strong>: 인간보다 훨씬 빠르게 수많은 구조를 탐색할 수 있습니다.</li>
<li><strong>혁신성</strong>: 인간이 생각하지 못한 새로운 구조를 발견할 수 있습니다.</li>
<li><strong>복잡성 관리</strong>: 대규모 신경망 구조도 효과적으로 설계할 수 있습니다.</li>
<li><strong>다중 목표 최적화</strong>: 정확도, 속도, 메모리 등 여러 목표를 동시에 고려해 최적화할 수 있습니다[1][4][5].</li>
</ul>
<h4 id="nas_3">NAS의 도전과제<a class="headerlink" href="#nas_3" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>높은 계산 비용</strong>: 탐색 과정에서 막대한 연산 자원이 필요합니다.</li>
<li><strong>탐색 공간 설계</strong>: 너무 넓거나 좁으면 효율적인 탐색이 어렵습니다.</li>
<li><strong>설명 가능성</strong>: 자동으로 만들어진 구조의 원리나 동작을 해석하기 어렵습니다.</li>
<li><strong>공정성 및 편향</strong>: 데이터 편향이 결과에 영향을 미칠 수 있습니다[1].</li>
</ul>
<hr />
<h4 id="nas-automl">NAS와 AutoML의 차이<a class="headerlink" href="#nas-automl" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>구분</th>
<th>NAS</th>
<th>AutoML</th>
</tr>
</thead>
<tbody>
<tr>
<td>목적</td>
<td>신경망 구조(아키텍처) 자동 탐색</td>
<td>전체 ML 파이프라인(전처리, 모델선택 등) 자동화</td>
</tr>
<tr>
<td>범위</td>
<td>모델 구조 자체에 집중</td>
<td>데이터 전처리, 특성 엔지니어링, 하이퍼파라미터 튜닝 등 포함</td>
</tr>
<tr>
<td>활용</td>
<td>주로 딥러닝 모델 설계에 사용</td>
<td>머신러닝 전반에 적용 가능</td>
</tr>
</tbody>
</table>
<p>[4]</p>
<hr />
<h4 id="nas_4">NAS의 미래와 활용<a class="headerlink" href="#nas_4" title="Permanent link">&para;</a></h4>
<ul>
<li>NAS는 AI 모델 개발의 표준이 되어가고 있으며, 누구나 데이터를 입력하면 최적의 AI 모델을 자동으로 만들 수 있는 AutoML 플랫폼의 핵심이 되고 있습니다.</li>
<li>의료, 자율주행, 에너지 등 다양한 분야에서 NAS의 활용이 확산되고 있습니다[1].</li>
</ul>
<hr />
<h4 id="_5">요약<a class="headerlink" href="#_5" title="Permanent link">&para;</a></h4>
<ul>
<li>NAS는 AI가 AI 모델 구조를 자동으로 설계하는 혁신적인 기술입니다.</li>
<li>탐색 공간, 탐색 전략, 성능 추정의 3단계로 작동합니다.</li>
<li>자동화, 효율성, 혁신성 등 다양한 장점이 있지만, 계산 비용 등 해결해야 할 과제도 존재합니다.</li>
<li>앞으로 AI 설계의 민주화와 접근성 확대에 중요한 역할을 할 것으로 기대됩니다[1][4][5].</li>
</ul>
<p>출처
[1] &lt;지식 사전&gt; Neural Architecture Search(NAS)란? AI가 AI를 설계하는 ... https://blog.kakaocloud.com/121
[2] [MAT 3편] NAS(Network architecture search)란 무엇일까? - 데이콘 https://dacon.io/codeshare/4879
[3] 뉴럴 아키텍처 서치(NAS)의 효율적인 구현 방법: 인공지능 모델 설계 ... https://www.jaenung.net/tree/20954
[4] 신경망 아키텍처 검색(NAS) 설명 - Ultralytics https://www.ultralytics.com/ko/glossary/neural-architecture-search-nas
[5] [AI] 인공지능 모델 설계 자동화를 위한 NAS https://easyjwork.tistory.com/63
[6] "AI·딥러닝으로 NAS에 저장된 사진 자동 분류" - 지디넷코리아 https://zdnet.co.kr/view/?no=20200305164334
[7] Neural Architecture Search (NAS) - Framework - doingAI - 티스토리 https://doing-ai.tistory.com/entry/Neural-Architecture-Search-NAS-Framework
[8] [NeurIPS 2021] Neural Architecture Search Review : 네이버 블로그 https://post.naver.com/viewer/postView.naver?volumeNo=33500636&amp;memberNo=52249799
[9] QNAP, AI 기반 RAG 검색으로 Qsirch에 힘을 실어 NAS를 스마트 지식 ... https://www.qnap.com/ko-kr/news/2025/qnap-ai-%EA%B8%B0%EB%B0%98-rag-%EA%B2%80%EC%83%89%EC%9C%BC%EB%A1%9C-qsirch%EC%97%90-%ED%9E%98%EC%9D%84-%EC%8B%A4%EC%96%B4-nas%EB%A5%BC-%EC%8A%A4%EB%A7%88%ED%8A%B8-%EC%A7%80%EC%8B%9D-%ED%97%88%EB%B8%8C%EB%A1%9C
[10] LLM 탑재 'AI NAS'...대용량 콘텐츠에 최적화된 지능형 네트워크 연결 ... https://www.gttkorea.com/news/articleView.html?idxno=18240</p>
<h3 id="32-darts-differentiable-architecture-search">3.2 DARTS (Differentiable Architecture Search)<a class="headerlink" href="#32-darts-differentiable-architecture-search" title="Permanent link">&para;</a></h3>
<p><strong>DARTS</strong>는 Neural Architecture Search(NAS) 분야에서 혁신적인 방법론으로, 기존의 강화학습이나 진화알고리즘 기반 NAS가 가진 비효율성과 높은 계산 비용 문제를 해결하기 위해 제안된 미분 가능한 아키텍처 탐색 기법입니다.</p>
<hr />
<p><strong>핵심 아이디어</strong></p>
<ul>
<li>기존 NAS는 이산적(discrete)인 탐색 공간에서 후보 아키텍처를 하나씩 샘플링하고 평가하는 방식이었으나, DARTS는 탐색 공간을 연속적으로(relaxation) 확장하여 미분이 가능하도록 만듭니다[1][2][3][7].</li>
<li>이로 인해 아키텍처 탐색을 gradient descent(경사하강법)로 최적화할 수 있어, 탐색 속도가 대폭 향상되고 계산 자원이 크게 절약됩니다[1][2][5][7].</li>
</ul>
<hr />
<p><strong>DARTS의 구조와 탐색 방식</strong></p>
<ul>
<li><strong>Cell-based 구조</strong>: 전체 네트워크를 여러 개의 작은 cell(모듈)로 나누고, 각 cell 내부의 구조를 탐색합니다. 이 cell을 반복적으로 쌓아 전체 네트워크를 만듭니다[2][4][7].</li>
<li><strong>DAG(Directed Acyclic Graph)</strong>: 각 cell은 여러 노드(데이터의 표현)와 엣지(연산자)로 구성된 DAG 형태로 표현됩니다. 각 엣지에는 여러 후보 연산(예: convolution, pooling, none 등)이 할당될 수 있습니다[2][7].</li>
<li><strong>Mixed Operation</strong>: 각 엣지에 대해 여러 연산의 가중합(mixed operation)을 정의하고, 이 가중치(architecture parameter, α)를 학습합니다. 이 가중치는 softmax로 확률화되어 각 연산의 중요도를 나타냅니다[2][5][7].</li>
<li><strong>Bi-level Optimization</strong>: 아키텍처 파라미터(α)는 validation loss를, 네트워크 파라미터(w)는 training loss를 최소화하도록 번갈아가며 최적화합니다[2][5].</li>
<li><strong>최종 아키텍처 도출</strong>: 탐색이 끝나면 각 엣지마다 가장 높은 가중치를 가진 연산을 선택해(discretization) 최종 네트워크 구조를 확정합니다[2][4][7].</li>
</ul>
<hr />
<p><strong>장점 및 성능</strong></p>
<ul>
<li>기존 NAS 대비 수십 배 빠른 탐색 속도와 낮은 연산 비용[1][5].</li>
<li>강화학습, 진화알고리즘 기반 방법과 유사하거나 더 나은 성능을 이미지 분류(CIFAR-10, ImageNet)와 언어 모델링(PTB, WikiText-2) 등 다양한 분야에서 입증[1][2][5].</li>
<li>단일 GPU 환경에서도 실용적으로 적용 가능하며, ENAS 등 다른 One-Shot NAS 기법보다도 효율적임[5][6].</li>
</ul>
<hr />
<p><strong>한계 및 개선점</strong></p>
<ul>
<li>탐색 과정에서 모든 후보 연산을 동시에 처리하므로 GPU 메모리 사용량이 높아, 대규모 탐색 공간에서는 메모리 이슈가 발생할 수 있음[6].</li>
<li>Continuous encoding과 discrete architecture 간의 불일치로 인한 성능 저하 가능성이 존재함. 이를 완화하기 위한 softmax temperature annealing 등 다양한 후속 연구가 진행 중임[5].</li>
<li>메모리 및 효율성 문제를 개선하는 Zero-One-Shot NAS 등 다양한 파생 연구가 활발히 이루어지고 있음[6].</li>
</ul>
<hr />
<h4 id="_6">요약<a class="headerlink" href="#_6" title="Permanent link">&para;</a></h4>
<ul>
<li>DARTS는 NAS의 탐색 공간을 연속적으로 relax하여 gradient 기반 최적화를 가능하게 한 미분 가능한 아키텍처 탐색 기법입니다.</li>
<li>Cell-based 구조, mixed operation, bi-level optimization이 핵심이며, 기존 NAS 대비 빠르고 효율적입니다.</li>
<li>높은 메모리 요구 등 한계도 있으나, 다양한 파생 연구를 통해 개선되고 있습니다.</li>
</ul>
<hr />
<h2 id="4-llm">4. 대형 언어 모델(LLM) 아키텍처<a class="headerlink" href="#4-llm" title="Permanent link">&para;</a></h2>
<h4 id="1_1">1. 구조와 아키텍처<a class="headerlink" href="#1_1" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>Transformer 기반</strong><br />
  LLM은 Transformer 아키텍처를 기반으로 하며, 특히 <em>decoder-only</em> 구조가 주로 사용됩니다.</li>
<li><strong>Autoregressive 방식</strong><br />
  이전까지 생성된 토큰을 바탕으로 다음 토큰을 순차적으로 예측하며 텍스트를 생성합니다.</li>
<li>** Self attention**
  같은 문장 내에서 단어들 간의 의미 관계를 고려하는 것 (참고: https://cn-c.tistory.com/68)</li>
</ul>
<h4 id="2_1">2. 학습 방식<a class="headerlink" href="#2_1" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>사전학습(Pre-training)</strong><br />
  대규모 텍스트 데이터로 사전학습을 진행하여 언어의 패턴과 지식을 광범위하게 습득합니다.</li>
<li><strong>Zero-shot &amp; Few-shot 학습</strong><br />
  별도의 파인튜닝(fine-tuning) 없이도, 또는 몇 개의 예시만으로 다양한 작업을 수행할 수 있습니다.<br />
  이는 LLM이 일반적인 언어적 맥락과 추론 능력을 내재화했기 때문입니다.</li>
</ul>
<h4 id="3_1">3. 파라미터 효율적 튜닝<a class="headerlink" href="#3_1" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>LoRA(Low-Rank Adaptation)</strong><br />
  전체 모델 파라미터를 업데이트하지 않고, 저차원 행렬을 추가로 학습하여 파라미터 효율성을 높이고, 메모리와 연산 자원을 절약하면서도 성능을 유지합니다.</li>
</ul>
<h4 id="4-tokenization">4. 토큰화(Tokenization)<a class="headerlink" href="#4-tokenization" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>서브워드 기반 토큰화</strong><br />
  BPE(Byte-Pair Encoding), WordPiece 등 통계적 병합 방식을 활용하여 어휘 집합을 구성합니다.</li>
<li><strong>OOV(Out-of-Vocabulary) 대응</strong><br />
  새로운 단어에도 유연하게 대처할 수 있어 다양한 언어와 도메인에 효과적입니다.</li>
</ul>
<h4 id="_7">용어 요약<a class="headerlink" href="#_7" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>용어</th>
<th>설명</th>
</tr>
</thead>
<tbody>
<tr>
<td>Transformer</td>
<td>대규모 병렬 연산과 장기 의존성 학습이 가능한 신경망 구조</td>
</tr>
<tr>
<td>Decoder-only</td>
<td>입력 시퀀스만 받아 생성형 작업에 특화된 구조</td>
</tr>
<tr>
<td>Autoregressive</td>
<td>이전 토큰 기반 순차적 예측 방식</td>
</tr>
<tr>
<td>Zero-shot/Few-shot</td>
<td>별도 파인튜닝 없이, 혹은 소수 예시만으로 작업 수행</td>
</tr>
<tr>
<td>LoRA</td>
<td>저차원 행렬 추가로 효율적 파라미터 튜닝</td>
</tr>
<tr>
<td>BPE/WordPiece</td>
<td>빈번한 문자/부분단어 병합으로 어휘 구성</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="5">5. 파인튜닝 전략<a class="headerlink" href="#5" title="Permanent link">&para;</a></h2>
<h3 id="51-lora-low-rank-adaptation">5.1 LoRA (Low-Rank Adaptation)<a class="headerlink" href="#51-lora-low-rank-adaptation" title="Permanent link">&para;</a></h3>
<ul>
<li>
<p><strong>1. LoRA란?</strong>
LoRA(Low-Rank Adaptation)는 Huggingface에서 제안한 Parameter-Efficient Fine-Tuning(PEFT) 방식 중 하나로, 대규모 사전학습 언어모델(LLM)을 효율적으로 미세 조정(fine-tuning) 하기 위한 기법입니다. Python에서는 <a href="https://github.com/huggingface/peft">peft</a> 라이브러리를 통해 손쉽게 구현할 수 있습니다.</p>
</li>
<li>
<p><strong>2. 개요 및 구조</strong>
기존의 full fine-tuning은 모델의 전체 파라미터를 수정하면서 학습합니다. 이 경우 연산량과 메모리 사용이 크며, 복수 작업에 맞춰 각각 모델을 새로 학습해야 하는 비용이 큽니다.</p>
</li>
</ul>
<p>LoRA는 이 문제를 해결하기 위해 다음과 같은 방식으로 작동합니다:</p>
<ul>
<li>기존 LLM의 선형 계층(Linear layer)은 동결(freeze)</li>
<li>해당 위치에 저랭크 행렬 두 개 (A, B layer) 를 추가 삽입</li>
<li>학습 시에는 A와 B 행렬만 학습 (즉, ΔW ≈ A·B 형태로 weight 변화 학습)</li>
</ul>
<p>이로 인해 전체 파라미터 대비 1~2% 수준만 학습되며, 기존 weight는 그대로 유지됩니다.</p>
<ul>
<li><strong>3. 장점</strong></li>
<li>✅ 효율성: 기존 모델 대비 메모리 사용량 및 연산량 절감</li>
<li>✅ 확장성: 하나의 베이스 모델에 여러 LoRA adapter를 붙여 다중 작업 지원 가능</li>
<li>✅ 경량화된 파인튜닝: 적은 자원으로 빠르게 task-specific 모델 개발 가능</li>
<li>✅ 성능 유지 또는 향상: RoBERTa, DeBERTa, GPT-2/3 등에 적용 시 full fine-tuning 수준의 성능 확보</li>
</ul>
<h3 id="52-instruction-tuning">5.2 Instruction Tuning<a class="headerlink" href="#52-instruction-tuning" title="Permanent link">&para;</a></h3>
<p><strong>Instruction Tuning</strong>은 대형 언어 모델(LLM)을 다양한 명령(Instruction)에 잘 반응하도록 만드는 특수한 파인튜닝(fine-tuning) 기법입니다. 기존의 사전학습(pre-training) 모델이 단순히 다음 단어 예측에 최적화되어 있다면, Instruction Tuning을 거친 모델은 사용자의 명확한 지시(예: “요약해줘”, “단계별로 설명해줘”)를 더 잘 이해하고, 다양한 작업을 수행할 수 있게 됩니다[1][2][3].</p>
<h4 id="1_2">1. 개념 및 목적<a class="headerlink" href="#1_2" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>Instruction Tuning</strong>은 (Instruction, Input, Output) 형태의 데이터셋을 이용해 LLM을 추가 학습시키는 과정입니다.</li>
<li><em>Instruction</em>: 사람이 자연어로 작성한 명령(예: “이 글을 요약하라”)</li>
<li><em>Input</em>: 명령의 대상이 되는 입력 데이터(예: 요약할 본문)</li>
<li><em>Output</em>: 명령과 입력에 대한 바람직한 결과(예: 요약문)</li>
<li>목적은 모델이 다양한 작업에 대해 “설명만 보고” 적절한 행동을 하도록 만드는 데 있습니다[1][2].</li>
</ul>
<h4 id="2_2">2. 기존 파인튜닝과의 차이점<a class="headerlink" href="#2_2" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>구분</th>
<th>일반 파인튜닝(SFT)</th>
<th>Instruction Tuning</th>
</tr>
</thead>
<tbody>
<tr>
<td>데이터</td>
<td>입력-출력 쌍</td>
<td>명령-입력-출력 쌍</td>
</tr>
<tr>
<td>목표</td>
<td>특정 작업 성능 향상</td>
<td>다양한 명령에 대한 범용성 향상</td>
</tr>
<tr>
<td>예시</td>
<td>번역, 감정분석 등 단일 작업</td>
<td>요약, 번역, QA 등 다중 작업</td>
</tr>
<tr>
<td>일반화 능력</td>
<td>제한적</td>
<td>높음 (zero/few-shot 가능)</td>
</tr>
</tbody>
</table>
<p>Instruction Tuning은 다양한 명령을 학습함으로써, 새로운 작업에 대한 일반화(zero-shot/few-shot 성능)를 크게 높입니다[3][4][5].</p>
<h4 id="3_2">3. 데이터셋 구성과 학습 방식<a class="headerlink" href="#3_2" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>Instruction Dataset</strong>: 다양한 명령과 그에 대한 입력/출력 예시로 구성</li>
<li>수작업 또는 LLM을 활용해 생성</li>
<li>FLAN, Alpaca, Self-Instruct 등 대표적 공개 데이터셋 존재[2][6]</li>
<li><strong>학습 방식</strong>: 지도학습(Supervised Learning)으로, 주어진 명령과 입력에 대해 정답 출력을 예측하도록 모델을 미세조정</li>
</ul>
<h4 id="4">4. 효과와 특징<a class="headerlink" href="#4" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>명령 이해 및 수행 능력 강화</strong>: 사용자의 다양한 프롬프트에 더 정확하게 반응</li>
<li><strong>복잡한 요청 처리</strong>: 다단계 지시, 특정 형식/톤 유지 등 복합적 요구에 대응[3]</li>
<li><strong>범용성</strong>: 새로운 작업이나 도메인에 빠르게 적응(zero-shot/few-shot)</li>
<li><strong>일관성 및 신뢰성 향상</strong>: 유사한 요청에 대해 일관된 결과 제공</li>
</ul>
<h4 id="5_1">5. 실제 적용 예시<a class="headerlink" href="#5_1" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>대화형 AI</strong>: 사용자의 질문, 요약 요청, 번역, 단계별 설명 등 다양한 명령에 효과적으로 대응</li>
<li><strong>특정 도메인 특화</strong>: 법률, 의료, 영업 등 특정 분야의 명령-응답 쌍으로 튜닝 시 해당 분야에 특화된 AI 비서 구현[7]</li>
</ul>
<h4 id="6">6. 한계와 과제<a class="headerlink" href="#6" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>고품질 Instruction 데이터셋 구축의 어려움</strong>: 다양한 작업과 명령을 포괄하는 데이터셋이 필요[2]</li>
<li><strong>표면적 패턴 학습의 한계</strong>: 명령의 의미를 깊이 이해하기보다는 형식적 패턴에 치중할 수 있음</li>
<li><strong>도메인 편향</strong>: 학습 데이터에 없는 새로운 명령에는 여전히 약점을 보일 수 있음</li>
</ul>
<h4 id="_8">요약<a class="headerlink" href="#_8" title="Permanent link">&para;</a></h4>
<p>Instruction Tuning은 LLM이 다양한 자연어 명령을 이해하고 수행할 수 있도록 만드는 핵심 기법입니다. 이를 통해 범용적이고 유연한 AI 시스템을 구현할 수 있으며, 실제로 많은 최신 LLM(ChatGPT, Llama-2-Chat 등)이 이 과정을 거쳐 출시되고 있습니다[1][2][3].</p>
<p>출처
[1] What Is Instruction Tuning? | IBM https://www.ibm.com/think/topics/instruction-tuning
[2] Instruction Tuning for Large Language Models: A Survey - arXiv https://arxiv.org/html/2308.10792v5
[3] Base LLM vs. instruction-tuned LLM - Toloka https://toloka.ai/blog/base-llm-vs-instruction-tuned-llm/
[4] Instruction Tuning: What is fine-tuning? https://datascientest.com/en/instruction-tuning-what-is-fine-tuning
[5] Difference between Fine-Tuning, Supervised ... https://www.geeksforgeeks.org/difference-between-fine-tuning-supervised-fine-tuning-sft-and-instruction-fine-tuning/
[6] How to Fine-Tune an LLM Part 1: Preparing a Dataset for Instruction ... https://wandb.ai/capecape/alpaca_ft/reports/How-to-Fine-Tune-an-LLM-Part-1-Preparing-a-Dataset-for-Instruction-Tuning--Vmlldzo1NTcxNzE2
[7] 10 Examples of Instruction Tuning of LLMs - Incubity by Ambilio https://incubity.ambilio.com/10-examples-of-instruction-tuning-of-llms/
[8] [LLM study] 5. Instruction Tuning - velog https://velog.io/@zvezda/LLM-study-5
[9] Instruction tuning : LLM이 사람 말을 알아 듣는 방법 https://devocean.sk.com/blog/techBoardDetail.do?ID=165806&amp;boardType=techBlog
[10] 얼렁뚱땅 LLM을 만들어보자 [3/3] - zzaebok https://zzaebok.github.io/machine_learning/nlp/llm-finetune/
[11] Fine-tuning large language models (LLMs) in 2025 - SuperAnnotate https://www.superannotate.com/blog/llm-fine-tuning
[12] Difference between Instruction Tuning vs Non ... https://stackoverflow.com/questions/76451205/difference-between-instruction-tuning-vs-non-instruction-tuning-large-language-m
[13] Instruction Tuning with LLM - Kaggle https://www.kaggle.com/code/lonnieqin/instruction-tuning-with-llm
[14] Instruction Tuning이란? - velog https://velog.io/@nellcome/Instruction-Tuning%EC%9D%B4%EB%9E%80
[15] What is the difference between pre-training, fine-tuning ... https://www.reddit.com/r/learnmachinelearning/comments/19f04y3/what_is_the_difference_between_pretraining/
[16] Supervised fine-tuning (SFT), instruction fine-tuning and ... https://community.deeplearning.ai/t/supervised-fine-tuning-sft-instruction-fine-tuning-and-full-fine-tuning/527651</p>
<h3 id="53-supervised-fine-tuning-vs-instruction-tuning">5.3 Supervised Fine-Tuning vs Instruction Tuning<a class="headerlink" href="#53-supervised-fine-tuning-vs-instruction-tuning" title="Permanent link">&para;</a></h3>
<h4 id="_9">데이터 구조 차이<a class="headerlink" href="#_9" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>Supervised Fine-Tuning(SFT)</strong>  </li>
<li>데이터는 <em>입력(input)</em>과 그에 대응되는 <em>정답(output)</em> 쌍의 형태로 구성됩니다.  </li>
<li>예시: 텍스트 분류라면 [입력 문장, 분류 라벨], 번역에서는 [원문, 번역문]처럼 각각 특정 태스크에 정답을 직접 제공하는 방식입니다.  </li>
<li>각 샘플은 명확한 태스크와 목표가 있으며 task-specific dataset을 사용합니다[2][3][7].</li>
<li><strong>Instruction Tuning</strong></li>
<li>데이터는 <em>Instruction(지시문)</em>, <em>입력(input)</em>, <em>출력(output)</em> 삼중 구조 또는 [instruction, input, output]의 형태를 가집니다.  </li>
<li>여기서 instruction은 모델이 무엇을 해야 하는지 자연어 안내(예: “다음 문장을 프랑스어로 번역하시오”)를 포함합니다.  </li>
<li>데이터셋은 다양한 행동 예시와 이에 대한 지시문-출력 쌍을 담고 있어서 여러 태스크를 포괄할 수 있는 형태입니다[3][5][6][7].</li>
</ul>
<h4 id="_10">목표 차이<a class="headerlink" href="#_10" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>Supervised Fine-Tuning(SFT)</strong>  </li>
<li>한 가지 명확한 태스크에서 성능을 높이는 것(예: 특정 도메인의 문서 분류, 정보 추출 등)이 목표입니다.  </li>
<li>모델은 주어진 입력에 대해 올바른 정답을 예측하도록 학습됩니다.  </li>
<li>
<p>일반적으로 single-task 방식이며, 도메인 특화 능력 향상에 집중합니다[2][7].</p>
</li>
<li>
<p><strong>Instruction Tuning</strong></p>
</li>
<li>모델이 “지시문”을 이해하고, 다양한 태스크와 요청에 맞게 보다 유연하게 행동할 수 있도록 만드는 것이 목표입니다.  </li>
<li>한 가지 태스크가 아니라 “프롬프트(요청)”에 따라 다양한 태스크를 처리하고 응답하는 능력을 강화합니다.  </li>
<li>multitask·다목적 대응과 일반화(Generalization) 능력, 사용자의 프롬프트 요청 해석 능력 개선이 핵심입니다[2][3][5][6].</li>
</ul>
<p>요약하면, SFT는 “정확한 정답을 맞추는” 단일 태스크 중심이고, Instruction Tuning은 “명시적 지시를 따라 폭넓은 태스크에 대응하는” 멀티태스크 및 프롬프트 중심 구조와 목적을 가진다고 할 수 있습니다.</p>
<p>출처
[1] Supervised fine-tuning (SFT), instruction fine-tuning and full fine-tuning https://community.deeplearning.ai/t/supervised-fine-tuning-sft-instruction-fine-tuning-and-full-fine-tuning/527651
[2] Difference between Fine-Tuning, Supervised fine-tuning (SFT) and ... https://www.geeksforgeeks.org/artificial-intelligence/difference-between-fine-tuning-supervised-fine-tuning-sft-and-instruction-fine-tuning/
[3] Instruction Tuning Vol. 1 - by Sebastian Ruder - NLP News https://newsletter.ruder.io/p/instruction-tuning-vol-1
[4] Difference between Instruction Tuning vs Non ... - Stack Overflow https://stackoverflow.com/questions/76451205/difference-between-instruction-tuning-vs-non-instruction-tuning-large-language-m
[5] Instruction Tuning: What is fine-tuning? - DataScientest https://datascientest.com/en/instruction-tuning-what-is-fine-tuning
[6] What Is Instruction Tuning? | IBM https://www.ibm.com/think/topics/instruction-tuning
[7] What is supervised fine-tuning in LLMs? Unveiling the process https://nebius.com/blog/posts/fine-tuning/supervised-fine-tuning
[8] What is the difference between pre-training, fine-tuning, and instruct ... https://www.reddit.com/r/learnmachinelearning/comments/19f04y3/what_is_the_difference_between_pretraining/</p>
<hr />
<h2 id="6-self-supervised-learning">6. 자기지도 학습(Self-Supervised Learning) 구조<a class="headerlink" href="#6-self-supervised-learning" title="Permanent link">&para;</a></h2>
<ul>
<li>label없는 unsupervised learning에 속하여, 스스로 task를 만들어서 모델을 학습함.</li>
<li>목적</li>
<li>unlabelled dataset으로 부터 좋은 representation을 얻고자 하는 학습 방식, representation learing의 일종</li>
<li>
<p>label(y)없이 input(x)내에서 target으로 쓰일만한 것을 self로 task를 정해서 모델 학습</p>
</li>
<li>
<p>SimCLR
https://kyujinpy.tistory.com/39</p>
</li>
</ul>
<h3 id="simclr"><strong>SimCLR</strong><a class="headerlink" href="#simclr" title="Permanent link">&para;</a></h3>
<p>A Simple Framework for Contrastive Learning of Visual Representations</p>
<ul>
<li>이미지 분류</li>
<li>핵심 아이디어 : 유사한 이미지와 다른 이미지를 생성 후, 유사한 이미지는 feature space에 가깝도록, 다른 이미지는 멀도록 학습</li>
<li>loss function
<img alt="image.png" src="attachment:692747ca-88a3-4d55-8e29-1e777abdd761:image.png" /><ul>
<li>분자: positive sample(유사한샘플)간의 유사도</li>
<li>분모: negative sample(다른샘플)간의 유사도 </li>
</ul>
</li>
</ul>
<hr />
<h3 id="byol"><strong>BYOL</strong><a class="headerlink" href="#byol" title="Permanent link">&para;</a></h3>
<p>Bootstrap your own latent: A new approach to self-supervised Learning</p>
<ul>
<li>이미지 분류</li>
<li>핵심 아이디어: negative sample없이, positivie sample만 활용하여 지도학습과 비교가능한 성능 보임</li>
<li>BYOL은 같은 이미지를 다르게 변형한 두 개를 보고,  “이 둘이 같다고 생각하게” 모델을 훈련</li>
<li>online, target network로 구성</li>
<li>online network: representation을 구성/ 학습의 주체</li>
<li>target network: 학습의 기준 제공.
<img alt="image.png" src="attachment:6825192d-8ae5-4f5a-97ba-0629a0a1edc6:image.png" /></li>
<li>온라인 네트워크가 경사하강법을 통해 업데이트</li>
<li>타켓 네트워크는 온라인 네트워크의 파라미터를 이용한 exponential moving average(EMA) 방식으로 업데이트</li>
<li>논문리뷰 링크 : https://kyujinpy.tistory.com/44</li>
</ul>
<hr />
<h3 id="jigsaw-puzzle"><strong>jigsaw puzzle</strong><a class="headerlink" href="#jigsaw-puzzle" title="Permanent link">&para;</a></h3>
<ul>
<li>핵심아이디어 : 레이블 없이도 이미지를 무작위로 섞은 퍼즐을 맞추는 과제를 통해, 모델이 이미지의 공간 구조와 의미적 특징을 학습하게 하는 방식</li>
</ul>
<hr />
<h3 id="maemasked-auto-encoder"><strong>MAE(masked auto encoder)</strong><a class="headerlink" href="#maemasked-auto-encoder" title="Permanent link">&para;</a></h3>
<ul>
<li>입력 이미지에 랜덤 패치 마스킹→ 픽셀 공간에서 누락된 패치 재구성</li>
<li>MAE는 비대칭 인코더-디코더 형식<ul>
<li>인코더는 (마스크 토큰 없이) 보이는 패치의 부분집합에서만 작동하며</li>
<li>디코더는 가볍고 마스크 토큰과 함께 latent 표현에서 입력을 재구성</li>
</ul>
</li>
<li>논문 리뷰 링크 : <a href="https://kimjy99.github.io/%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0/mae/">Masked Autoencoders Are Scalable Vision Learners (MAE)</a></li>
</ul>
<h3 id="rotnet"><strong>RotNet</strong><a class="headerlink" href="#rotnet" title="Permanent link">&para;</a></h3>
<ul>
<li>이미지 회전량을 맞추는 모델 학습</li>
<li><img alt="image.png" src="attachment:56c7bfec-147a-4caf-a48c-afd8589da538:image.png" /></li>
</ul>
<hr />
<h2 id="7-unsupervised-learning">7. 비지도 학습(Unsupervised Learning) 구조<a class="headerlink" href="#7-unsupervised-learning" title="Permanent link">&para;</a></h2>
<h3 id="unsupervised-learning">비지도 학습(Unsupervised Learning) 개념<a class="headerlink" href="#unsupervised-learning" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>비지도 학습</strong>은 정답(라벨)이 없는 데이터로부터 패턴이나 구조를 스스로 발견하는 머신러닝 방법입니다. 즉, 입력 데이터만 주어지고, 데이터 내의 숨겨진 관계나 그룹, 특징을 찾아내는 것이 목적입니다[1][2][3].</li>
<li>지도학습(Supervised Learning)과 달리, 목표값(정답)이 주어지지 않으므로 데이터의 분포, 유사성, 특징 등을 기반으로 학습을 진행합니다.</li>
<li>주요 활용 분야로는 데이터의 <strong>군집화(Clustering)</strong>, <strong>차원 축소(Dimensionality Reduction)</strong>, <strong>이상치 탐지(Anomaly Detection)</strong> 등이 있습니다[2][4].</li>
</ul>
<h3 id="_11">비지도 학습의 대표적 예시<a class="headerlink" href="#_11" title="Permanent link">&para;</a></h3>
<table>
<thead>
<tr>
<th>알고리즘/분야</th>
<th>설명 및 활용 예시</th>
</tr>
</thead>
<tbody>
<tr>
<td>Clustering(군집화)</td>
<td>- 유사한 데이터를 그룹으로 묶음<br>- 예: 고객을 구매 패턴에 따라 그룹화, MBTI 유형 분류, 이미지 내 유사한 얼굴 그룹화[2][5]</td>
</tr>
<tr>
<td>Dimensionality Reduction(차원 축소)</td>
<td>- 데이터의 차원을 줄여서 본질적인 특징만 추출<br>- 예: PCA(주성분 분석)로 데이터 시각화, 노이즈 제거, 텍스트 주제 추출[2][6]</td>
</tr>
<tr>
<td>Anomaly Detection(이상치 탐지)</td>
<td>- 정상 패턴에서 벗어난 데이터 탐지<br>- 예: 금융 거래에서 이상 거래 탐지, 네트워크 보안에서 비정상 트래픽 탐지[2]</td>
</tr>
<tr>
<td>Generative Models(생성 모델)</td>
<td>- 새로운 데이터를 생성<br>- 예: GAN을 활용한 가짜 이미지 생성, VAE를 통한 데이터 샘플링[4]</td>
</tr>
<tr>
<td>Self-supervised Learning(자기 지도 학습)</td>
<td>- 데이터의 일부를 예측하도록 학습하여 유의미한 표현 학습<br>- 예: 문장 내 단어 예측, 이미지의 일부 복원[4]</td>
</tr>
</tbody>
</table>
<h3 id="_12">실제 활용 사례<a class="headerlink" href="#_12" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>고객 세그먼테이션</strong>: 마케팅에서 고객 데이터를 군집화하여 맞춤형 전략 수립[2].</li>
<li><strong>이미지 압축 및 시각화</strong>: 고차원 이미지 데이터를 PCA 등으로 차원 축소 후 시각화[2][4].</li>
<li><strong>추천 시스템</strong>: 사용자 행동 데이터를 분석해 유사한 사용자 그룹을 찾아 콘텐츠 추천[2].</li>
<li><strong>이상 탐지</strong>: 신용카드 거래, 네트워크 트래픽 등에서 정상 패턴과 다른 이상치 탐지[2].</li>
</ul>
<h3 id="_13">비지도 학습의 특징<a class="headerlink" href="#_13" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>라벨 없이 데이터의 구조, 패턴, 유사성 등을 스스로 파악</strong>합니다.</li>
<li><strong>평가가 어렵고 결과 해석이 쉽지 않다는 한계</strong>가 있지만, 데이터 탐색 및 전처리, 새로운 패턴 발견 등에 매우 유용하게 사용됩니다[2][6].</li>
</ul>
<p>비지도 학습은 데이터 내재적 구조를 파악하고, 라벨이 없는 환경에서도 의미 있는 정보를 추출할 수 있는 강력한 머신러닝 방법론입니다.</p>
<p>출처
[1] 비지도학습(Unsupervised Learning)에 대하여 - velog https://velog.io/@deep_lini/%EB%B9%84%EC%A7%80%EB%8F%84%ED%95%99%EC%8A%B5Unsupervised-Learning%EC%97%90-%EB%8C%80%ED%95%98%EC%97%AC
[2] Unsupervised Learning (비지도 학습) 이란? - Data Science Diary https://datasciencediary.tistory.com/entry/Unsupervised-Learning-%EB%B9%84%EC%A7%80%EB%8F%84-%ED%95%99%EC%8A%B5-%EC%9D%B4%EB%9E%80
[3] unsupervised learning (비지도 학습) - 위키독스 https://wikidocs.net/120198
[4] 정리 : Deep Unsupervised Learning - Research Blog https://animilux.github.io/study/2021/01/29/unsupervised_learning.html
[5] [머신러닝 기초] 비지도학습(Unsupervised-learning) - 군집화 ... https://ai-creator.tistory.com/591
[6] Unsupervised Learning사례, data preprocessing - Dev https://dev-jm.tistory.com/31
[7] 비지도 학습(Unsupervised Learning) 이해를 돕는 심플 가이드 - Appier https://www.appier.com/ko-kr/blog/a-simple-guide-to-unsupervised-learning
[8] [머신러닝 순한맛] 비지도 학습(Unsupervised Learning)이란? : K ... https://box-world.tistory.com/30
[9] Supervised Learning과 Unsupervised Learning 차이 - 유니의 공부 https://process-mining.tistory.com/98
[10] 앤드류 응의 머신러닝 (1-4) : 비지도 학습 https://brunch.co.kr/@linecard/440
[11] [기계학습] Supervised Learning &amp; Unsupervised Learning https://velog.io/@tjswodud/%EA%B8%B0%EA%B3%84%ED%95%99%EC%8A%B5-Supervised-Learning-Unsupervised-Learning
[12] 머신러닝 개요, 비지도 학습(Unsupervised Learning) https://sandol20.tistory.com/147
[13] [머신러닝 순한맛] 비지도 학습(Unsupervised Learning)이란? : 최적화 ... https://box-world.tistory.com/31
[14] 지도 학습 vs 비지도 학습 (Supervised learning ... - CAI - 티스토리 https://kjh-ai-blog.tistory.com/12
[15] 09장. 비지도 학습(Unsupervised Learning) https://wikidocs.net/145557</p>
<hr />
<h2 id="8">8. 영상 처리용 아키텍처<a class="headerlink" href="#8" title="Permanent link">&para;</a></h2>
<table>
<thead>
<tr>
<th>모델 아키텍처</th>
<th>처리 구조 요약</th>
<th>시간 정보 처리</th>
<th>활용 예시</th>
<th>장단점 요약</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>CNN</strong></td>
<td>단일 이미지 처리</td>
<td>❌ 없음</td>
<td>객체 검출, 얼굴 인식</td>
<td>✅ 빠르고 단순<br>❌ 시간 정보 반영 불가</td>
</tr>
<tr>
<td><strong>3D CNN / C3D</strong></td>
<td>3D 커널로 짧은 프레임 시퀀스 처리</td>
<td>✅ 짧은 시퀀스</td>
<td>동작 인식, 스포츠 영상 분석</td>
<td>✅ 시간+공간 통합<br>❌ 고정된 길이 필요</td>
</tr>
<tr>
<td><strong>ConvLSTM</strong></td>
<td>CNN → LSTM 결합 구조로 시공간 정보 분리 처리</td>
<td>✅ 장기 시퀀스</td>
<td>이상 행동 탐지, 의료 영상</td>
<td>✅ 연속성 표현 가능<br>❌ 학습 속도 느림</td>
</tr>
<tr>
<td><strong>Two-Stream CNN</strong></td>
<td>RGB 이미지 + Optical Flow 2경로 처리</td>
<td>✅ 움직임 추출</td>
<td>액션 인식, 보안 영상 분석</td>
<td>✅ 움직임 민감<br>❌ Preprocessing 시간 요구</td>
</tr>
<tr>
<td><strong>I3D</strong></td>
<td>기존 2D CNN을 3D로 확장하여 영상 클립 처리</td>
<td>✅ 짧은 시퀀스</td>
<td>비디오 분류, Kinetics 영상 분석</td>
<td>✅ 사전학습 사용 가능<br>❌ 고사양 자원 필요</td>
</tr>
<tr>
<td><strong>Video Transformer (TimeSformer 등)</strong></td>
<td>Transformer를 시공간 입력에 적용</td>
<td>✅ 긴 시퀀스</td>
<td>긴 영상 분석, 이벤트 감지</td>
<td>✅ 글로벌 표현력 뛰어남<br>❌ 계산량 매우 큼</td>
</tr>
<tr>
<td><strong>SlowFast Network</strong></td>
<td>빠른 변화/느린 변화 2개의 시퀀스로 병렬 처리</td>
<td>✅ 이중 시퀀스</td>
<td>행동 분석, 자율주행</td>
<td>✅ 다양한 속도 감지<br>❌ 구현 복잡</td>
</tr>
</tbody>
</table>
<hr />
<h3 id="1-cnn-convolutional-neural-network">1️⃣ CNN (Convolutional Neural Network)<a class="headerlink" href="#1-cnn-convolutional-neural-network" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>설명:</strong>  </li>
<li>고전적인 이미지 인식에서 사용되는 구조.</li>
<li>컨볼루션 필터를 통해 공간적인 특징(경계, 모양, 색상 등)을 추출.</li>
<li>시간 개념은 전혀 고려되지 않음.</li>
<li><strong>활용 예:</strong>  </li>
<li>단일 프레임 기반 CCTV 이벤트 탐지, 얼굴 감지, 번호판 인식 등.</li>
<li><strong>한계:</strong>  </li>
<li>움직임 기반 이벤트(예: 싸움, 넘어짐 등)의 감지 불가능.</li>
<li>시간에 따른 변화 학습 불가.</li>
</ul>
<hr />
<h3 id="2-3d-cnn-c3d-convolutional-3d-networks">2️⃣ 3D CNN / C3D (Convolutional 3D Networks)<a class="headerlink" href="#2-3d-cnn-c3d-convolutional-3d-networks" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>설명:</strong>  </li>
<li>입력 영상이 3차원 텐서 (프레임 수, 높이, 너비)로 구성되며, 커널도 3D로 확장됨.</li>
<li>프레임 간 움직임 및 공간 패턴을 동시에 학습.</li>
<li><strong>대표 모델:</strong>  </li>
<li>C3D (Facebook, 2015)</li>
<li><strong>활용 예:</strong>  </li>
<li>스포츠 행동 분석, 단기 동작 인식 (예: 손 흔들기, 박수 등).</li>
<li><strong>한계:</strong>  </li>
<li>고정된 프레임 수(예: 16)로 입력을 구성해야 함.</li>
<li>긴 시퀀스를 처리하거나 프레임 수가 가변적인 경우 처리 어렵다.</li>
</ul>
<hr />
<h3 id="3-convlstm-convolutional-long-short-term-memory">3️⃣ ConvLSTM (Convolutional Long Short-Term Memory)<a class="headerlink" href="#3-convlstm-convolutional-long-short-term-memory" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>설명:</strong>  </li>
<li>CNN으로 각 프레임의 공간 정보를 추출하고, 그 결과를 LSTM에 순차적으로 입력해 시간 흐름을 학습.</li>
<li>시간과 공간 정보를 <strong>분리하여 단계적으로</strong> 처리함.</li>
<li><strong>대표 논문:</strong>  </li>
<li>Shi et al., “Convolutional LSTM Network for Precipitation Nowcasting”, NIPS 2015</li>
<li><strong>활용 예:</strong>  </li>
<li>이상 행동 감지, 의료 영상 시퀀스 분석, 드론/위성영상 예측.</li>
<li><strong>장점:</strong>  </li>
<li>긴 시퀀스에 대해 유연하게 대응 가능.</li>
<li>CNN을 통해 영상 처리에 최적화된 구조.</li>
<li><strong>단점:</strong>  </li>
<li>학습 시간 길고, 연산량이 많음.</li>
<li>고해상도 영상에서는 GPU 메모리 부담.</li>
</ul>
<hr />
<h3 id="4-two-stream-cnn">4️⃣ Two-Stream CNN<a class="headerlink" href="#4-two-stream-cnn" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>설명:</strong>  </li>
<li>하나의 네트워크는 RGB 프레임을 입력받고, 다른 하나는 Optical Flow (움직임 벡터)를 입력받음.</li>
<li>두 결과를 결합하여 예측.</li>
<li><strong>대표 논문:</strong>  </li>
<li>Simonyan &amp; Zisserman, “Two-Stream Convolutional Networks for Action Recognition”, NIPS 2014</li>
<li><strong>활용 예:</strong>  </li>
<li>스포츠 영상 분석, 동작 분류.</li>
<li><strong>장점:</strong>  </li>
<li>움직임과 모양 정보를 분리 학습 가능.</li>
<li><strong>단점:</strong>  </li>
<li>Optical Flow 사전 계산 필요 → 느림.</li>
<li>실시간 분석에는 부적합.</li>
</ul>
<hr />
<h3 id="5-i3d-inflated-3d-convnet">5️⃣ I3D (Inflated 3D ConvNet)<a class="headerlink" href="#5-i3d-inflated-3d-convnet" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>설명:</strong>  </li>
<li>기존 2D CNN (ex: Inception-V1)을 3D 커널로 "Inflate"하여 시공간 처리 가능하도록 확장.</li>
<li>사전 학습된 2D 모델 파라미터를 3D 모델로 이전할 수 있어 효율적.</li>
<li><strong>대표 논문:</strong>  </li>
<li>Carreira &amp; Zisserman, “Quo Vadis, Action Recognition?”, CVPR 2017</li>
<li><strong>활용 예:</strong>  </li>
<li>Kinetics dataset 기반 동작 인식.</li>
<li><strong>장점:</strong>  </li>
<li>대규모 사전 학습 활용 가능.</li>
<li>정확도 높음.</li>
<li><strong>단점:</strong>  </li>
<li>연산 자원 많이 필요 (GPU, 메모리).</li>
<li>학습 속도 느림.</li>
</ul>
<hr />
<h3 id="6-video-transformer-timesformer-vivit">6️⃣ Video Transformer (TimeSformer, ViViT 등)<a class="headerlink" href="#6-video-transformer-timesformer-vivit" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>설명:</strong>  </li>
<li>Vision Transformer의 구조를 영상에 확장.</li>
<li>프레임의 공간/시간 관계를 self-attention으로 모델링함.</li>
<li>시간 위치와 공간 위치 정보를 positional embedding으로 부여.</li>
<li><strong>대표 논문:</strong>  </li>
<li>TimeSformer (Bertasius et al., CVPR 2021), ViViT (Arnab et al., 2021)</li>
<li><strong>활용 예:</strong>  </li>
<li>긴 영상 속 이벤트 탐지, 광고 검열, 영화 장면 분류 등.</li>
<li><strong>장점:</strong>  </li>
<li>멀리 떨어진 프레임 간 관계까지 학습 가능.</li>
<li>스케일이 큰 모델에도 유연하게 대응.</li>
<li><strong>단점:</strong>  </li>
<li>매우 높은 연산 요구 (GPU 클러스터 필요).</li>
<li>작은 데이터셋에는 과적합 위험 있음.</li>
</ul>
<hr />
<h3 id="7-slowfast-networks">7️⃣ SlowFast Networks<a class="headerlink" href="#7-slowfast-networks" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>설명:</strong>  </li>
<li>서로 다른 프레임 속도의 두 가지 입력 스트림 사용.</li>
<li>느린 스트림은 전체 컨텍스트 유지, 빠른 스트림은 세밀한 움직임 포착.</li>
<li><strong>대표 논문:</strong>  </li>
<li>Feichtenhofer et al., “SlowFast Networks for Video Recognition”, ICCV 2019</li>
<li><strong>활용 예:</strong>  </li>
<li>자율주행 영상, 감시 시스템, 스포츠 영상 인식 등.</li>
<li><strong>장점:</strong>  </li>
<li>다양한 속도의 움직임 표현에 강함.</li>
<li><strong>단점:</strong>  </li>
<li>구현 복잡, 메모리 사용량 높음.</li>
</ul>
<hr />
<h3 id="_14">🧭 시나리오별 추천 모델 요약<a class="headerlink" href="#_14" title="Permanent link">&para;</a></h3>
<table>
<thead>
<tr>
<th>분석 시나리오</th>
<th>추천 모델</th>
<th>이유</th>
</tr>
</thead>
<tbody>
<tr>
<td>단일 이미지 분석 (정지 이미지)</td>
<td>CNN</td>
<td>간단하고 빠르며 잘 알려져 있음</td>
</tr>
<tr>
<td>짧은 동작 분석 (예: 손짓, 점프)</td>
<td>3D CNN, C3D, I3D</td>
<td>프레임 간 공간+시간 패턴을 통합 처리 가능</td>
</tr>
<tr>
<td>이상 행동 감지 (시간 연속성 중요)</td>
<td>✅ ConvLSTM</td>
<td>프레임 간의 시퀀스를 고려한 시공간 분석에 최적</td>
</tr>
<tr>
<td>빠른 움직임 분석 (예: 격투, 추락)</td>
<td>Two-Stream, SlowFast</td>
<td>움직임 정보 명시적 활용, 속도 구간 분리 가능</td>
</tr>
<tr>
<td>긴 영상 분석 (예: 영화, 스포츠)</td>
<td>Video Transformer</td>
<td>전체 맥락 이해 및 장거리 의존 관계 표현 가능</td>
</tr>
</tbody>
</table>
<hr />
<h3 id="_15">🔗 참고 문헌<a class="headerlink" href="#_15" title="Permanent link">&para;</a></h3>
<ul>
<li>Shi et al. (2015), <a href="https://arxiv.org/abs/1506.04214">ConvLSTM</a></li>
<li>Carreira &amp; Zisserman (2017), <a href="https://arxiv.org/abs/1705.07750">I3D</a></li>
<li>Bertasius et al. (2021), <a href="https://arxiv.org/abs/2102.05095">TimeSformer</a></li>
<li>Feichtenhofer et al. (2019), <a href="https://arxiv.org/abs/1812.03982">SlowFast</a></li>
<li>Simonyan &amp; Zisserman (2014), <a href="https://arxiv.org/abs/1412.0767">Two-Stream CNN</a></li>
</ul>
<hr />
<h2 id="9">9. 앙상블 구조<a class="headerlink" href="#9" title="Permanent link">&para;</a></h2>
<p>앙상블 학습(Ensemble Learning)은 여러 개별 모델의 예측을 조합해 단일 모델보다 향상된 정확도와 일반화 성능을 달성하는 기법입니다. 이는 "집단 지성" 원리를 적용해 편향(Bias)과 분산(Variance)을 동시에 최적화합니다[1][2][3]. 주요 기법인 배깅(Bagging), 부스팅(Boosting), 스태킹(Stacking)의 작동 원리와 차이는 다음과 같습니다.</p>
<p><a href="https://brunch.co.kr/@chris-song/98">참고 블로그</a></p>
<h3 id="1-bagging">1. 배깅(Bagging): 분산 감소 중심<a class="headerlink" href="#1-bagging" title="Permanent link">&para;</a></h3>
<blockquote>
<p><strong>Bootstrap Aggregating</strong>의 약자로, 고분산 모델의 과적합을 방지합니다[4][5][6].</p>
</blockquote>
<ul>
<li><strong>작동 방식</strong>:<br />
  훈련 데이터를 중복 허용 무작위 샘플링(부트스트랩)으로 여러 부분집합 생성 → 각 부분집합으로 독립적인 모델(주로 결정 트리) 훈련 → 평균(회귀) 또는 다수결(분류)로 예측 통합[4][5].</li>
<li><strong>핵심 효과</strong>:<br />
  모델의 <strong>분산을 감소</strong>시켜 안정성 향상. 특히 노이즈가 많은 데이터에서 효과적[5][6].</li>
<li><strong>대표 알고리즘</strong>:<br />
  Random Forest (의사결정나무 기반 배깅의 확장)[4][6].</li>
</ul>
<h3 id="2-boosting">2. 부스팅(Boosting): 편향 감소 중심<a class="headerlink" href="#2-boosting" title="Permanent link">&para;</a></h3>
<blockquote>
<p>순차적 학습으로 약한 학습기(Weak Learner)를 강한 모델로 발전시킵니다[7][8].</p>
</blockquote>
<ul>
<li><strong>작동 방식</strong>:<br />
  초기 모델이 잘못 예측한 샘플에 가중치 부여 → 새로운 모델이 오류 보정에 집중해 순차적 훈련 → 가중치 기반 예측 통합(예: AdaBoost)[7][8].</li>
<li><strong>핵심 효과</strong>:<br />
  모델의 <strong>편향을 감소</strong>시켜 정확도 향상. 고편향 모델(예: 얕은 결정 트리)에 효과적[7][8].</li>
<li><strong>대표 알고리즘</strong>:<br />
  AdaBoost, Gradient Boosting, XGBoost[8][6].</li>
</ul>
<h3 id="3-stacking">3. 스태킹(Stacking): 비선형 조합<a class="headerlink" href="#3-stacking" title="Permanent link">&para;</a></h3>
<blockquote>
<p>메타-모델(Meta-Model)이 기본 모델의 예측을 최적화해 통합합니다[9][10].</p>
</blockquote>
<ul>
<li><strong>작동 단계</strong>:  </li>
<li>다양한 기본 모델(Base Model: RF, SVM 등) 훈련  </li>
<li>기본 모델들의 예측값을 새로운 입력 데이터로 변환  </li>
<li>메타-모델(Logistic Regression, NN 등)이 이 값을 학습해 최종 예측 생성[9][10].</li>
<li><strong>핵심 효과</strong>:<br />
  기본 모델의 한계를 넘어 <strong>복잡한 패턴 포착</strong> 가능. 이질적 모델 조합에 적합[10][6].</li>
<li><strong>장점</strong>:<br />
  비선형 관계 학습 가능성으로 단순 평균/투표보다 표현력 우수[9][10].</li>
</ul>
<h3 id="_16">비교 표: 앙상블 기법 특성<a class="headerlink" href="#_16" title="Permanent link">&para;</a></h3>
<table>
<thead>
<tr>
<th>특성</th>
<th>배깅(Bagging)</th>
<th>부스팅(Boosting)</th>
<th>스태킹(Stacking)</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>목적</strong></td>
<td>분산 감소</td>
<td>편향 감소</td>
<td>예측 정확도 극대화</td>
</tr>
<tr>
<td><strong>학습 방식</strong></td>
<td>병렬 독립 학습</td>
<td>순차적 오류 보정</td>
<td>메타-모델 통합</td>
</tr>
<tr>
<td><strong>모델 관계</strong></td>
<td>동질적 모델</td>
<td>동질적 모델</td>
<td>이질적 모델 가능</td>
</tr>
<tr>
<td><strong>과적합</strong></td>
<td>덜 취약</td>
<td>취약 (조기 종료 필수)</td>
<td>데이터 양에 영향</td>
</tr>
<tr>
<td><strong>대표 사례</strong></td>
<td>Random Forest</td>
<td>AdaBoost, XGBoost</td>
<td>블렌딩(Blending)</td>
</tr>
</tbody>
</table>
<h3 id="_17">요약<a class="headerlink" href="#_17" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>배깅</strong>: 무작위 샘플링으로 모델 다양성 확보 → <strong>분산 감소</strong>  </li>
<li><strong>부스팅</strong>: 오류 중심 순차 학습 → <strong>편향 감소</strong>  </li>
<li><strong>스태킹</strong>: 메타 모델이 예측을 비선형 통합 → <strong>표현력 극대화</strong>  </li>
</ul>
<p>앙상블 기법 선택은 문제 특성에 따라 달라집니다: 고분산 문제에는 배깅, 고편향 문제에는 부스팅, 복잡한 비선형 관계에는 스태킹이 효과적입니다[5][8][10].</p>
<p>출처
[1] What is ensemble learning? - IBM https://www.ibm.com/think/topics/ensemble-learning
[2] Ensemble learning - Wikipedia https://en.wikipedia.org/wiki/Ensemble_learning
[3] Ensemble Learning: A Combined Prediction Model Guide - viso.ai https://viso.ai/deep-learning/ensemble-learning/
[4] What Is Bagging? | IBM https://www.ibm.com/think/topics/bagging
[5] Introduction to Bagging and Ensemble Methods | Paperspace Blog https://blog.paperspace.com/bagging-ensemble-methods/
[6] Bagging in Machine Learning: Step to Perform and Its Advantages https://www.simplilearn.com/tutorials/machine-learning-tutorial/bagging-in-machine-learning
[7] BOOSTING: Ensemble Learning Method in Machine Learning https://www.youtube.com/watch?v=ikaeV2XA9Kk
[8] What Is Boosting? | IBM https://www.ibm.com/think/topics/boosting
[9] Mastering Complexity: The Comprehensive Guide to Stacking Ensemble Models https://ai.plainenglish.io/mastering-complexity-the-comprehensive-guide-to-stacking-ensemble-models-7c0ef4876eda?gi=6acbc9390e87
[10] Unleashing the Full Potential of Ensemble Stacking in Machine Learning and Deep Learning - 33rd Square https://www.33rdsquare.com/ensemble-stacking-for-machine-learning-and-deep-learning/
[11] A Comprehensive Guide to Ensemble Learning: What Exactly Do ... https://neptune.ai/blog/ensemble-learning-guide
[12] A Data Scientist’s Guide to Ensemble Learning: Techniques, Benefits, and Code https://pub.towardsai.net/a-data-scientists-guide-to-ensemble-learning-techniques-benefits-and-code-2f1d82654fb9?gi=fb251f17d1ea
[13] What is Ensemble Learning? https://www.dremio.com/wiki/ensemble-learning/
[14] A Guide to Bagging in Machine Learning - DataCamp https://www.datacamp.com/tutorial/what-bagging-in-machine-learning-a-guide-with-examples
[15] Ensemble learning: Bagging and Boosting | Towards Data Science https://towardsdatascience.com/ensemble-learning-bagging-and-boosting-23f9336d3cb0/
[16] Ensemble Learning: Bagging, Boosting &amp; Stacking - Kaggle https://www.kaggle.com/code/satishgunjal/ensemble-learning-bagging-boosting-stacking
[17] Ensemble Learning - an overview | ScienceDirect Topics https://www.sciencedirect.com/topics/computer-science/ensemble-learning
[18] 8 Key Advantages of Ensemble Learning in Machine Intelligence https://www.numberanalytics.com/blog/8-key-advantages-ensemble-learning-machine-intelligence
[19] What is Ensemble Learning? https://www.lyzr.ai/glossaries/ensemble-learning/
[20] The Essential Guide to Ensemble Learning https://www.v7labs.com/blog/ensemble-learning-guide</p>
<hr />
<h2 id="10">10. 모델 성능 및 과적합/과소적합<a class="headerlink" href="#10" title="Permanent link">&para;</a></h2>
<ul>
<li>Overfitting &amp; Underfitting 정의 및 원인
<img alt="" src="https://aiml.com/wp-content/uploads/2023/02/overfitting_underfitting.png" /></li>
</ul>
<table>
<thead>
<tr>
<th>구분</th>
<th>정의</th>
<th>주요 원인</th>
<th>해결 방법 및 설명</th>
</tr>
</thead>
<tbody>
<tr>
<td>Overfitting</td>
<td>모델이 학습 데이터에 과도하게 적합하여, 테스트 데이터에 일반화되지 못함</td>
<td>- 모델이 너무 복잡함<br>- 학습 데이터가 부족함<br>- 노이즈에 민감하게 학습함</td>
<td>- <strong>데이터 양 증가</strong>: 더 많은 학습 데이터를 수집하거나 데이터 증강(Data Augmentation)으로 다양성 확보<br>- <strong>정규화(Regularization)</strong>: L1, L2 규제를 통해 과도한 가중치를 억제<br>- <strong>Dropout</strong>: 학습 중 일부 뉴런을 무작위로 꺼서 과적합을 방지<br>- <strong>모델 단순화</strong>: 파라미터 수가 적은 간단한 모델 사용<br>- <strong>조기 종료(Early Stopping)</strong>: 검증 손실이 증가하기 시작하면 학습을 중단하여 과도한 학습 방지</td>
</tr>
<tr>
<td>Underfitting</td>
<td>모델이 학습 데이터의 패턴조차 제대로 학습하지 못함</td>
<td>- 모델이 지나치게 단순함<br>- 학습 시간이 부족함<br>- 중요한 feature 누락</td>
<td>- <strong>더 복잡한 모델 사용</strong>: 층 수나 뉴런 수를 늘린 딥러닝 모델로 전환<br>- <strong>학습 시간 증가</strong>: epoch 수를 늘려 충분히 학습<br>- <strong>feature 엔지니어링</strong>: 중요한 입력 특성 추가 및 전처리 개선<br>- <strong>하이퍼파라미터 튜닝</strong>: 학습률, 배치 크기 등 조절로 학습 성능 개선</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="11">11. 메모리 및 효율성 고려<a class="headerlink" href="#11" title="Permanent link">&para;</a></h2>
<h3 id="oom-out-of-memory">OOM (Out-Of-Memory)<a class="headerlink" href="#oom-out-of-memory" title="Permanent link">&para;</a></h3>
<h4 id="oomout-of-memory">OOM(Out Of Memory)란?<a class="headerlink" href="#oomout-of-memory" title="Permanent link">&para;</a></h4>
<p>OOM(Out Of Memory, 메모리 부족)은 컴퓨터나 GPU가 어떤 작업을 수행하는 데 필요한 메모리를 모두 소진해 더 이상 메모리를 할당할 수 없을 때 발생하는 오류입니다. 이 오류는 물리적 RAM이나 가상 메모리(디스크를 활용해 확장된 메모리)가 모두 부족할 때 나타나며, 프로그램이 중단되거나 시스템이 비정상적으로 종료될 수 있습니다[1][2][3]. 딥러닝에서는 특히 대용량 모델이나 데이터셋을 사용할 때 자주 발생합니다.</p>
<h4 id="cuda-oomout-of-memory">CUDA OOM(Out Of Memory) 완화 방법<a class="headerlink" href="#cuda-oomout-of-memory" title="Permanent link">&para;</a></h4>
<p>딥러닝에서 GPU를 사용할 때 발생하는 CUDA OOM 오류는 GPU 메모리가 부족해 텐서나 모델 파라미터를 할당하지 못할 때 발생합니다. 이를 완화하기 위한 주요 방법을 정리하면 다음과 같습니다.</p>
<h5 id="1-batch-size">1. <strong>배치 크기(batch size) 줄이기</strong><a class="headerlink" href="#1-batch-size" title="Permanent link">&para;</a></h5>
<ul>
<li>한 번에 처리하는 데이터 양을 줄이면 필요한 GPU 메모리도 줄어듭니다. 가장 기본적이고 효과적인 방법입니다[4][5][6][7].</li>
</ul>
<h5 id="2_3">2. <strong>모델 크기 축소</strong><a class="headerlink" href="#2_3" title="Permanent link">&para;</a></h5>
<ul>
<li>레이어 수나 파라미터 수를 줄여 모델 자체가 사용하는 메모리를 줄입니다[4][5].</li>
</ul>
<h5 id="3-mixed-precision-training">3. <strong>혼합 정밀도 학습(Mixed Precision Training)</strong><a class="headerlink" href="#3-mixed-precision-training" title="Permanent link">&para;</a></h5>
<ul>
<li>float32 대신 float16 등 더 작은 데이터 타입을 사용해 메모리 사용량을 줄일 수 있습니다. PyTorch의 <code>torch.cuda.amp</code> 등을 활용합니다[4][5].</li>
</ul>
<h5 id="4_1">4. <strong>불필요한 변수/텐서 삭제</strong><a class="headerlink" href="#4_1" title="Permanent link">&para;</a></h5>
<ul>
<li>사용이 끝난 변수나 텐서는 <code>del</code> 명령어로 삭제하고, 필요 없는 텐서는 GPU가 아닌 CPU에 올려 메모리 점유를 줄입니다[4][8][7].</li>
</ul>
<h5 id="5_2">5. <strong>메모리 캐시 비우기</strong><a class="headerlink" href="#5_2" title="Permanent link">&para;</a></h5>
<ul>
<li>PyTorch의 <code>torch.cuda.empty_cache()</code>를 사용해 캐시된 미사용 메모리를 해제할 수 있습니다. 다만, 실제 사용 중인 텐서의 메모리는 해제되지 않으므로 효과가 제한적일 수 있습니다[9][8][7].</li>
</ul>
<h5 id="6-torchno_grad-modeleval">6. <strong>torch.no_grad() 및 model.eval() 사용</strong><a class="headerlink" href="#6-torchno_grad-modeleval" title="Permanent link">&para;</a></h5>
<ul>
<li>추론(inference) 시에는 <code>torch.no_grad()</code>로 불필요한 그래디언트 저장을 방지하고, <code>model.eval()</code>로 학습에만 필요한 레이어를 비활성화해 메모리 사용을 줄입니다[6][7].</li>
</ul>
<h5 id="7">7. <strong>입력 데이터 크기/해상도 줄이기</strong><a class="headerlink" href="#7" title="Permanent link">&para;</a></h5>
<ul>
<li>이미지나 텍스트 등 입력 데이터의 크기를 줄이면 메모리 사용량이 감소합니다[5].</li>
</ul>
<h5 id="8-dataloader">8. <strong>데이터 분할 및 DataLoader 활용</strong><a class="headerlink" href="#8-dataloader" title="Permanent link">&para;</a></h5>
<ul>
<li>대용량 데이터를 한 번에 올리지 않고, DataLoader 등으로 나눠서 처리합니다[4].</li>
</ul>
<h5 id="9_1">9. <strong>메모리 누수 방지</strong><a class="headerlink" href="#9_1" title="Permanent link">&para;</a></h5>
<ul>
<li>사용이 끝난 메모리를 즉시 해제하고, 불필요한 데이터가 계속 쌓이지 않도록 주의합니다[3][10].</li>
</ul>
<h5 id="10-gpu">10. <strong>더 큰 GPU로 업그레이드</strong><a class="headerlink" href="#10-gpu" title="Permanent link">&para;</a></h5>
<ul>
<li>위의 방법으로도 해결이 안 되면, 메모리 용량이 더 큰 GPU로 교체하는 것이 근본적인 해결책이 될 수 있습니다[5][6].</li>
</ul>
<h5 id="gpu">참고: GPU 메모리 상태 확인<a class="headerlink" href="#gpu" title="Permanent link">&para;</a></h5>
<ul>
<li><code>nvidia-smi</code> 또는 <code>GPUtil</code>을 사용해 현재 GPU 메모리 사용량을 실시간으로 모니터링할 수 있습니다[8][7].</li>
</ul>
<h4 id="_18">요약<a class="headerlink" href="#_18" title="Permanent link">&para;</a></h4>
<p>OOM(Out Of Memory)은 시스템이나 GPU의 메모리가 부족할 때 발생하는 오류입니다. CUDA OOM을 완화하려면 배치 크기 줄이기, 모델 축소, 혼합 정밀도 학습, 불필요한 변수 삭제, 캐시 비우기, 데이터 크기 축소, DataLoader 활용 등 다양한 방법을 적용할 수 있습니다. 그래도 해결이 안 된다면 더 큰 GPU로 업그레이드하는 것이 필요합니다[4][5][6][8][7].</p>
<p>출처
[1] What Does "Out of Memory" (OOM) Mean? - phoenixNAP https://phoenixnap.com/glossary/out-of-memory
[2] Out of memory - Wikipedia https://en.wikipedia.org/wiki/Out_of_memory
[3] What is OOM? A Guide to Out of Memory Issues - Last9 https://last9.io/blog/what-is-oom/
[4] PyTorch CUDA OutOfMemoryError 해결 https://velog.io/@hly1013/PyTorch-CUDA-OutOfMemoryError-%ED%95%B4%EA%B2%B0
[5] CUDA 메모리 부족" 오류가 발생 - post - 티스토리 https://post.tistory.com/entry/CUDA-%EB%A9%94%EB%AA%A8%EB%A6%AC-%EB%B6%80%EC%A1%B1-%EC%98%A4%EB%A5%98%EA%B0%80-%EB%B0%9C%EC%83%9D
[6] [pytorch, 딥러닝] CUDA out of memory 에러 해결방법(이미지 ... https://mopipe.tistory.com/192
[7] 10. GPU OOM(Out Of Memory) 해결방법 - 코딩소비 - 티스토리 https://sobeee.tistory.com/251
[8] Week_3 Pytorch - Out of Memory, OOM 해결 https://memesoo99.tistory.com/53
[9] PyTorch와 CUDA를 이용한 GPU 메모리 관리 마스터하기 https://intelloper.tistory.com/entry/PyTorch-CUDA-GPU-memory-management
[10] Out-of-Memory Error (CUDA Out of Memory): What It Is and How to Fix It https://hatchjs.com/outofmemoryerror-cuda-out-of-memory/
[11] How to Fix CUDA Out of Memory Errors in PyTorch https://hatchjs.com/cuda-out-of-memory-pytorch/
[12] Out-of-Memory (OOM) or Excessive Memory Usage https://www.osc.edu/documentation/knowledge_base/out_of_memory_oom_or_excessive_memory_usage
[13] Out of Memory Killer - Red Hat Learning Community https://learn.redhat.com/t5/Platform-Linux/Out-of-Memory-Killer/td-p/48828
[14] CUDA OOM 해결 사례 공유 - PyTorch all_gather_object 의 비밀 https://devocean.sk.com/blog/techBoardDetail.do?ID=167403&amp;boardType=techBlog
[15] (딥러닝)CUDA out of memory 해결방법 - limmmmm - 티스토리 https://limmmmm.tistory.com/10
[16] Linux Out of Memory killer - Knowledge Base - Neo4j https://neo4j.com/developer/kb/linux-out-of-memory-killer/
[17] GPU VRAM Overflow: Causes and Solutions - DiskMFR https://www.diskmfr.com/gpu-vram-overflow-causes-and-solutions/
[18] Here's how to clear GPU memory using 6 methods https://www.pcguide.com/gpu/how-to/clear-memory/
[19] GPU OOM과 이별하는 법 https://pizzathiefz.github.io/posts/gpu-out-of-memory/
[20] GPU 메모리·연산 효율 최적화 완전 가이드 - Deep Learning study https://hichoe95.tistory.com/143</p>
<h3 id="gpu_1">GPU 메모리 절감 기법<a class="headerlink" href="#gpu_1" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>그레이디언트 체크포인팅(Gradient Checkpointing)</strong><br />
  순전파 과정에서 중간 활성화(activation)를 모두 저장하지 않고, 필요한 일부만 저장한 뒤 역전파(backward) 때에만 나머지는 재계산하여 메모리 사용량을 크게 줄입니다. 계산량은 늘어나지만, 대용량 모델의 학습이 가능해집니다.</li>
<li><strong>그레이디언트 누적(Gradient Accumulation)</strong><br />
  배치 크기를 줄여 여러 미니배치에 대해 그레이디언트를 누적하여 한 번에 업데이트함으로써 작은 배치로 큰 효과를 내고 GPU 메모리 사용량을 절감할 수 있습니다.</li>
<li><strong>분산 병렬화(Data/Model/Tensor Parallelism)</strong><br />
  파라미터와 계산을 여러 GPU로 분산해 병렬 처리하면 단일 GPU당 메모리 부담을 줄일 수 있습니다.</li>
<li><strong>ZeRO Optimizer, PEFT(LoRA/QLoRA)</strong><br />
  옵티마이저의 중복 변수 저장을 최소화(Lora 등 파라미터 효율적 미세조정)함으로써 대규모 모델 파인튜닝 시 메모리 부담을 줄입니다.</li>
<li><strong>KV Cache, Flash Attention</strong><br />
  트랜스포머 계열에서 주로 쓰이는 기법으로, 반복적으로 사용되는 토큰의 Key/Value 텐서를 캐싱하거나, Self-Attention 계산을 블록 단위로 처리해 메모리 사용과 연산 효율을 개선합니다[1][5][4].</li>
</ul>
<h3 id="mixed-precision-training">Mixed Precision Training<a class="headerlink" href="#mixed-precision-training" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>개념</strong><br />
  일부 연산을 FP16(16비트 부동소수점)으로 처리하고 중요한 연산은 FP32(32비트)로 유지하는 <em>혼합 정밀도</em> 학습 방식입니다.</li>
<li><strong>효과</strong><br />
  메모리 사용량은 거의 절반 가까이 절감되며, 연산 대역폭 활용 극대화로 학습 속도 또한 빨라질 수 있습니다.</li>
<li><strong>기술 적용</strong><br />
  딥러닝 프레임워크(PyTorch, TensorFlow 등)에서는 <code>autocast()</code>, <code>GradScaler</code> API로 적용 가능하며, 모델 파라미터는 32비트로 저장되고, 순전파/역전파 계산 및 Gradient 처리는 16비트 연산이 혼합되어 메모리/속도 최적화가 가능합니다[2][7].</li>
</ul>
<hr />
<p>이와 같은 GPU 메모리 절감 및 Mixed Precision 기술은 대규모 모델의 학습과 추론에서 필수적인 요소로, 모델 개발의 생산성과 비용 효율성을 크게 높일 수 있습니다.</p>
<p>출처
[1] 효율적인 GPU 메모리 사용을 위한 여러 기법 - DevOcean - SK https://devocean.sk.com/blog/techBoardDetail.do?ID=167051&amp;boardType=techBlog
[2] GPU 메모리·연산 효율 최적화 팁 - Deep Learning study - 티스토리 https://hichoe95.tistory.com/136
[3] GPU 메모리·연산 효율 최적화 완전 가이드 - Deep Learning study https://hichoe95.tistory.com/143
[4] LLM 추론 시 GPU 메모리 사용량 알아보기 https://g3lu.tistory.com/51
[5] 효율적인 GPU 메모리 사용을 위한 여러 기법 - 데보션 - Velopers https://www.velopers.kr/post/2706
[6] GPU 메모리와 모델 최적화: 실전 사례 분석 - F-Lab https://f-lab.kr/insight/gpu-memory-and-model-optimization
[7] [딥러닝] LLM 학습 방법 정리(모델 경량화, GPU 요구사항, 학습 효율 ... https://railly-linker.tistory.com/205
[8] GP-GPU를 사용한 Deep Learning Inference의 메모리 절약 기법 https://www.dbpia.co.kr/Journal/articleDetail?nodeId=NODE10448418
[9] 딥러닝 모델 학습 속도를 2배 높이는 GPU 최적화 방법 https://console.runyour.ai/homefeed/gpu-optimization</p>
<hr />
<h2 id="12_1">12. 추가 고려 요소<a class="headerlink" href="#12_1" title="Permanent link">&para;</a></h2>
<h3 id="pruning-quantization-knowledge-distillation">모델 경량화 기법 (Pruning, Quantization, Knowledge Distillation)<a class="headerlink" href="#pruning-quantization-knowledge-distillation" title="Permanent link">&para;</a></h3>
<h4 id="1-pruning">1. Pruning (가지치기)<a class="headerlink" href="#1-pruning" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>개념</strong>: 모델 내에서 중요도가 낮거나 거의 영향을 미치지 않는 가중치(파라미터)를 제거하여 모델 크기와 계산량을 줄이는 기법입니다.</li>
<li><strong>종류</strong>:  </li>
<li><em>Unstructured Pruning</em>: 개별 가중치를 임의로 제거해 희소 행렬을 만듭니다. 하지만 하드웨어 가속 최적화에는 한계가 있습니다.  </li>
<li><em>Structured Pruning</em>: 채널, 필터, 레이어 단위로 가지치기하여 하드웨어 효율적인 경량화를 구현합니다.  </li>
<li><strong>효과</strong>: 모델 크기 감소, 계산 속도 향상, 메모리 절감  </li>
<li><strong>적용</strong>: 모바일/엣지 디바이스 및 빠른 추론이 필요한 서비스에 적합합니다[3][4][5].</li>
</ul>
<h4 id="2-quantization">2. Quantization (양자화)<a class="headerlink" href="#2-quantization" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>개념</strong>: 모델 파라미터 및 연산 결과를 고정소수점 또는 저비트 정수(예: 8비트, 4비트)로 변환해 모델 저장 용량과 연산 자원을 줄이는 방법입니다.</li>
<li><strong>방식</strong>:  </li>
<li><em>Post-Training Quantization (PTQ)</em>: 훈련 완료 후 양자화  </li>
<li><em>Quantization-Aware Training (QAT)</em>: 학습 과정에서 양자화를 고려하여 성능 저하를 최소화  </li>
<li><strong>효과</strong>: 모델 크기 축소, 연산 속도 증가, 에너지 효율 향상  </li>
<li>
<p><strong>적용</strong>: 제한된 하드웨어에서 AI 모델을 원활히 실행하고자 할 때 유용합니다[3][4][5].</p>
</li>
<li>
<p><strong>정적 양자화 (Static Quantization)</strong>:<br />
  모델 배포 전, 모든 가중치와 활성화를 고정된 저정밀도(예: INT8)로 변환  </p>
</li>
<li>장점: 추론 속도 향상, 메모리 접근 효율 증가  </li>
<li>단점: Calibration 필요(학습 데이터셋으로 양자화 범위 보정), 새로운 데이터에서는 성능 저하 가능</li>
<li><strong>동적 양자화 (Dynamic Quantization)</strong>:<br />
  활성화 값을 추론 시점에 동적으로 양자화  </li>
<li>Per-tensor Dynamic Quantization (Tensor-wise)  </li>
<li>Hybrid Dynamic Quantization (하이브리드 동적 양자화)</li>
</ul>
<h4 id="3-knowledge-distillation">3. Knowledge Distillation (지식 증류)<a class="headerlink" href="#3-knowledge-distillation" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>개념</strong>: 큰 모델(teacher)이 학습한 지식을 작은 모델(student)에게 전달하여, 작은 모델이 비슷한 성능을 내도록 학습시키는 기법입니다.</li>
<li><strong>방식</strong>: teacher 모델의 예측(soft targets)을 student 모델의 학습 목표로 하여 일반 정답(label)보다 더 풍부한 정보로 학습합니다.</li>
<li><strong>효과</strong>: 성능은 유지하면서 모델 크기와 연산량을 크게 줄일 수 있음  </li>
<li><strong>적용</strong>: 모바일, 임베디드 시스템 등 자원 제한 환경에서 복잡한 대형 모델을 경량화하는 데 효과적입니다[3][4][5].</li>
</ul>
<p>이들 기법을 적절히 조합하면, 다중 목적에 맞는 최적화된 경량 AI 모델 설계가 가능합니다. 특히 모바일 및 엣지 컴퓨팅에서 경량 모델 수요가 증가하면서, Pruning, Quantization, Knowledge Distillation은 필수적인 핵심 기술로 자리 잡고 있습니다.</p>
<p>출처
[1] [CNN Networks] 6. 딥러닝 모델 경량화 - velog https://velog.io/@woojinn8/LightWeight-Deep-Learning-0.-%EB%94%A5%EB%9F%AC%EB%8B%9D-%EB%AA%A8%EB%8D%B8-%EA%B2%BD%EB%9F%89%ED%99%94
[2] [PDF] 딥러닝 모델 경량화 기술 분석 https://repository.kisti.re.kr/bitstream/10580/15591/1/(%EA%B8%B0%EC%88%A0)%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EB%AA%A8%EB%8D%B8%20%EA%B2%BD%EB%9F%89%ED%99%94%20%EA%B8%B0%EC%88%A0%20%EB%B6%84%EC%84%9D.pdf
[3] [딥러닝 경량화] 모델, 네트워크 경량화 : Quantization - PTQ, QAT https://u-b-h.tistory.com/13
[4] 딥러닝 모델 최적화 방법: 모델 경량화와 모델 추론 속도 가속화 https://blog-ko.superb-ai.com/how-to-optimize-deep-learning-models/
[5] [PDF] 경량 딥러닝 기술 동향 https://ettrends.etri.re.kr/ettrends/176/0905176005/34-2_40-50.pdf
[6] [딥러닝 경량화] 실무에서 적용중인 딥러닝 모델 경량화/최적화 기법은? https://developers.hyundaimotorgroup.com/blog/366
[7] [딥러닝 모델 경량화] 다양한 종류의 Convolution https://sotudy.tistory.com/10
[8] 딥러닝 경량화를 위한 구조, 가지치기, 지식증류 비교 - DBpia https://www.dbpia.co.kr/journal/articleDetail?nodeId=NODE10672328</p>
<hr />
<h3 id="cnn-transformer">하이브리드 아키텍처 (CNN + Transformer)<a class="headerlink" href="#cnn-transformer" title="Permanent link">&para;</a></h3>
<p>CNN과 Transformer의 결합 형태는 이미지, 영상 등 복잡한 데이터에서 지역(Local) 및 전역(Global) 특징을 동시에 포착하기 위해 등장한 하이브리드 구조입니다. 일반적으로 입력의 저수준 패턴(모서리, 텍스처)은 CNN으로 추출하고, 이후 Transformer 블록에서 전역적 의존성, 맥락 정보를 학습합니다. 예를 들어 자율주행, 의료 영상 분할 등에서 CNN은 디테일한 객체 감지에, Transformer는 전체 장면의 의미 해석에 강점을 보입니다. 이런 구조는 성능, 효율 두 측면 모두에서 기존 단일 모델 대비 뛰어난 균형을 달성할 수 있습니다. 실제로 CMT, BEFUnet, MobileViT, ConvNeXt 등 다양한 하이브리드 모델들이 현업에 적용되고 있습니다[1][2].</p>
<hr />
<h3 id="vision-language-audio-text">멀티모달 아키텍처 설계 (Vision-Language, Audio-Text)<a class="headerlink" href="#vision-language-audio-text" title="Permanent link">&para;</a></h3>
<p>멀티모달 아키텍처는 서로 다른 데이터 유형(예: 이미지+텍스트, 오디오+문자)에서 정보를 결합해 더 풍부한 추론 및 생성 능력을 획득하도록 설계됩니다. 각 모달리티 별로 전문 인코더(예: CNN/ViT for vision, Transformer for text, Spectrogram+Transformer for audio)를 활용하여 데이터를 벡터로 변환하고, 이후 공통 의미 공간(latent space)에서 통합합니다. 대표적 기술로 CLIP(이미지/텍스트), VLM(비전-언어 모델), AudioLM(오디오-언어 모델) 등이 있으며, 이미지 설명, 질의응답, 멀티미디어 분석 같은 복합 태스크에 최적화되어 있습니다[3][4].</p>
<hr />
<h3 id="_19">하이퍼파라미터 최적화와 구조 설계의 관계<a class="headerlink" href="#_19" title="Permanent link">&para;</a></h3>
<p>하이퍼파라미터(예: 학습률, 은닉층/노드 수, 드롭아웃, 규제, 배치 사이즈 등)는 모델 구조의 복잡성과 학습 패턴을 결정짓는 핵심 요소입니다. 적절한 하이퍼파라미터 설정은 과적합/과소적합 균형, 모델의 표현력, 데이터 및 문제 특성에 맞는 최적 구조 선택에 직접적으로 연결됩니다. 예를 들어 Dropout 비율이나 Layer 수 설정은 구조의 복잡성을 제어하고, 규제 파라미터는 모델의 안정성에 영향을 미칩니다.
튜닝 방식에 따라 모델 구조 선택과 성능 달성에 중요한 영향을 주므로, 구조 설계 과정에서 하이퍼파라미터 최적화가 필연적으로 병행되어야 합니다[5][6].</p>
<hr />
<h3 id="explainability">모델 해석 가능성(Explainability)과 설계<a class="headerlink" href="#explainability" title="Permanent link">&para;</a></h3>
<p>AI 모델의 해석 가능성(Explainability)은 예측·판단 결과에 대해 사람이 원리·근거를 이해할 수 있게 만드는 설계 요소입니다. 높은 해석 가능성을 가지려면, 모델 구조가 명확한 인과 관계를 드러내거나(post-hoc 분석 가능성 등), 내재적으로 직관적인 가시성을 제공해야 합니다.
설명 가능성이 보장되면 AI의 신뢰성, 투명성, 윤리적 책임, 디버깅 및 개선 용이성 등 여러 효용이 높아집니다. 이를 위해 의사결정 경로를 기록하고, Feature Importance 분석, Attention Map, Rule-based Layer 등 다양한 XAI 기법을 모델 설계에 반영할 수 있습니다[7].</p>
<hr />
<p>결론적으로, 각 요소는 실전 AI 시스템 설계에서 성능, 확장성, 신뢰성, 효율성을 최적화하기 위한 핵심 기술적 전략의 일부입니다.</p>
<p>출처
[1] CNN 대 트랜스포머: 이미지 인식 모델 설명 - Flypix https://flypix.ai/ko/blog/image-recognition-models-cnns/
[2] [Paper Review] CMT 논문 이해하기 https://rahites.tistory.com/373
[3] 멀티모달 AI란? LLM을 넘는 차세대 인공지능의 핵심 기술 https://www.koreadeep.com/blog/multimodal-ai
[4] 멀티모달 VLM 기술 동향 - 한컴테크 https://tech.hancom.com/multimodal-vlm-trends/
[5] AI 모델 성능을 극대화하는 하이퍼 파라미터 완벽 가이드 https://www.impactive-ai.com/insight/what-is-hyper-parameter-tuning
[6] 06-2 하이퍼파라미터 튜닝 - 모델 성능 최적화하기 - 위키독스 https://wikidocs.net/273831
[7] AI 모델 해석 가능성 연구: 인공지능의 투명성과 신뢰성 확보 방안 https://s1275702.tistory.com/entry/AI-%EB%AA%A8%EB%8D%B8-%ED%95%B4%EC%84%9D-%EA%B0%80%EB%8A%A5%EC%84%B1-%EC%97%B0%EA%B5%AC-%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5%EC%9D%98-%ED%88%AC%EB%AA%85%EC%84%B1%EA%B3%BC-%EC%8B%A0%EB%A2%B0%EC%84%B1-%ED%99%95%EB%B3%B4-%EB%B0%A9%EC%95%88
[8] [논문 리뷰] CMT: Convolutional Neural Networks Meet Vision ... https://day-to-day.tistory.com/59
[9] [논문 리뷰] BEFUnet: A Hybrid CNN-Transformer Architecture for ... https://www.themoonlight.io/ko/review/befunet-a-hybrid-cnn-transformer-architecture-for-precise-medical-image-segmentation
[10] 01. A survey of the Vision Transformers and its CNN-Transformer ... https://wikidocs.net/237414
[11] [논문 리뷰] X-ray illicit object detection using hybrid CNN-transformer ... https://www.themoonlight.io/ko/review/x-ray-illicit-object-detection-using-hybrid-cnn-transformer-neural-network-architectures</p>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "..", "features": ["navigation.instant", "navigation.tracking", "search.suggest", "search.highlight", "content.code.copy"], "search": "../assets/javascripts/workers/search.973d3a69.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../assets/javascripts/bundle.92b07e13.min.js"></script>
      
    
  </body>
</html>