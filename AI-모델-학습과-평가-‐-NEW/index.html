
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
        <link rel="canonical" href="https://zoro0rkd.github.io/AI-%EB%AA%A8%EB%8D%B8-%ED%95%99%EC%8A%B5%EA%B3%BC-%ED%8F%89%EA%B0%80-%E2%80%90-NEW/">
      
      
        <link rel="prev" href="../XAI-%E2%80%90-NEW/">
      
      
        <link rel="next" href="../AI-%EB%AA%A8%EB%8D%B8-%ED%8A%9C%EB%8B%9D-%E2%80%90-NEW/">
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.17">
    
    
      
        <title>AI 모델 학습과 평가 - Notes</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.7e37652d.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="https://unpkg.com/katex@0/dist/katex.min.css">
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#1-ai" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="Notes" class="md-header__button md-logo" aria-label="Notes" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Notes
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              AI 모델 학습과 평가
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="Notes" class="md-nav__button md-logo" aria-label="Notes" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    Notes
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    홈
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    AI 데이터 전처리
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            AI 데이터 전처리
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%88%98%EC%A7%91-%E2%80%90-NEW/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    데이터 수집
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%A0%95%EC%A0%9C-%E2%80%90-NEW/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    데이터 정제
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%A6%9D%EA%B0%95-%E2%80%90-NEW/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    데이터 증강
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" checked>
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    AI 모델 개발
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            AI 모델 개발
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../AI-%EB%AA%A8%EB%8D%B8-%EC%95%84%ED%82%A4%ED%85%8D%EC%B2%98-%EC%84%A4%EA%B3%84-%E2%80%90-NEW/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    AI 모델 아키텍처 설계
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../XAI-%E2%80%90-NEW/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    XAI
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    AI 모델 학습과 평가
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    AI 모델 학습과 평가
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#1-ai" class="md-nav__link">
    <span class="md-ellipsis">
      1. AI 모델 학습 개요
    </span>
  </a>
  
    <nav class="md-nav" aria-label="1. AI 모델 학습 개요">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_1" class="md-nav__link">
    <span class="md-ellipsis">
      학습의 정의
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#supervised-learning" class="md-nav__link">
    <span class="md-ellipsis">
      지도학습(Supervised Learning)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="지도학습(Supervised Learning)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_2" class="md-nav__link">
    <span class="md-ellipsis">
      지도학습의 정의
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_3" class="md-nav__link">
    <span class="md-ellipsis">
      지도학습의 특징
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_4" class="md-nav__link">
    <span class="md-ellipsis">
      지도학습 과정 예시
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_5" class="md-nav__link">
    <span class="md-ellipsis">
      주요 지도학습 알고리즘 예
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#unsupervised-learning" class="md-nav__link">
    <span class="md-ellipsis">
      비지도학습(Unsupervised Learning)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reinforcement-learning" class="md-nav__link">
    <span class="md-ellipsis">
      강화학습(Reinforcement Learning)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="강화학습(Reinforcement Learning)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_6" class="md-nav__link">
    <span class="md-ellipsis">
      강화학습의 정의
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_7" class="md-nav__link">
    <span class="md-ellipsis">
      강화학습의 주요 구성 요소
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_8" class="md-nav__link">
    <span class="md-ellipsis">
      강화학습의 특징
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_9" class="md-nav__link">
    <span class="md-ellipsis">
      예시
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_10" class="md-nav__link">
    <span class="md-ellipsis">
      대표적인 강화학습 알고리즘
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#self-supervised-learning" class="md-nav__link">
    <span class="md-ellipsis">
      Self-supervised learning
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Self-supervised learning">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#self-supervised-learningssl" class="md-nav__link">
    <span class="md-ellipsis">
      Self-Supervised Learning(SSL) 방법의 두 단계 상세 설명
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Self-Supervised Learning(SSL) 방법의 두 단계 상세 설명">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-pretraining-with-unlabeled-data" class="md-nav__link">
    <span class="md-ellipsis">
      1. 대규모 라벨 없는 데이터로 표현 사전학습 (Pretraining with Unlabeled Data)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-fine-tuning-with-labeled-data" class="md-nav__link">
    <span class="md-ellipsis">
      2. 소량 라벨 데이터로 미세조정 (Fine-tuning with Labeled Data)
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_11" class="md-nav__link">
    <span class="md-ellipsis">
      요약 표
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_12" class="md-nav__link">
    <span class="md-ellipsis">
      실제 예시
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2" class="md-nav__link">
    <span class="md-ellipsis">
      2. 학습 데이터 준비
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2. 학습 데이터 준비">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#train-validation-test" class="md-nav__link">
    <span class="md-ellipsis">
      데이터 분할 (Train / Validation / Test)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="데이터 분할 (Train / Validation / Test)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_13" class="md-nav__link">
    <span class="md-ellipsis">
      분할 구성
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_14" class="md-nav__link">
    <span class="md-ellipsis">
      일반적인 분할 비율
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_15" class="md-nav__link">
    <span class="md-ellipsis">
      분할의 중요성
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_16" class="md-nav__link">
    <span class="md-ellipsis">
      요약
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cross-validation" class="md-nav__link">
    <span class="md-ellipsis">
      교차검증(Cross Validation)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="교차검증(Cross Validation)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_17" class="md-nav__link">
    <span class="md-ellipsis">
      왜 교차 검증을 사용하는가?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_18" class="md-nav__link">
    <span class="md-ellipsis">
      대표적인 교차 검증 방법
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#k-fold" class="md-nav__link">
    <span class="md-ellipsis">
      K-Fold 교차 검증 예시
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_19" class="md-nav__link">
    <span class="md-ellipsis">
      장점과 단점
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#k-fold_1" class="md-nav__link">
    <span class="md-ellipsis">
      계층별(층화) K-Fold 교차 검증
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_20" class="md-nav__link">
    <span class="md-ellipsis">
      요약
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_21" class="md-nav__link">
    <span class="md-ellipsis">
      데이터 불균형 처리
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_22" class="md-nav__link">
    <span class="md-ellipsis">
      데이터 증강과 전처리의 학습 영향
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3" class="md-nav__link">
    <span class="md-ellipsis">
      3. 모델 학습 과정
    </span>
  </a>
  
    <nav class="md-nav" aria-label="3. 모델 학습 과정">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#weight-initialization" class="md-nav__link">
    <span class="md-ellipsis">
      초기화 (Weight Initialization)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="초기화 (Weight Initialization)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_23" class="md-nav__link">
    <span class="md-ellipsis">
      왜 가중치 초기화가 중요한가?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_24" class="md-nav__link">
    <span class="md-ellipsis">
      대표적인 가중치 초기화 방법과 예시
    </span>
  </a>
  
    <nav class="md-nav" aria-label="대표적인 가중치 초기화 방법과 예시">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1" class="md-nav__link">
    <span class="md-ellipsis">
      1. 균등 분포/정규 분포 무작위 초기화
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-xavier-glorot-initialization" class="md-nav__link">
    <span class="md-ellipsis">
      2. Xavier 초기화 (Glorot Initialization)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-he-kaiming-initialization" class="md-nav__link">
    <span class="md-ellipsis">
      3. He 초기화 (Kaiming Initialization)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4-bias" class="md-nav__link">
    <span class="md-ellipsis">
      4. Bias(바이어스) 초기화
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_25" class="md-nav__link">
    <span class="md-ellipsis">
      핵심 정리
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#forward-propagation" class="md-nav__link">
    <span class="md-ellipsis">
      순전파(Forward Propagation)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#activation-function" class="md-nav__link">
    <span class="md-ellipsis">
      활성 함수(Activation Function)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="활성 함수(Activation Function)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#step-function" class="md-nav__link">
    <span class="md-ellipsis">
      Step Function을 활성화 함수로 사용하지 않는 이유
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Step Function을 활성화 함수로 사용하지 않는 이유">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1_1" class="md-nav__link">
    <span class="md-ellipsis">
      1. 미분 불가능성과 기울기 소실 문제
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2_1" class="md-nav__link">
    <span class="md-ellipsis">
      2. 이산적인 출력으로 인한 최적화 어려움
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-relu" class="md-nav__link">
    <span class="md-ellipsis">
      3. ReLU와의 비교: 부분 미분 가능성
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_26" class="md-nav__link">
    <span class="md-ellipsis">
      ✅ 결론
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#activation-function_1" class="md-nav__link">
    <span class="md-ellipsis">
      주요 Activation Function(활성화 함수) 장단점 비교표
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_27" class="md-nav__link">
    <span class="md-ellipsis">
      참고 및 요약
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_28" class="md-nav__link">
    <span class="md-ellipsis">
      상황별 권장 함수
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#loss-function" class="md-nav__link">
    <span class="md-ellipsis">
      손실함수 (Loss Function)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="손실함수 (Loss Function)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_29" class="md-nav__link">
    <span class="md-ellipsis">
      대표적인 손실함수 정리
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#backpropagation" class="md-nav__link">
    <span class="md-ellipsis">
      역전파 (Backpropagation)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#optimization" class="md-nav__link">
    <span class="md-ellipsis">
      최적화(Optimization)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="최적화(Optimization)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sgd-adam-rmsprop" class="md-nav__link">
    <span class="md-ellipsis">
      SGD, Adam, RMSprop
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#batch-mini-batch-stochastic-gradient-descent" class="md-nav__link">
    <span class="md-ellipsis">
      Batch, Mini-Batch, Stochastic Gradient Descent 비교표
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Batch, Mini-Batch, Stochastic Gradient Descent 비교표">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_30" class="md-nav__link">
    <span class="md-ellipsis">
      추가 설명
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_31" class="md-nav__link">
    <span class="md-ellipsis">
      요약
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#learning-rate" class="md-nav__link">
    <span class="md-ellipsis">
      학습률(Learning rate)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="학습률(Learning rate)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_32" class="md-nav__link">
    <span class="md-ellipsis">
      수식 이해
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#learning-rate_1" class="md-nav__link">
    <span class="md-ellipsis">
      learning Rate 의 영향
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_33" class="md-nav__link">
    <span class="md-ellipsis">
      관련 기법
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#normalization" class="md-nav__link">
    <span class="md-ellipsis">
      정규화(Normalization)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="정규화(Normalization)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#normalization_1" class="md-nav__link">
    <span class="md-ellipsis">
      Normalization이란?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_34" class="md-nav__link">
    <span class="md-ellipsis">
      일반적인 목적과 효과
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#normalization_2" class="md-nav__link">
    <span class="md-ellipsis">
      대표적인 Normalization 종류
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#batch-normalization-batchnorm" class="md-nav__link">
    <span class="md-ellipsis">
      Batch Normalization (BatchNorm) 요약
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#layer-normalization-layernorm" class="md-nav__link">
    <span class="md-ellipsis">
      Layer Normalization (LayerNorm) 요약
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#normalization_3" class="md-nav__link">
    <span class="md-ellipsis">
      참고: 그 외 Normalization
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_35" class="md-nav__link">
    <span class="md-ellipsis">
      요약
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#regularization" class="md-nav__link">
    <span class="md-ellipsis">
      Regularization
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Regularization">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#dropout" class="md-nav__link">
    <span class="md-ellipsis">
      Dropout
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#l1l2" class="md-nav__link">
    <span class="md-ellipsis">
      L1/L2 정규화
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gradient-clipping" class="md-nav__link">
    <span class="md-ellipsis">
      Gradient clipping
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#4" class="md-nav__link">
    <span class="md-ellipsis">
      4. 학습 모니터링 및 개선
    </span>
  </a>
  
    <nav class="md-nav" aria-label="4. 학습 모니터링 및 개선">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_36" class="md-nav__link">
    <span class="md-ellipsis">
      손실 및 정확도 곡선 모니터링
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#early-stopping" class="md-nav__link">
    <span class="md-ellipsis">
      조기 종료(Early Stopping)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#automl" class="md-nav__link">
    <span class="md-ellipsis">
      자동화된 하이퍼파라미터 탐색 (AutoML)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pflops-days" class="md-nav__link">
    <span class="md-ellipsis">
      PFLOPS-days 절감 전략 선택을 위한 기본 개념 정리
    </span>
  </a>
  
    <nav class="md-nav" aria-label="PFLOPS-days 절감 전략 선택을 위한 기본 개념 정리">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#pflops-days_1" class="md-nav__link">
    <span class="md-ellipsis">
      총학습 비용(PFLOPS-days)에 영향을 주는 요소
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pflops-days_2" class="md-nav__link">
    <span class="md-ellipsis">
      대표적 PFLOPS-days 절감 전략
    </span>
  </a>
  
    <nav class="md-nav" aria-label="대표적 PFLOPS-days 절감 전략">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1_2" class="md-nav__link">
    <span class="md-ellipsis">
      1. 하드웨어 및 인프라 효율화
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2_2" class="md-nav__link">
    <span class="md-ellipsis">
      2. 모델 구조 및 학습 알고리즘 개선
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3_1" class="md-nav__link">
    <span class="md-ellipsis">
      3. 학습 프로세스 최적화
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4-managedautoml" class="md-nav__link">
    <span class="md-ellipsis">
      4. Managed/AutoML 서비스 활용
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_37" class="md-nav__link">
    <span class="md-ellipsis">
      주요 개념 요약
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#5" class="md-nav__link">
    <span class="md-ellipsis">
      5. 모델 평가 개요
    </span>
  </a>
  
    <nav class="md-nav" aria-label="5. 모델 평가 개요">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#classification" class="md-nav__link">
    <span class="md-ellipsis">
      분류(Classification) 지표
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#5_1" class="md-nav__link">
    <span class="md-ellipsis">
      5. 모델 평가 개요
    </span>
  </a>
  
    <nav class="md-nav" aria-label="5. 모델 평가 개요">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#regression" class="md-nav__link">
    <span class="md-ellipsis">
      회귀(Regression) 지표
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ranking" class="md-nav__link">
    <span class="md-ellipsis">
      랭킹(Ranking) 지표
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_38" class="md-nav__link">
    <span class="md-ellipsis">
      코사인 유사도
    </span>
  </a>
  
    <nav class="md-nav" aria-label="코사인 유사도">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_39" class="md-nav__link">
    <span class="md-ellipsis">
      코사인 유사도의 정의
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_40" class="md-nav__link">
    <span class="md-ellipsis">
      코사인 유사도 계산식
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_41" class="md-nav__link">
    <span class="md-ellipsis">
      코사인 유사도 이용 사례
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_42" class="md-nav__link">
    <span class="md-ellipsis">
      코사인 유사도와 비슷한 다른 개념
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#6" class="md-nav__link">
    <span class="md-ellipsis">
      6. 모델 평가 심화
    </span>
  </a>
  
    <nav class="md-nav" aria-label="6. 모델 평가 심화">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#confusion-matrix" class="md-nav__link">
    <span class="md-ellipsis">
      혼동 행렬(Confusion Matrix) 해석
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sample-efficiency" class="md-nav__link">
    <span class="md-ellipsis">
      샘플 효율성(Sample Efficiency)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#oodout-of-distribution" class="md-nav__link">
    <span class="md-ellipsis">
      OOD(Out-of-Distribution) 데이터 평가
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llm" class="md-nav__link">
    <span class="md-ellipsis">
      LLM·멀티모달 모델 평가 방법론
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#7" class="md-nav__link">
    <span class="md-ellipsis">
      7. 모델 성능 향상 기법
    </span>
  </a>
  
    <nav class="md-nav" aria-label="7. 모델 성능 향상 기법">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#transfer-learning" class="md-nav__link">
    <span class="md-ellipsis">
      전이학습(Transfer Learning)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#fine-tuning" class="md-nav__link">
    <span class="md-ellipsis">
      파인튜닝(Fine-tuning)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lora-prefix-tuning" class="md-nav__link">
    <span class="md-ellipsis">
      LoRA, Prefix Tuning 등 경량 학습 기법
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ensemble" class="md-nav__link">
    <span class="md-ellipsis">
      앙상블(Ensemble) 기법
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_43" class="md-nav__link">
    <span class="md-ellipsis">
      경량화 기법
    </span>
  </a>
  
    <nav class="md-nav" aria-label="경량화 기법">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#pruning" class="md-nav__link">
    <span class="md-ellipsis">
      Pruning의 작동 원리
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_44" class="md-nav__link">
    <span class="md-ellipsis">
      주요 모델별 적용 사례
    </span>
  </a>
  
    <nav class="md-nav" aria-label="주요 모델별 적용 사례">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-squeezenet" class="md-nav__link">
    <span class="md-ellipsis">
      1. SqueezeNet
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-mobilenet" class="md-nav__link">
    <span class="md-ellipsis">
      2. MobileNet
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-efficientnet" class="md-nav__link">
    <span class="md-ellipsis">
      3. EfficientNet
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#8" class="md-nav__link">
    <span class="md-ellipsis">
      8. 모델 학습과 평가의 한계 및 고려사항
    </span>
  </a>
  
    <nav class="md-nav" aria-label="8. 모델 학습과 평가의 한계 및 고려사항">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#bias-variance" class="md-nav__link">
    <span class="md-ellipsis">
      Bias &amp; Variance
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Bias &amp; Variance">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#bias-variance-tradeoff" class="md-nav__link">
    <span class="md-ellipsis">
      Bias &amp; Variance tradeoff
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#tradeoff" class="md-nav__link">
    <span class="md-ellipsis">
      왜 Tradeoff(트레이드오프)인가?
    </span>
  </a>
  
    <nav class="md-nav" aria-label="왜 Tradeoff(트레이드오프)인가?">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_45" class="md-nav__link">
    <span class="md-ellipsis">
      수식과 시각적 이해
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_46" class="md-nav__link">
    <span class="md-ellipsis">
      실전에서의 활용
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#overfitting-underfitting" class="md-nav__link">
    <span class="md-ellipsis">
      과적합(Overfitting) 및 과소적합(Underfitting)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="과적합(Overfitting) 및 과소적합(Underfitting)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#vs" class="md-nav__link">
    <span class="md-ellipsis">
      간단한 예: 고양이 vs 강아지 구분
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#learning-curve" class="md-nav__link">
    <span class="md-ellipsis">
      확인 방법: 학습 곡선(Learning Curve)
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_47" class="md-nav__link">
    <span class="md-ellipsis">
      평가 지표의 해석 한계
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_48" class="md-nav__link">
    <span class="md-ellipsis">
      사회적·윤리적 영향 고려
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../AI-%EB%AA%A8%EB%8D%B8-%ED%8A%9C%EB%8B%9D-%E2%80%90-NEW/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    AI 모델 튜닝
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    AI 시스템 구축
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            AI 시스템 구축
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../AI-%EC%8B%9C%EC%8A%A4%ED%85%9C-%EC%84%A4%EA%B3%84-%EB%B0%8F-%EB%B0%B0%ED%8F%AC-%E2%80%90-NEW/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    AI 시스템 설계 및 배포
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../AI-%EC%8B%9C%EC%8A%A4%ED%85%9C-%EC%B5%9C%EC%A0%81%ED%99%94-%E2%80%90-NEW/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    AI 시스템 최적화
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../%EC%9C%A0%EC%9A%A9%ED%95%9C-%EC%82%AC%EC%9D%B4%ED%8A%B8/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    유용한 사이트
    
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#1-ai" class="md-nav__link">
    <span class="md-ellipsis">
      1. AI 모델 학습 개요
    </span>
  </a>
  
    <nav class="md-nav" aria-label="1. AI 모델 학습 개요">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_1" class="md-nav__link">
    <span class="md-ellipsis">
      학습의 정의
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#supervised-learning" class="md-nav__link">
    <span class="md-ellipsis">
      지도학습(Supervised Learning)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="지도학습(Supervised Learning)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_2" class="md-nav__link">
    <span class="md-ellipsis">
      지도학습의 정의
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_3" class="md-nav__link">
    <span class="md-ellipsis">
      지도학습의 특징
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_4" class="md-nav__link">
    <span class="md-ellipsis">
      지도학습 과정 예시
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_5" class="md-nav__link">
    <span class="md-ellipsis">
      주요 지도학습 알고리즘 예
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#unsupervised-learning" class="md-nav__link">
    <span class="md-ellipsis">
      비지도학습(Unsupervised Learning)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reinforcement-learning" class="md-nav__link">
    <span class="md-ellipsis">
      강화학습(Reinforcement Learning)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="강화학습(Reinforcement Learning)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_6" class="md-nav__link">
    <span class="md-ellipsis">
      강화학습의 정의
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_7" class="md-nav__link">
    <span class="md-ellipsis">
      강화학습의 주요 구성 요소
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_8" class="md-nav__link">
    <span class="md-ellipsis">
      강화학습의 특징
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_9" class="md-nav__link">
    <span class="md-ellipsis">
      예시
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_10" class="md-nav__link">
    <span class="md-ellipsis">
      대표적인 강화학습 알고리즘
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#self-supervised-learning" class="md-nav__link">
    <span class="md-ellipsis">
      Self-supervised learning
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Self-supervised learning">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#self-supervised-learningssl" class="md-nav__link">
    <span class="md-ellipsis">
      Self-Supervised Learning(SSL) 방법의 두 단계 상세 설명
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Self-Supervised Learning(SSL) 방법의 두 단계 상세 설명">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-pretraining-with-unlabeled-data" class="md-nav__link">
    <span class="md-ellipsis">
      1. 대규모 라벨 없는 데이터로 표현 사전학습 (Pretraining with Unlabeled Data)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-fine-tuning-with-labeled-data" class="md-nav__link">
    <span class="md-ellipsis">
      2. 소량 라벨 데이터로 미세조정 (Fine-tuning with Labeled Data)
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_11" class="md-nav__link">
    <span class="md-ellipsis">
      요약 표
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_12" class="md-nav__link">
    <span class="md-ellipsis">
      실제 예시
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2" class="md-nav__link">
    <span class="md-ellipsis">
      2. 학습 데이터 준비
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2. 학습 데이터 준비">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#train-validation-test" class="md-nav__link">
    <span class="md-ellipsis">
      데이터 분할 (Train / Validation / Test)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="데이터 분할 (Train / Validation / Test)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_13" class="md-nav__link">
    <span class="md-ellipsis">
      분할 구성
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_14" class="md-nav__link">
    <span class="md-ellipsis">
      일반적인 분할 비율
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_15" class="md-nav__link">
    <span class="md-ellipsis">
      분할의 중요성
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_16" class="md-nav__link">
    <span class="md-ellipsis">
      요약
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cross-validation" class="md-nav__link">
    <span class="md-ellipsis">
      교차검증(Cross Validation)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="교차검증(Cross Validation)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_17" class="md-nav__link">
    <span class="md-ellipsis">
      왜 교차 검증을 사용하는가?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_18" class="md-nav__link">
    <span class="md-ellipsis">
      대표적인 교차 검증 방법
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#k-fold" class="md-nav__link">
    <span class="md-ellipsis">
      K-Fold 교차 검증 예시
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_19" class="md-nav__link">
    <span class="md-ellipsis">
      장점과 단점
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#k-fold_1" class="md-nav__link">
    <span class="md-ellipsis">
      계층별(층화) K-Fold 교차 검증
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_20" class="md-nav__link">
    <span class="md-ellipsis">
      요약
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_21" class="md-nav__link">
    <span class="md-ellipsis">
      데이터 불균형 처리
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_22" class="md-nav__link">
    <span class="md-ellipsis">
      데이터 증강과 전처리의 학습 영향
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3" class="md-nav__link">
    <span class="md-ellipsis">
      3. 모델 학습 과정
    </span>
  </a>
  
    <nav class="md-nav" aria-label="3. 모델 학습 과정">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#weight-initialization" class="md-nav__link">
    <span class="md-ellipsis">
      초기화 (Weight Initialization)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="초기화 (Weight Initialization)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_23" class="md-nav__link">
    <span class="md-ellipsis">
      왜 가중치 초기화가 중요한가?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_24" class="md-nav__link">
    <span class="md-ellipsis">
      대표적인 가중치 초기화 방법과 예시
    </span>
  </a>
  
    <nav class="md-nav" aria-label="대표적인 가중치 초기화 방법과 예시">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1" class="md-nav__link">
    <span class="md-ellipsis">
      1. 균등 분포/정규 분포 무작위 초기화
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-xavier-glorot-initialization" class="md-nav__link">
    <span class="md-ellipsis">
      2. Xavier 초기화 (Glorot Initialization)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-he-kaiming-initialization" class="md-nav__link">
    <span class="md-ellipsis">
      3. He 초기화 (Kaiming Initialization)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4-bias" class="md-nav__link">
    <span class="md-ellipsis">
      4. Bias(바이어스) 초기화
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_25" class="md-nav__link">
    <span class="md-ellipsis">
      핵심 정리
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#forward-propagation" class="md-nav__link">
    <span class="md-ellipsis">
      순전파(Forward Propagation)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#activation-function" class="md-nav__link">
    <span class="md-ellipsis">
      활성 함수(Activation Function)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="활성 함수(Activation Function)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#step-function" class="md-nav__link">
    <span class="md-ellipsis">
      Step Function을 활성화 함수로 사용하지 않는 이유
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Step Function을 활성화 함수로 사용하지 않는 이유">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1_1" class="md-nav__link">
    <span class="md-ellipsis">
      1. 미분 불가능성과 기울기 소실 문제
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2_1" class="md-nav__link">
    <span class="md-ellipsis">
      2. 이산적인 출력으로 인한 최적화 어려움
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-relu" class="md-nav__link">
    <span class="md-ellipsis">
      3. ReLU와의 비교: 부분 미분 가능성
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_26" class="md-nav__link">
    <span class="md-ellipsis">
      ✅ 결론
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#activation-function_1" class="md-nav__link">
    <span class="md-ellipsis">
      주요 Activation Function(활성화 함수) 장단점 비교표
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_27" class="md-nav__link">
    <span class="md-ellipsis">
      참고 및 요약
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_28" class="md-nav__link">
    <span class="md-ellipsis">
      상황별 권장 함수
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#loss-function" class="md-nav__link">
    <span class="md-ellipsis">
      손실함수 (Loss Function)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="손실함수 (Loss Function)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_29" class="md-nav__link">
    <span class="md-ellipsis">
      대표적인 손실함수 정리
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#backpropagation" class="md-nav__link">
    <span class="md-ellipsis">
      역전파 (Backpropagation)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#optimization" class="md-nav__link">
    <span class="md-ellipsis">
      최적화(Optimization)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="최적화(Optimization)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sgd-adam-rmsprop" class="md-nav__link">
    <span class="md-ellipsis">
      SGD, Adam, RMSprop
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#batch-mini-batch-stochastic-gradient-descent" class="md-nav__link">
    <span class="md-ellipsis">
      Batch, Mini-Batch, Stochastic Gradient Descent 비교표
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Batch, Mini-Batch, Stochastic Gradient Descent 비교표">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_30" class="md-nav__link">
    <span class="md-ellipsis">
      추가 설명
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_31" class="md-nav__link">
    <span class="md-ellipsis">
      요약
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#learning-rate" class="md-nav__link">
    <span class="md-ellipsis">
      학습률(Learning rate)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="학습률(Learning rate)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_32" class="md-nav__link">
    <span class="md-ellipsis">
      수식 이해
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#learning-rate_1" class="md-nav__link">
    <span class="md-ellipsis">
      learning Rate 의 영향
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_33" class="md-nav__link">
    <span class="md-ellipsis">
      관련 기법
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#normalization" class="md-nav__link">
    <span class="md-ellipsis">
      정규화(Normalization)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="정규화(Normalization)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#normalization_1" class="md-nav__link">
    <span class="md-ellipsis">
      Normalization이란?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_34" class="md-nav__link">
    <span class="md-ellipsis">
      일반적인 목적과 효과
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#normalization_2" class="md-nav__link">
    <span class="md-ellipsis">
      대표적인 Normalization 종류
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#batch-normalization-batchnorm" class="md-nav__link">
    <span class="md-ellipsis">
      Batch Normalization (BatchNorm) 요약
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#layer-normalization-layernorm" class="md-nav__link">
    <span class="md-ellipsis">
      Layer Normalization (LayerNorm) 요약
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#normalization_3" class="md-nav__link">
    <span class="md-ellipsis">
      참고: 그 외 Normalization
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_35" class="md-nav__link">
    <span class="md-ellipsis">
      요약
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#regularization" class="md-nav__link">
    <span class="md-ellipsis">
      Regularization
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Regularization">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#dropout" class="md-nav__link">
    <span class="md-ellipsis">
      Dropout
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#l1l2" class="md-nav__link">
    <span class="md-ellipsis">
      L1/L2 정규화
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gradient-clipping" class="md-nav__link">
    <span class="md-ellipsis">
      Gradient clipping
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#4" class="md-nav__link">
    <span class="md-ellipsis">
      4. 학습 모니터링 및 개선
    </span>
  </a>
  
    <nav class="md-nav" aria-label="4. 학습 모니터링 및 개선">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_36" class="md-nav__link">
    <span class="md-ellipsis">
      손실 및 정확도 곡선 모니터링
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#early-stopping" class="md-nav__link">
    <span class="md-ellipsis">
      조기 종료(Early Stopping)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#automl" class="md-nav__link">
    <span class="md-ellipsis">
      자동화된 하이퍼파라미터 탐색 (AutoML)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pflops-days" class="md-nav__link">
    <span class="md-ellipsis">
      PFLOPS-days 절감 전략 선택을 위한 기본 개념 정리
    </span>
  </a>
  
    <nav class="md-nav" aria-label="PFLOPS-days 절감 전략 선택을 위한 기본 개념 정리">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#pflops-days_1" class="md-nav__link">
    <span class="md-ellipsis">
      총학습 비용(PFLOPS-days)에 영향을 주는 요소
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pflops-days_2" class="md-nav__link">
    <span class="md-ellipsis">
      대표적 PFLOPS-days 절감 전략
    </span>
  </a>
  
    <nav class="md-nav" aria-label="대표적 PFLOPS-days 절감 전략">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1_2" class="md-nav__link">
    <span class="md-ellipsis">
      1. 하드웨어 및 인프라 효율화
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2_2" class="md-nav__link">
    <span class="md-ellipsis">
      2. 모델 구조 및 학습 알고리즘 개선
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3_1" class="md-nav__link">
    <span class="md-ellipsis">
      3. 학습 프로세스 최적화
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4-managedautoml" class="md-nav__link">
    <span class="md-ellipsis">
      4. Managed/AutoML 서비스 활용
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_37" class="md-nav__link">
    <span class="md-ellipsis">
      주요 개념 요약
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#5" class="md-nav__link">
    <span class="md-ellipsis">
      5. 모델 평가 개요
    </span>
  </a>
  
    <nav class="md-nav" aria-label="5. 모델 평가 개요">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#classification" class="md-nav__link">
    <span class="md-ellipsis">
      분류(Classification) 지표
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#5_1" class="md-nav__link">
    <span class="md-ellipsis">
      5. 모델 평가 개요
    </span>
  </a>
  
    <nav class="md-nav" aria-label="5. 모델 평가 개요">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#regression" class="md-nav__link">
    <span class="md-ellipsis">
      회귀(Regression) 지표
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ranking" class="md-nav__link">
    <span class="md-ellipsis">
      랭킹(Ranking) 지표
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_38" class="md-nav__link">
    <span class="md-ellipsis">
      코사인 유사도
    </span>
  </a>
  
    <nav class="md-nav" aria-label="코사인 유사도">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_39" class="md-nav__link">
    <span class="md-ellipsis">
      코사인 유사도의 정의
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_40" class="md-nav__link">
    <span class="md-ellipsis">
      코사인 유사도 계산식
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_41" class="md-nav__link">
    <span class="md-ellipsis">
      코사인 유사도 이용 사례
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_42" class="md-nav__link">
    <span class="md-ellipsis">
      코사인 유사도와 비슷한 다른 개념
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#6" class="md-nav__link">
    <span class="md-ellipsis">
      6. 모델 평가 심화
    </span>
  </a>
  
    <nav class="md-nav" aria-label="6. 모델 평가 심화">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#confusion-matrix" class="md-nav__link">
    <span class="md-ellipsis">
      혼동 행렬(Confusion Matrix) 해석
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sample-efficiency" class="md-nav__link">
    <span class="md-ellipsis">
      샘플 효율성(Sample Efficiency)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#oodout-of-distribution" class="md-nav__link">
    <span class="md-ellipsis">
      OOD(Out-of-Distribution) 데이터 평가
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llm" class="md-nav__link">
    <span class="md-ellipsis">
      LLM·멀티모달 모델 평가 방법론
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#7" class="md-nav__link">
    <span class="md-ellipsis">
      7. 모델 성능 향상 기법
    </span>
  </a>
  
    <nav class="md-nav" aria-label="7. 모델 성능 향상 기법">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#transfer-learning" class="md-nav__link">
    <span class="md-ellipsis">
      전이학습(Transfer Learning)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#fine-tuning" class="md-nav__link">
    <span class="md-ellipsis">
      파인튜닝(Fine-tuning)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lora-prefix-tuning" class="md-nav__link">
    <span class="md-ellipsis">
      LoRA, Prefix Tuning 등 경량 학습 기법
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ensemble" class="md-nav__link">
    <span class="md-ellipsis">
      앙상블(Ensemble) 기법
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_43" class="md-nav__link">
    <span class="md-ellipsis">
      경량화 기법
    </span>
  </a>
  
    <nav class="md-nav" aria-label="경량화 기법">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#pruning" class="md-nav__link">
    <span class="md-ellipsis">
      Pruning의 작동 원리
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_44" class="md-nav__link">
    <span class="md-ellipsis">
      주요 모델별 적용 사례
    </span>
  </a>
  
    <nav class="md-nav" aria-label="주요 모델별 적용 사례">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-squeezenet" class="md-nav__link">
    <span class="md-ellipsis">
      1. SqueezeNet
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-mobilenet" class="md-nav__link">
    <span class="md-ellipsis">
      2. MobileNet
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-efficientnet" class="md-nav__link">
    <span class="md-ellipsis">
      3. EfficientNet
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#8" class="md-nav__link">
    <span class="md-ellipsis">
      8. 모델 학습과 평가의 한계 및 고려사항
    </span>
  </a>
  
    <nav class="md-nav" aria-label="8. 모델 학습과 평가의 한계 및 고려사항">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#bias-variance" class="md-nav__link">
    <span class="md-ellipsis">
      Bias &amp; Variance
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Bias &amp; Variance">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#bias-variance-tradeoff" class="md-nav__link">
    <span class="md-ellipsis">
      Bias &amp; Variance tradeoff
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#tradeoff" class="md-nav__link">
    <span class="md-ellipsis">
      왜 Tradeoff(트레이드오프)인가?
    </span>
  </a>
  
    <nav class="md-nav" aria-label="왜 Tradeoff(트레이드오프)인가?">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_45" class="md-nav__link">
    <span class="md-ellipsis">
      수식과 시각적 이해
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_46" class="md-nav__link">
    <span class="md-ellipsis">
      실전에서의 활용
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#overfitting-underfitting" class="md-nav__link">
    <span class="md-ellipsis">
      과적합(Overfitting) 및 과소적합(Underfitting)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="과적합(Overfitting) 및 과소적합(Underfitting)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#vs" class="md-nav__link">
    <span class="md-ellipsis">
      간단한 예: 고양이 vs 강아지 구분
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#learning-curve" class="md-nav__link">
    <span class="md-ellipsis">
      확인 방법: 학습 곡선(Learning Curve)
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_47" class="md-nav__link">
    <span class="md-ellipsis">
      평가 지표의 해석 한계
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_48" class="md-nav__link">
    <span class="md-ellipsis">
      사회적·윤리적 영향 고려
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  



  <h1>AI 모델 학습과 평가</h1>

<h2 id="1-ai">1. AI 모델 학습 개요<a class="headerlink" href="#1-ai" title="Permanent link">&para;</a></h2>
<h3 id="_1">학습의 정의<a class="headerlink" href="#_1" title="Permanent link">&para;</a></h3>
<p>AI 모델 학습이란, 머신러닝 모델이 주어진 데이터 셋을 통해 작업 수행에 필요한 패턴과 상관관계를 학습하여 성능을 최적화하는 과정입니다. 즉, AI 모델이 데이터에서 특정 패턴을 인식하고 결과를 예측하거나 결정을 내릴 수 있도록 모델의 내부 매개변수(가중치와 편향 등)를 조정하는 것을 말합니다.</p>
<p>모델 학습 과정은 다음과 같은 특징을 갖습니다:</p>
<ul>
<li>학습 데이터는 모델이 실제 문제를 해결할 때 유사한 상황을 경험할 수 있도록 관련성이 있어야 합니다.</li>
<li>모델은 주어진 데이터를 통해 수학적 함수의 계수를 최적화하여 더 정확한 출력을 생성하도록 조정됩니다.</li>
<li>학습 결과로 얻어진 내부 매개변수는 모델이 이전에 본 적 없는 새로운 데이터에서도 예측을 수행할 수 있는 '지식'을 나타냅니다.</li>
<li>학습은 지도학습, 비지도학습, 강화학습 등 다양한 방법으로 이루어질 수 있으며, 복잡한 신경망을 포함하는 딥러닝 모델도 이 과정을 거칩니다.</li>
<li>AI 모델 학습은 단순한 예측 모델부터 생성형 AI까지 다양한 형태의 AI 시스템에서 핵심적인 역할을 합니다.</li>
</ul>
<p>한편, AI 학습은 기존 머신러닝 범위를 넘어서 자율적으로 적응하고 추론하며 의사 결정을 내릴 수 있는 시스템을 만드는 더 넓은 개념을 포함하기도 합니다.</p>
<p>요약하면, AI 모델 학습은 데이터 기반으로 모델을 '가르치고' 최적화하여 미래에 유사한 문제에서 올바른 결과를 도출하도록 만드는 과정을 의미합니다. 이는 AI 모델 성능을 결정하는 가장 중요한 단계입니다. </p>
<p>출처
[1] 모델 학습이란 무엇인가요? <a href="https://www.ibm.com/kr-ko/think/topics/model-training">https://www.ibm.com/kr-ko/think/topics/model-training</a>
[2] AI 모델이란? | 용어 해설 <a href="https://www.hpe.com/kr/ko/what-is/ai-models.html">https://www.hpe.com/kr/ko/what-is/ai-models.html</a>
[3] AI 모델 훈련: 그것이 무엇이고 어떻게 작동하는가 <a href="https://www.mendix.com/ko/blog/ai-model-training/">https://www.mendix.com/ko/blog/ai-model-training/</a>
[4] AI 모델(AI Model)의 이해 <a href="https://www.databricks.com/kr/glossary/ai-models">https://www.databricks.com/kr/glossary/ai-models</a>
[5] AI 학습이란 무엇인가요? <a href="https://www.lenovo.com/kr/ko/glossary/ai-learning/">https://www.lenovo.com/kr/ko/glossary/ai-learning/</a>
[6] AI 모델이란 무엇인가요? <a href="https://www.ibm.com/kr-ko/think/topics/ai-model">https://www.ibm.com/kr-ko/think/topics/ai-model</a>
[7] AI 모델 훈련이란 무엇이고 왜 중요한 것인가요? <a href="https://www.oracle.com/kr/artificial-intelligence/ai-model-training/">https://www.oracle.com/kr/artificial-intelligence/ai-model-training/</a>
[8] AI 모델이란? <a href="https://cloud.google.com/discover/what-is-an-ai-model?hl=ko">https://cloud.google.com/discover/what-is-an-ai-model?hl=ko</a>
[9] [All Around AI 6편] 생성형 AI의 개념과 모델 - SK하이닉스 뉴스룸 <a href="https://news.skhynix.co.kr/all-around-ai-6/">https://news.skhynix.co.kr/all-around-ai-6/</a>
[10] [AI] 딥러닝 모델 학습과정 이해하기 - EveryDay.DevUp - 티스토리 <a href="https://everyday-devup.tistory.com/200">https://everyday-devup.tistory.com/200</a></p>
<h3 id="supervised-learning">지도학습(Supervised Learning)<a class="headerlink" href="#supervised-learning" title="Permanent link">&para;</a></h3>
<p>지도학습(Supervised Learning)은 머신러닝의 주요 학습 방법 중 하나로, 입력 데이터와 그에 대응하는 정답(레이블)이 함께 제공되는 학습 데이터를 사용하여 모델을 학습시키는 과정입니다. </p>
<h4 id="_2">지도학습의 정의<a class="headerlink" href="#_2" title="Permanent link">&para;</a></h4>
<ul>
<li>주어진 입력값(features)과 그에 해당하는 출력값(label)의 쌍으로 이루어진 데이터셋을 통해 모델이 입력과 출력 간의 관계를 학습함</li>
<li>목표는 학습 데이터로부터 함수 혹은 규칙을 찾아내어, 새로운 입력 데이터에 대해 정확한 출력을 예측하는 것</li>
</ul>
<h4 id="_3">지도학습의 특징<a class="headerlink" href="#_3" title="Permanent link">&para;</a></h4>
<ul>
<li>정답(label)이 명시된 데이터를 기반으로 학습하기 때문에 모델의 학습 정확도가 높음</li>
<li>레이블링된 데이터가 필요하며, 이 데이터 준비 과정이 비용과 시간이 많이 들 수 있음</li>
<li>학습된 모델은 새로운 데이터에 대해 분류(Classification)나 회귀(Regression) 문제를 해결할 수 있음</li>
<li><strong>분류</strong>: 입력 데이터가 특정 클래스(예: 고양이, 개, 새 등) 중 어느 것에 속하는지 예측</li>
<li><strong>회귀</strong>: 연속적인 값(예: 주택가격, 온도 등)을 예측</li>
</ul>
<h4 id="_4">지도학습 과정 예시<a class="headerlink" href="#_4" title="Permanent link">&para;</a></h4>
<ul>
<li>10,000장 새 이미지 중 8,000장에 새 종류 레이블을 붙여 학습 데이터로 사용</li>
<li>모델이 이 데이터를 통해 새의 특징을 학습하면 나머지 2,000장에 대해 새 종류를 예측할 수 있음</li>
</ul>
<h4 id="_5">주요 지도학습 알고리즘 예<a class="headerlink" href="#_5" title="Permanent link">&para;</a></h4>
<ul>
<li>나이브 베이즈 분류기 (Naïve Bayes)</li>
<li>서포트 벡터 머신 (SVM)</li>
<li>결정 트리 및 랜덤 포레스트</li>
<li>신경망(Neural Networks)</li>
<li>로지스틱 회귀(Logistic Regression) 등</li>
</ul>
<p>지도학습은 명확한 정답이 있는 문제에서 매우 효과적이며, 일상적인 AI 응용 분야의 상당 부분에서 핵심적인 학습 방법으로 사용됩니다. 대표적인 응용으로는 스팸 메일 분류, 이미지 인식, 음성 인식, 금융 데이터 기반 예측 등이 있습니다.</p>
<p>출처
[1] 지도 학습(Supervised Learning)이란 무엇인가? - Appier <a href="https://www.appier.com/ko-kr/blog/what-is-supervised-learning">https://www.appier.com/ko-kr/blog/what-is-supervised-learning</a>
[2] 지도 학습이란 무엇인가요? - IBM <a href="https://www.ibm.com/kr-ko/think/topics/supervised-learning">https://www.ibm.com/kr-ko/think/topics/supervised-learning</a>
[3] 지도 학습 (Supervised Learning) - 도리의 디지털라이프 <a href="https://blog.skby.net/%EC%A7%80%EB%8F%84-%ED%95%99%EC%8A%B5-supervised-learning/">https://blog.skby.net/%EC%A7%80%EB%8F%84-%ED%95%99%EC%8A%B5-supervised-learning/</a>
[4] 지도학습이란? | Supervised Learning(Regression, Classification) <a href="https://standing-o.github.io/posts/supervised-learning/">https://standing-o.github.io/posts/supervised-learning/</a>
[5] 지도학습(Supervised learning)에 대해 알아보자 - CAI - 티스토리 <a href="https://kjh-ai-blog.tistory.com/3">https://kjh-ai-blog.tistory.com/3</a>
[6] supervised learning (지도학습) - 인공지능(AI) &amp; 머신러닝(ML) 사전 <a href="https://wikidocs.net/120172">https://wikidocs.net/120172</a>
[7] 지도학습 - 주홍색 코딩 <a href="https://kwonkai.tistory.com/154">https://kwonkai.tistory.com/154</a>
[8] [인공지능] 지도학습, 비지도학습, 강화학습 - 삶은 확률의 구름 <a href="https://ebbnflow.tistory.com/165">https://ebbnflow.tistory.com/165</a>
[9] 지도 학습이란 무엇인가요? | Oracle 대한민국 <a href="https://www.oracle.com/kr/artificial-intelligence/machine-learning/supervised-learning/">https://www.oracle.com/kr/artificial-intelligence/machine-learning/supervised-learning/</a></p>
<hr />
<h3 id="unsupervised-learning"><a href="https://github.com/zoro0rkd/ai_study/wiki/AI-모델-아키텍처-설계-%E2%80%90-NEW#7-비지도-학습unsupervised-learning-구조">비지도학습(Unsupervised Learning)</a><a class="headerlink" href="#unsupervised-learning" title="Permanent link">&para;</a></h3>
<p>링크 참고</p>
<hr />
<h3 id="reinforcement-learning">강화학습(Reinforcement Learning)<a class="headerlink" href="#reinforcement-learning" title="Permanent link">&para;</a></h3>
<p>강화학습(Reinforcement Learning, RL)은 머신러닝의 한 분야로, 에이전트(학습 주체)가 환경과 상호작용하면서 시행착오를 통해 최적의 행동을 학습하는 방법입니다.</p>
<h4 id="_6">강화학습의 정의<a class="headerlink" href="#_6" title="Permanent link">&para;</a></h4>
<ul>
<li>에이전트가 현재 상태(State)를 인식하고, 가능한 여러 행동(Action) 중에서 하나를 선택함</li>
<li>선택한 행동에 대해 환경으로부터 보상(Reward)을 받음</li>
<li>에이전트는 이 보상을 최대화하기 위해 앞으로 어떤 행동을 선택해야 할지 학습함</li>
<li>명시적인 정답(Label) 없이도 보상 신호를 통해 스스로 학습하는 점이 특징</li>
</ul>
<h4 id="_7">강화학습의 주요 구성 요소<a class="headerlink" href="#_7" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>에이전트(Agent):</strong> 학습을 수행하는 주체</li>
<li><strong>환경(Environment):</strong> 에이전트가 상호작용하는 대상</li>
<li><strong>상태(State):</strong> 에이전트가 처한 현재 상황</li>
<li><strong>행동(Action):</strong> 에이전트가 선택할 수 있는 동작</li>
<li><strong>보상(Reward):</strong> 행동에 대한 평가 신호로, 에이전트가 목표 달성 정도를 판단하는 기준</li>
<li><strong>정책(Policy):</strong> 특정 상태에서 어떤 행동을 할지 결정하는 전략</li>
<li><strong>가치함수(Value Function):</strong> 특정 상태 또는 상태-행동 쌍의 기대 보상을 평가</li>
</ul>
<h4 id="_8">강화학습의 특징<a class="headerlink" href="#_8" title="Permanent link">&para;</a></h4>
<ul>
<li>시행착오(Trial and Error)를 기반으로 학습하며 경험을 통해 성능을 향상시킴</li>
<li>환경에 대한 명확한 모델이 없어도 학습 가능</li>
<li>인간의 학습 방식 중 하나인 '시행착오 학습'과 유사</li>
<li>미래의 누적 보상을 최대화하는 행동을 학습하는 것이 목적</li>
<li>탐험(Exploration)과 활용(Exploitation)의 균형을 맞추어야 함 (새로운 행동 시도 vs 이미 아는 최적 행동 수행)</li>
</ul>
<h4 id="_9">예시<a class="headerlink" href="#_9" title="Permanent link">&para;</a></h4>
<ul>
<li>알파고가 바둑 대국에서 자기 자신과 반복적으로 대국하며 전략을 개선하는 과정</li>
<li>게임 AI가 플레이하며 승리 전략을 터득하는 것</li>
</ul>
<p>강화학습은 복잡한 의사결정 문제나 동적 환경에서 최적의 행동을 배우는 데 매우 효과적인 방법입니다. 최근 딥러닝과 결합해 많은 분야에서 혁신적인 성과를 내고 있습니다.</p>
<p>출처
[1] 강화학습 개요 - Introduction of Reinforcement Learning <a href="https://skidrow6122.tistory.com/3">https://skidrow6122.tistory.com/3</a>
[2] 강화 학습 - 위키백과, 우리 모두의 백과사전 <a href="https://ko.wikipedia.org/wiki/%EA%B0%95%ED%99%94_%ED%95%99%EC%8A%B5">https://ko.wikipedia.org/wiki/%EA%B0%95%ED%99%94_%ED%95%99%EC%8A%B5</a>
[3] 강화 학습이란 무엇인가요? - IBM <a href="https://www.ibm.com/kr-ko/think/topics/reinforcement-learning">https://www.ibm.com/kr-ko/think/topics/reinforcement-learning</a>
[4] 강화학습이란? - MATLAB &amp; Simulink - 매스웍스 <a href="https://kr.mathworks.com/discovery/reinforcement-learning.html">https://kr.mathworks.com/discovery/reinforcement-learning.html</a>
[5] 머신러닝의 꽃, 강화학습 - 브런치 <a href="https://brunch.co.kr/@namujini/22">https://brunch.co.kr/@namujini/22</a>
[6] 강화학습이란? - velog <a href="https://velog.io/@dorthy/%EA%B0%95%ED%99%94%ED%95%99%EC%8A%B5%EC%9D%B4%EB%9E%80">https://velog.io/@dorthy/%EA%B0%95%ED%99%94%ED%95%99%EC%8A%B5%EC%9D%B4%EB%9E%80</a>
[7] 강화학습 개념부터 Deep Q Networks까지, 10분만에 훑어보기 <a href="https://jeinalog.tistory.com/20">https://jeinalog.tistory.com/20</a>
[8] 딥러닝 (8) - [RL1] 강화학습(Reinforcement Learning)이란? <a href="https://davinci-ai.tistory.com/31">https://davinci-ai.tistory.com/31</a>
[9] Q: 강화 학습이란 무엇인가요? - AWS <a href="https://aws.amazon.com/ko/what-is/reinforcement-learning/">https://aws.amazon.com/ko/what-is/reinforcement-learning/</a>
[10] 강화학습 - 나무위키 <a href="https://namu.wiki/w/%EA%B0%95%ED%99%94%ED%95%99%EC%8A%B5">https://namu.wiki/w/%EA%B0%95%ED%99%94%ED%95%99%EC%8A%B5</a></p>
<h4 id="_10">대표적인 강화학습 알고리즘<a class="headerlink" href="#_10" title="Permanent link">&para;</a></h4>
<ol>
<li><strong>Q-러닝 (Q-Learning)</strong></li>
<li>가치 기반 강화학습 알고리즘으로, 에이전트가 상태-행동 쌍의 가치를 학습하여 최적의 행동 정책을 찾음</li>
<li>
<p>오프폴리시(off-policy) 방식으로, 현재 정책이 아닌 최적 정책에 맞게 학습</p>
</li>
<li>
<p><strong>SARSA (State-Action-Reward-State-Action)</strong></p>
</li>
<li>온폴리시(on-policy) 방식의 가치 기반 알고리즘으로, 에이전트가 따르는 정책에 따라 학습</li>
<li>
<p>Q-러닝보다 더 안정적으로 학습하지만 최적 정책 탐색이 느릴 수 있음</p>
</li>
<li>
<p><strong>DQN (Deep Q-Network)</strong></p>
</li>
<li>Q-러닝에 딥러닝을 결합한 알고리즘</li>
<li>
<p>신경망을 이용해 Q-값을 근사하여 고차원 입력 데이터나 복잡한 환경에서 효과적임</p>
</li>
<li>
<p><strong>REINFORCE</strong></p>
</li>
<li>정책 기반(Policy-Based) 강화학습 기법</li>
<li>
<p>직접 정책을 학습하며 확률적 정책을 사용해 행동 선택</p>
</li>
<li>
<p><strong>A2C, A3C (Advantage Actor-Critic, Asynchronous Advantage Actor-Critic)</strong></p>
</li>
<li>액터-크리틱(actor-critic) 구조를 이용하는 정책 기반 알고리즘</li>
<li>
<p>액터는 정책을, 크리틱은 가치 함수를 동시에 학습</p>
</li>
<li>
<p><strong>SAC (Soft Actor-Critic)</strong></p>
</li>
<li>최대 엔트로피 강화학습 기법으로 안정적이며 샘플 효율성이 높음</li>
<li>
<p>연속적인 행동 공간에 적합</p>
</li>
<li>
<p><strong>모델 기반 강화학습 (Model-Based RL)</strong></p>
</li>
<li>환경 모델을 학습하여 시뮬레이션 기반으로 최적 행동을 탐색</li>
<li>예: Dyna-Q, MBPO 등</li>
</ol>
<p>이들 알고리즘은 다양한 환경과 문제 유형에 맞게 선택되고, 심층 강화학습(Deep RL) 기술과 결합되며 자율주행, 게임 AI, 로보틱스 등 다양한 분야에서 활발히 활용됩니다.</p>
<p>출처
[1] 강화학습 알고리즘의 종류(분류) - DACON <a href="https://dacon.io/forum/406104">https://dacon.io/forum/406104</a>
[2] [인공지능] 강화학습 기법 - 종류와 해당 알고리즘 정리 - 매석의 메모장 <a href="https://maeseok.tistory.com/135">https://maeseok.tistory.com/135</a>
[3] 강화학습 알고리즘 종류와 특징 - 건강 생활 - 티스토리 <a href="https://healthlife88.tistory.com/entry/%EA%B0%95%ED%99%94%ED%95%99%EC%8A%B5-%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98-%EC%A2%85%EB%A5%98%EC%99%80-%ED%8A%B9%EC%A7%95">https://healthlife88.tistory.com/entry/%EA%B0%95%ED%99%94%ED%95%99%EC%8A%B5-%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98-%EC%A2%85%EB%A5%98%EC%99%80-%ED%8A%B9%EC%A7%95</a>
[4] RL 알고리즘의 종류 : Model-Free vs Model-Based - deeep - 티스토리 <a href="https://dalpo0814.tistory.com/52">https://dalpo0814.tistory.com/52</a>
[5] 4. 강화 학습(Reinforcement Learning) 알고리즘 - 내 삶속 AI - 위키독스 <a href="https://wikidocs.net/236199">https://wikidocs.net/236199</a>
[6] [강화학습] REINFORCE 알고리즘 : 개념 및 수식 - HIGHQUAL <a href="https://mengu.tistory.com/136">https://mengu.tistory.com/136</a>
[7] 강화학습 - 나무위키 <a href="https://namu.wiki/w/%EA%B0%95%ED%99%94%ED%95%99%EC%8A%B5">https://namu.wiki/w/%EA%B0%95%ED%99%94%ED%95%99%EC%8A%B5</a>
[8] 강화학습(Reinforcement Learning)이란 무엇일까? - 인공지능 연구소 <a href="https://minsuksung-ai.tistory.com/13">https://minsuksung-ai.tistory.com/13</a>
[9] 적절한 머신러닝 알고리즘을 선택하는 방법 | 블로그 - 모두의연구소 <a href="https://modulabs.co.kr/blog/choosing-right-machine-learning-algorithm">https://modulabs.co.kr/blog/choosing-right-machine-learning-algorithm</a></p>
<hr />
<h3 id="self-supervised-learning">Self-supervised learning<a class="headerlink" href="#self-supervised-learning" title="Permanent link">&para;</a></h3>
<p><strong>Self-Supervised Learning(자기지도학습)</strong>입니다.
- <strong>Self-Supervised Learning</strong>은 <strong>라벨이 없는 대규모 데이터</strong>에서 스스로 학습 신호를 만들어내어, 별도의 수작업 라벨 없이도 효과적으로 모델을 학습시키는 AI 방법입니다[1][6][8][9].
- 이 방식은 <strong>표현 사전학습(Pretraining)</strong> 단계에서 라벨 없는 데이터를 활용해 인코더를 학습하고, 이후 <strong>소량의 라벨 데이터</strong>로 미세조정(Fine-tuning)하여 특정 과제를 수행합니다[1][4][9].
- 대표적으로 InfoNCE 등 <strong>contrastive(대비) 손실</strong>을 활용하며, 음성·이미지·자연어 등 다양한 분야에서 <strong>라벨링 비용 절감</strong>과 <strong>전이 학습</strong> 효과로 주목받고 있습니다[1][4][9].</p>
<p>출처
[1] Self-Supervised Learning: 최소한의 라벨로 학습하는 AI 방법 - AI꿀정보 <a href="https://lifestyleimformation.tistory.com/entry/Self-Supervised-Learning-%EC%B5%9C%EC%86%8C%ED%95%9C%EC%9D%98-%EB%9D%BC%EB%B2%A8%EB%A1%9C-%ED%95%99%EC%8A%B5%ED%95%98%EB%8A%94-AI-%EB%B0%A9%EB%B2%95">https://lifestyleimformation.tistory.com/entry/Self-Supervised-Learning-%EC%B5%9C%EC%86%8C%ED%95%9C%EC%9D%98-%EB%9D%BC%EB%B2%A8%EB%A1%9C-%ED%95%99%EC%8A%B5%ED%95%98%EB%8A%94-AI-%EB%B0%A9%EB%B2%95</a>
[2] 2-3-3 딥러닝의 학습 방법 - AI와 클라우드컴퓨팅 입문 - 위키독스 <a href="https://wikidocs.net/240099">https://wikidocs.net/240099</a>
[3] 머신러닝이란 무엇인가요? <a href="https://channel.io/ko/blog/articles/what-is-machine-learning-6f0de76a">https://channel.io/ko/blog/articles/what-is-machine-learning-6f0de76a</a>
[4] 대형 사전훈련 모델의 파인튜닝을 통한 강건한 한국어 음성인식 모델 ... <a href="https://www.eksss.org/archive/view_article?pid=pss-15-3-75">https://www.eksss.org/archive/view_article?pid=pss-15-3-75</a>
[5] [논문리뷰] Robust Speech Recognition via Large-Scale Weak ... <a href="https://kimjy99.github.io/%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0/whisper/">https://kimjy99.github.io/%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0/whisper/</a>
[6] 자기 지도 학습이란 무엇인가요? - IBM <a href="https://www.ibm.com/kr-ko/think/topics/self-supervised-learning">https://www.ibm.com/kr-ko/think/topics/self-supervised-learning</a>
[7] 수동 어노테이션의 한계를 극복하는 기술 4가지 -오토레이블링, 자기 ... <a href="https://ahha.ai/2023/12/19/autolabeling/">https://ahha.ai/2023/12/19/autolabeling/</a>
[8] 자연어 처리 13강 - Self Supervised Learning 1 - 공대생 도전 일지 <a href="https://yoonschallenge.tistory.com/584">https://yoonschallenge.tistory.com/584</a>
[9] 자기지도학습(Self-supervised Learning) - C's Shelter - 티스토리 <a href="https://gnuhcjh.tistory.com/49">https://gnuhcjh.tistory.com/49</a></p>
<h4 id="self-supervised-learningssl">Self-Supervised Learning(SSL) 방법의 두 단계 상세 설명<a class="headerlink" href="#self-supervised-learningssl" title="Permanent link">&para;</a></h4>
<h5 id="1-pretraining-with-unlabeled-data">1. 대규모 라벨 없는 데이터로 표현 사전학습 (Pretraining with Unlabeled Data)<a class="headerlink" href="#1-pretraining-with-unlabeled-data" title="Permanent link">&para;</a></h5>
<ul>
<li><strong>목적:</strong><br />
  라벨이 없는 대규모 데이터(예: 음성, 이미지, 텍스트 등)에서 유용한 특징(Feature) 표현을 자동으로 학습합니다.</li>
<li><strong>학습 방식:</strong>  </li>
<li>입력 데이터에서 인위적으로 학습 신호를 생성합니다.</li>
<li>예시: 같은 음성 클립에서 두 가지 서로 다른 변환(증강)을 적용해 파형 쌍 <span class="arithmatex">\(<span class="arithmatex">\((x, x')\)</span>\)</span>를 만듭니다.</li>
<li>인코더(Encoder)는 이 두 파형의 표현이 서로 가깝도록 학습합니다.</li>
<li><strong>대표적 손실 함수:</strong>  </li>
<li><strong>InfoNCE</strong>: 대표적인 contrastive(대비) 손실로, 같은 클립 쌍은 유사하게, 다른 클립은 멀어지도록 만듭니다.</li>
<li><strong>MSE(Mean Squared Error)</strong> 등 유사도 기반 손실도 사용됩니다.</li>
<li><strong>장점:</strong>  </li>
<li>라벨이 없는 데이터만으로도 강력한 표현을 학습할 수 있어, 대규모 수작업 라벨링 비용이 절감됩니다.</li>
<li>다양한 변환(Noise, Time Stretch, Masking 등)을 통해 모델이 견고한 특징을 익힙니다.</li>
</ul>
<h5 id="2-fine-tuning-with-labeled-data">2. 소량 라벨 데이터로 미세조정 (Fine-tuning with Labeled Data)<a class="headerlink" href="#2-fine-tuning-with-labeled-data" title="Permanent link">&para;</a></h5>
<ul>
<li><strong>목적:</strong><br />
  사전학습된 인코더의 파라미터를 이어받아, 소량의 라벨이 달린 데이터로 실제 과제를 수행하도록 미세조정합니다.</li>
<li><strong>학습 방식:</strong>  </li>
<li>인코더 가중치(θ)를 초기값으로 사용하고, 라벨 데이터(예: (x, y))로 분류기 등 다운스트림 모델을 학습합니다.</li>
<li>이때는 정답 라벨이 필요합니다.</li>
<li><strong>대표적 손실 함수:</strong>  </li>
<li><strong>CE(Cross Entropy)</strong>: 분류 문제에서 널리 사용.</li>
<li><strong>CTC(Connectionist Temporal Classification)</strong>: 음성 인식 등 시퀀스 라벨링에서 사용.</li>
<li><strong>장점:</strong>  </li>
<li>소량의 라벨 데이터만으로도 높은 성능을 달성할 수 있습니다.</li>
<li>사전학습된 표현 덕분에 데이터 효율성이 크게 향상됩니다.</li>
</ul>
<h4 id="_11">요약 표<a class="headerlink" href="#_11" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>단계</th>
<th>사용 데이터</th>
<th>학습 목표</th>
<th>주요 손실 함수</th>
<th>특징</th>
</tr>
</thead>
<tbody>
<tr>
<td>표현 사전학습</td>
<td>라벨 없는 대규모 데이터</td>
<td>유용한 특징 표현 학습</td>
<td>InfoNCE, MSE 등</td>
<td>라벨 불필요, 데이터 다양성 활용</td>
</tr>
<tr>
<td>과제 미세조정</td>
<td>소량 라벨 데이터</td>
<td>실제 과제 성능 극대화</td>
<td>CE, CTC 등</td>
<td>라벨 필요, 사전학습 인코더 사용</td>
</tr>
</tbody>
</table>
<h4 id="_12">실제 예시<a class="headerlink" href="#_12" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>음성:</strong>  </li>
<li>사전학습: 수십만 시간의 라벨 없는 음성으로 wav2vec, HuBERT 등 모델 학습  </li>
<li>미세조정: 수천 개 라벨 음성으로 음소/단어 인식기 학습</li>
<li><strong>이미지:</strong>  </li>
<li>사전학습: 이미지 일부 가리기(Masking) 등으로 특징 추출  </li>
<li>미세조정: 소량 라벨 이미지로 분류기 학습</li>
</ul>
<p>이처럼 SSL은 대규모 비라벨 데이터에서 표현을 먼저 익히고, 소량 라벨 데이터로 실제 과제에 맞게 모델을 미세조정하는 방식입니다.</p>
<h2 id="2">2. 학습 데이터 준비<a class="headerlink" href="#2" title="Permanent link">&para;</a></h2>
<h3 id="train-validation-test">데이터 분할 (Train / Validation / Test)<a class="headerlink" href="#train-validation-test" title="Permanent link">&para;</a></h3>
<p>머신러닝 및 AI 모델 학습에서 데이터 분할은 데이터를 학습과 평가에 적절히 사용하는 중요한 단계입니다. 전체 데이터를 여러 집합으로 나누어 각각의 역할을 분명히 함으로써 모델 성능을 정확하게 평가하고, 과적합(overfitting)을 방지합니다.</p>
<h4 id="_13">분할 구성<a class="headerlink" href="#_13" title="Permanent link">&para;</a></h4>
<ul>
<li>
<p><strong>Train Set (학습용 데이터)</strong><br />
  모델이 패턴과 관계를 학습하는 데 사용하는 데이터입니다. 이 데이터로 모델의 파라미터가 조정됩니다.</p>
</li>
<li>
<p><strong>Validation Set (검증용 데이터)</strong><br />
  학습 중간에 모델의 성능을 확인하고, 하이퍼파라미터 조정이나 모델 선택 등에 사용되는 데이터입니다. 학습에는 직접적으로 사용되지 않지만 모델 성능을 빠르게 피드백받는 데 활용됩니다.</p>
</li>
<li>
<p><strong>Test Set (평가용 데이터)</strong><br />
  학습과 검증이 완료된 후, 최종적으로 모델이 얼마나 잘 작동하는지 객관적으로 평가하는 데 사용하는 데이터입니다. 이 데이터는 절대 학습이나 검증에 사용하지 않으며, 한 번만 평가에 사용됩니다.</p>
</li>
</ul>
<h4 id="_14">일반적인 분할 비율<a class="headerlink" href="#_14" title="Permanent link">&para;</a></h4>
<ul>
<li>보통 전체 데이터셋을 <strong>Train : Validation : Test = 6 : 2 : 2</strong> 또는 60% : 20% : 20% 정도로 나누는 경우가 많습니다.</li>
<li>Train 데이터 내에서 Validation 데이터를 분할하는 경우도 있으며, 데이터가 적을 땐 교차검증(K-fold cross-validation) 기법을 활용해 데이터 효율을 높이기도 합니다.</li>
</ul>
<h4 id="_15">분할의 중요성<a class="headerlink" href="#_15" title="Permanent link">&para;</a></h4>
<ul>
<li>Train과 Test만 분할할 경우, 모델 성능을 한 번만 확인할 수 있고, 테스트 결과를 토대로 모델을 수정하면 과적합 위험이 커집니다.</li>
<li>Validation 데이터를 두면 모델 성능을 중간 점검 하면서 최적화할 수 있어 진짜 일반화 성능을 더 잘 측정할 수 있습니다.</li>
<li>Test 데이터는 학습 및 검증에 전혀 관여하지 않아야, 실제 환경에서의 성능을 객관적으로 평가할 수 있습니다.</li>
</ul>
<h4 id="_16">요약<a class="headerlink" href="#_16" title="Permanent link">&para;</a></h4>
<ol>
<li><strong>Train</strong>: 모델 학습에 사용  </li>
<li><strong>Validation</strong>: 학습 중 하이퍼파라미터 튜닝 및 중간 평가용  </li>
<li><strong>Test</strong>: 학습 완료 후 최종 성능 평가용  </li>
</ol>
<p>이처럼 데이터 분할은 머신러닝 모델의 성능을 신뢰성 있게 평가하고 개선하는 데 필수적인 절차입니다.</p>
<p>출처
[1] Train, Validation, and Test Set - 포자랩스의 기술 블로그 <a href="https://pozalabs.github.io/Dataset_Splitting/">https://pozalabs.github.io/Dataset_Splitting/</a>
[2] 데이터셋 나누기와 모델 검증 - velog <a href="https://velog.io/@minjung00/%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%85%8B-%EB%82%98%EB%88%84%EA%B8%B0%EC%99%80-%EB%AA%A8%EB%8D%B8-%EA%B2%80%EC%A6%9D">https://velog.io/@minjung00/%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%85%8B-%EB%82%98%EB%88%84%EA%B8%B0%EC%99%80-%EB%AA%A8%EB%8D%B8-%EA%B2%80%EC%A6%9D</a>
[3] [Machine Learning][머신러닝] 데이터셋 나누기와 교차검증 <a href="https://ysyblog.tistory.com/69">https://ysyblog.tistory.com/69</a>
[4] [머신러닝&amp;딥러닝] Train / Validation / Test 의 차이 - JoJo's Study Blog <a href="https://wkddmswh99.tistory.com/10">https://wkddmswh99.tistory.com/10</a>
[5] validation set이란? test set과의 차이점과 사용 방법 <a href="https://for-my-wealthy-life.tistory.com/19">https://for-my-wealthy-life.tistory.com/19</a>
[6] 데이터셋 나누기 &amp; 교차 검증 - 코딩하는 오리 - 티스토리 <a href="https://cori.tistory.com/162">https://cori.tistory.com/162</a>
[7] Sklearn 익히기 - train/test data set 분리 및 Cross Validation <a href="https://libertegrace.tistory.com/entry/Sklearn-%EC%9D%B5%ED%9E%88%EA%B8%B0-trainvaltest-data-set-%EB%B6%84%EB%A6%AC-%EB%B0%8F-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%A0%84%EC%B2%98%EB%A6%AC">https://libertegrace.tistory.com/entry/Sklearn-%EC%9D%B5%ED%9E%88%EA%B8%B0-trainvaltest-data-set-%EB%B6%84%EB%A6%AC-%EB%B0%8F-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%A0%84%EC%B2%98%EB%A6%AC</a>
[8] [데이터 전처리] 훈련 및 테스트 데이터 분할 - Smalldata Lab <a href="https://smalldatalab.tistory.com/21">https://smalldatalab.tistory.com/21</a></p>
<hr />
<h3 id="cross-validation">교차검증(Cross Validation)<a class="headerlink" href="#cross-validation" title="Permanent link">&para;</a></h3>
<p>교차 검증(Cross Validation)은 머신러닝에서 모델의 성능을 더 정확하고 일반화되게 평가하기 위해 데이터를 여러 번 나누어 학습과 검증을 반복하는 방법입니다. 단순히 한 번만 학습 데이터와 검증 데이터로 나누는 것이 아니라, 여러 번 나누어 각각의 경우에 대해 모델을 학습시키고 평가하여 평균 성능을 구합니다[1][2][4].</p>
<h4 id="_17">왜 교차 검증을 사용하는가?<a class="headerlink" href="#_17" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>과적합(Overfitting) 방지</strong>: 모델이 특정 데이터셋에만 잘 맞는 현상을 줄이고, 다양한 데이터 분할에서의 성능을 확인할 수 있습니다[3][2].</li>
<li><strong>일반화 성능 평가</strong>: 여러 번 나누어 평가함으로써, 전체 데이터에 대한 모델의 일반화 능력을 더 정확히 측정할 수 있습니다[1][2].</li>
<li><strong>데이터 부족 시 효과적</strong>: 데이터가 적을 때도 모든 데이터를 학습과 검증에 최대한 활용할 수 있습니다[1][2].</li>
</ul>
<h4 id="_18">대표적인 교차 검증 방법<a class="headerlink" href="#_18" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>방법명</th>
<th>설명</th>
</tr>
</thead>
<tbody>
<tr>
<td>K-Fold 교차 검증</td>
<td>데이터를 K개의 폴드로 나누고, 각 폴드가 한 번씩 검증 데이터셋이 되어 K번 반복 평가[1][2][4][9].</td>
</tr>
<tr>
<td>Stratified K-Fold</td>
<td>K-Fold와 같지만, 각 폴드에 클래스 비율이 동일하게 유지되도록 분할(불균형 데이터에 효과적)[1][2].</td>
</tr>
<tr>
<td>Leave-One-Out (LOOCV)</td>
<td>데이터 한 개를 검증 데이터로, 나머지를 학습 데이터로 하여 N번(데이터 개수만큼) 반복 평가[2].</td>
</tr>
<tr>
<td>Nested CV</td>
<td>바깥쪽과 안쪽에 각각 교차 검증을 적용하여 하이퍼파라미터 튜닝과 모델 평가를 동시에 진행[5].</td>
</tr>
</tbody>
</table>
<h4 id="k-fold">K-Fold 교차 검증 예시<a class="headerlink" href="#k-fold" title="Permanent link">&para;</a></h4>
<ol>
<li>데이터를 K개(예: 5개)로 나눕니다.</li>
<li>각 폴드가 한 번씩 검증 데이터가 되고, 나머지 폴드는 학습 데이터가 됩니다.</li>
<li>이 과정을 K번 반복하여 K개의 평가 결과를 얻고, 평균을 내어 최종 성능을 평가합니다[1][2][4][10].</li>
</ol>
<pre><code class="language-python">from sklearn.model_selection import cross_val_score
from sklearn.tree import DecisionTreeClassifier
from sklearn.datasets import load_iris

iris = load_iris()
model = DecisionTreeClassifier()
scores = cross_val_score(model, iris.data, iris.target, cv=5)
print('교차 검증별 점수:', scores)
print('평균 점수:', scores.mean())
</code></pre>
<h4 id="_19">장점과 단점<a class="headerlink" href="#_19" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>장점</th>
<th>단점</th>
</tr>
</thead>
<tbody>
<tr>
<td>데이터셋 전체를 학습/검증에 모두 활용 가능</td>
<td>반복 학습으로 인해 시간 소요가 큼</td>
</tr>
<tr>
<td>과적합/과소적합 탐지 및 일반화 성능 향상</td>
<td>데이터 분할 방법에 따라 성능 편차 발생</td>
</tr>
<tr>
<td>데이터가 적을 때도 효과적으로 평가 가능</td>
<td>시계열 데이터 등 순서가 중요한 경우 주의</td>
</tr>
</tbody>
</table>
<h4 id="k-fold_1">계층별(층화) K-Fold 교차 검증<a class="headerlink" href="#k-fold_1" title="Permanent link">&para;</a></h4>
<ul>
<li>데이터 클래스(레이블) 비율이 불균형할 때 각 폴드에 클래스 비율이 고르게 분포되도록 나누는 방식입니다.</li>
<li>예를 들어, 사기 거래가 전체의 5%라면 각 폴드에도 사기 거래가 5%씩 포함되도록 분할합니다[1][2].</li>
</ul>
<h4 id="_20">요약<a class="headerlink" href="#_20" title="Permanent link">&para;</a></h4>
<ul>
<li>교차 검증은 데이터를 여러 번 나누어 모델을 학습·평가하여, 성능을 평균적으로 평가하는 방법입니다.</li>
<li>대표적으로 K-Fold, Stratified K-Fold, LOOCV, Nested CV 등이 있습니다.</li>
<li>과적합 방지, 일반화 성능 평가, 데이터 부족 상황에서 효과적이지만, 반복 학습으로 시간이 더 걸릴 수 있습니다[1][2][4].</li>
</ul>
<p>출처
[1] [Machine learning] 쉽게 설명하는 Cross Validation 교차검증 <a href="https://huidea.tistory.com/30">https://huidea.tistory.com/30</a>
[2] [머신러닝] 교차검증 (Cross Validation) - velog <a href="https://velog.io/@soo_oo/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-%EA%B5%90%EC%B0%A8%EA%B2%80%EC%A6%9D-Cross-Validation">https://velog.io/@soo_oo/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-%EA%B5%90%EC%B0%A8%EA%B2%80%EC%A6%9D-Cross-Validation</a>
[3] 교차 검증(Cross Validation) : 네이버 블로그 <a href="https://blog.naver.com/ckdgus1433/221599517834">https://blog.naver.com/ckdgus1433/221599517834</a>
[4] [기계학습] 교차검증(Cross Validation) <a href="https://gsbang.tistory.com/entry/%EA%B8%B0%EA%B3%84%ED%95%99%EC%8A%B5-%EA%B5%90%EC%B0%A8%EA%B2%80%EC%A6%9DCross-Validation">https://gsbang.tistory.com/entry/%EA%B8%B0%EA%B3%84%ED%95%99%EC%8A%B5-%EA%B5%90%EC%B0%A8%EA%B2%80%EC%A6%9DCross-Validation</a>
[5] Nested cross validation - IBOK - 티스토리 <a href="https://bo-10000.tistory.com/85">https://bo-10000.tistory.com/85</a>
[6] [Recommend] K-Fold Cross Validation(CV, 교차 검증)의 개념 - velog <a href="https://velog.io/@recoder/Cross-Validation">https://velog.io/@recoder/Cross-Validation</a>
[7] [머신러닝] 크로스 밸리데이션(cross validation, 교차 검증)의 개념, 의미 <a href="https://losskatsu.github.io/machine-learning/cross-validation/">https://losskatsu.github.io/machine-learning/cross-validation/</a>
[8] [바람돌이/머신러닝] 교차검증(CV), Cross Validation, K-fold ... <a href="https://blog.naver.com/PostView.nhn?blogId=winddori2002&amp;logNo=221850530979">https://blog.naver.com/PostView.nhn?blogId=winddori2002&amp;logNo=221850530979</a>
[9] k-겹 교차 검증 - 인코덤, 생물정보 전문위키 <a href="https://incodom.kr/k-%EA%B2%B9_%EA%B5%90%EC%B0%A8_%EA%B2%80%EC%A6%9D">https://incodom.kr/k-%EA%B2%B9_%EA%B5%90%EC%B0%A8_%EA%B2%80%EC%A6%9D</a>
[10] [ML/DL] 교차 검증(Cross Validation) - K-Fold ... - Sonstory - 티스토리 <a href="https://sonstory.tistory.com/29">https://sonstory.tistory.com/29</a></p>
<h3 id="_21"><a href="https://github.com/zoro0rkd/ai_study/wiki/AI-모델-튜닝-%E2%80%90-NEW#3-클래스-불균형class-imbalanced-문제-해결">데이터 불균형 처리</a><a class="headerlink" href="#_21" title="Permanent link">&para;</a></h3>
<ul>
<li>데이터 불균형이란 특정 클래스(주로 소수 클래스)의 샘플 수가 매우 적어 학습 시 모델이 다수 클래스에 치우쳐 성능 저하를 일으키는 문제를 의미합니다.</li>
<li>주요 처리 방법으로는  </li>
<li><strong>오버샘플링(Over-sampling)</strong>: 소수 클래스 데이터를 인위적으로 복제하거나 생성(SMOTE 같은 합성 샘플 생성 기법 포함)하여 균형 맞추기  </li>
<li><strong>언더샘플링(Under-sampling)</strong>: 다수 클래스 데이터를 줄여 소수 클래스와 균형 맞추기 (정보 손실 가능성 주의)  </li>
<li><strong>앙상블 기법</strong>: 여러 모델의 예측을 통합해 불균형 영향을 완화 (예: 랜덤 포레스트, 부스팅)  </li>
<li><strong>비용 민감 학습(Cost-sensitive Learning)</strong>: 소수 클래스 오류에 더 큰 가중치를 부여하여 학습 시 반영  </li>
<li><strong>데이터 수집 확대</strong>: 추가 데이터 확보로 클래스 불균형 완화</li>
<li>적절한 평가지표(재현율, 정밀도, F1 점수 등)를 함께 사용해야 실질적 성능 향상을 확인할 수 있음[1][4][7][8].</li>
</ul>
<h3 id="_22"><a href="https://github.com/zoro0rkd/ai_study/wiki/데이터-증강-%E2%80%90-NEW">데이터 증강</a>과 <a href="https://github.com/zoro0rkd/ai_study/wiki/데이터-정제-%E2%80%90-NEW">전처리</a>의 학습 영향<a class="headerlink" href="#_22" title="Permanent link">&para;</a></h3>
<ul>
<li>데이터 증강(Data Augmentation)은 기존 학습 데이터를 변형(회전, 자르기, 잡음 추가 등)하여 다양한 학습 사례를 만들어내는 기법으로, 모델 일반화 성능 개선에 필수적입니다.</li>
<li>특히 이미지는 물론 텍스트, 음성 등 다양한 데이터 유형에 맞춘 증강 기법들이 발전하고 있음  </li>
<li>전처리(Preprocessing)는 노이즈 제거, 이상치 처리, 정규화, 토큰화 같은 데이터의 품질을 향상시키는 작업으로, 데이터 품질이 높을수록 학습 성능 향상 및 안정성 확보 가능  </li>
<li>잘 설계된 증강과 전처리 과정은 데이터 부족 및 불균형 문제를 완화하고, 과적합 방지, 학습 효율 개선에 기여함</li>
</ul>
<p>요약하면, 데이터 불균형 처리와 적절한 증강 및 전처리는 모델이 다양한 상황에서 안정적으로 작동하도록 하는데 필수적이며, 최종 성능 향상을 위한 중요한 초석입니다.</p>
<p>출처
[1] 불균형 데이터 (imbalanced data) 처리를 위한 샘플링 기법 <a href="https://casa-de-feel.tistory.com/15">https://casa-de-feel.tistory.com/15</a>
[2] [논문]머신러닝을 위한 불균형 데이터 처리 방법 <a href="https://scienceon.kisti.re.kr/srch/selectPORSrchArticle.do?cn=JAKO201900937327481">https://scienceon.kisti.re.kr/srch/selectPORSrchArticle.do?cn=JAKO201900937327481</a>
[3] 머신러닝 데이터 세트의 불균형 클래스와 싸우기 위한 8가지 ... <a href="https://www.nepirity.com/blog/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/">https://www.nepirity.com/blog/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/</a>
[4] 불균형 데이터(Imbalanced Data) 머신러닝 Classification ... <a href="https://datasciencediary.tistory.com/entry/%EB%B6%88%EA%B7%A0%ED%98%95-%EB%8D%B0%EC%9D%B4%ED%84%B0-Imbalanced-Data-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-Classification-%EB%AC%B8%EC%A0%9C%EC%A0%90-%ED%95%B4%EA%B2%B0%EB%B0%A9%EB%B2%95">https://datasciencediary.tistory.com/entry/%EB%B6%88%EA%B7%A0%ED%98%95-%EB%8D%B0%EC%9D%B4%ED%84%B0-Imbalanced-Data-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-Classification-%EB%AC%B8%EC%A0%9C%EC%A0%90-%ED%95%B4%EA%B2%B0%EB%B0%A9%EB%B2%95</a>
[5] 머신러닝을 위한 불균형 데이터 처리 방법 : 샘플링을 위주로 <a href="https://koreascience.kr/article/JAKO201900937327481.pdf">https://koreascience.kr/article/JAKO201900937327481.pdf</a>
[6] 불균형 데이터(imbalanced data) <a href="https://raziel.oopy.io/12a6fa0e-30de-80cd-9a8a-d05782acc94b">https://raziel.oopy.io/12a6fa0e-30de-80cd-9a8a-d05782acc94b</a>
[7] 불균형 데이터 처리, 언더 샘플링, 오버 샘플링 <a href="https://eewjddms.tistory.com/87">https://eewjddms.tistory.com/87</a>
[8] 머신러닝에서의 핵심 전략들 Imbalanced Dataset - 데이터 AI 벌집 <a href="https://datasciencebeehive.tistory.com/76">https://datasciencebeehive.tistory.com/76</a>
[9] 머신러닝을 위한 불균형 데이터 처리 방법 : 샘플링을 위주로 <a href="https://www.kci.go.kr/kciportal/ci/sereArticleSearch/ciSereArtiView.kci?sereArticleSearchBean.artiId=ART002527922">https://www.kci.go.kr/kciportal/ci/sereArticleSearch/ciSereArtiView.kci?sereArticleSearchBean.artiId=ART002527922</a>
[10] 불균형 데이터(imbalanced data)란 무엇이고, 무엇이 문제인가? <a href="https://rfriend.tistory.com/773">https://rfriend.tistory.com/773</a></p>
<h2 id="3">3. 모델 학습 과정<a class="headerlink" href="#3" title="Permanent link">&para;</a></h2>
<h3 id="weight-initialization">초기화 (Weight Initialization)<a class="headerlink" href="#weight-initialization" title="Permanent link">&para;</a></h3>
<p>신경망에서 <strong>Weight Initialization(가중치 초기화)</strong>는 각 층의 가중치를 학습 시작 전에 어떤 값으로 정할지 결정하는 과정입니다. 주요 목적은 학습의 효율성을 높이고, 기울기 소실이나 기울기 폭주 같은 문제를 방지하는 데 있습니다.</p>
<h4 id="_23">왜 가중치 초기화가 중요한가?<a class="headerlink" href="#_23" title="Permanent link">&para;</a></h4>
<ul>
<li>모든 가중치를 <strong>0</strong>이나 동일한 값으로 초기화하면, 각 뉴런이 똑같이 학습되어 대칭성이 깨지지 않아 모델이 무의미해집니다.</li>
<li>가중치가 너무 크거나 작게 설정되면, 역전파를 할 때 기울기가 0에 가까워지거나(기울기 소실), 폭발적으로 커져서 학습이 불안정해질 수 있습니다.</li>
</ul>
<h4 id="_24">대표적인 가중치 초기화 방법과 예시<a class="headerlink" href="#_24" title="Permanent link">&para;</a></h4>
<h5 id="1">1. <strong>균등 분포/정규 분포 무작위 초기화</strong><a class="headerlink" href="#1" title="Permanent link">&para;</a></h5>
<ul>
<li>각 가중치를 <code>-a</code>에서 <code>a</code> 사이(균등 분포) 또는 평균 0, 표준편차 σ(정규 분포)로 랜덤하게 설정합니다.</li>
<li>예시:</li>
<li><strong>Uniform:</strong> $$ w \sim U(-0.05, 0.05) $$</li>
<li><strong>Normal:</strong> $$ w \sim N(0, 0.01^2) $$</li>
<li>장점: 간단하고 빠름.</li>
<li>단점: 네트워크가 깊어질수록 기울기 문제가 생길 수 있음.</li>
</ul>
<h5 id="2-xavier-glorot-initialization">2. <strong>Xavier 초기화 (Glorot Initialization)</strong><a class="headerlink" href="#2-xavier-glorot-initialization" title="Permanent link">&para;</a></h5>
<ul>
<li>주로 <strong>Sigmoid, Tanh</strong> 활성화 함수에서 사용.</li>
<li>각 층의 입력 개수(<span class="arithmatex">\(<span class="arithmatex">\(n_{in}\)</span>\)</span>)와 출력 개수(<span class="arithmatex">\(<span class="arithmatex">\(n_{out}\)</span>\)</span>)에 따라 분포의 범위를 조정하여, 신호가 각 층에서 적절히 전달되도록 함.</li>
<li>예시:</li>
<li><strong>균등분포:</strong> $$ w \sim U\left(-\frac{\sqrt{6}}{\sqrt{n_{in} + n_{out}}}, \frac{\sqrt{6}}{\sqrt{n_{in} + n_{out}}}\right) $$</li>
<li><strong>정규분포:</strong> $$ w \sim N\left(0, \frac{2}{n_{in} + n_{out}}\right) $$</li>
</ul>
<h5 id="3-he-kaiming-initialization">3. <strong>He 초기화 (Kaiming Initialization)</strong><a class="headerlink" href="#3-he-kaiming-initialization" title="Permanent link">&para;</a></h5>
<ul>
<li><strong>ReLU</strong> 계열 활성화 함수 사용할 때 적합.</li>
<li>입력 노드 개수(<span class="arithmatex">\(<span class="arithmatex">\(n_{in}\)</span>\)</span>)만 고려해서 더 큰 분산을 사용, ReLU가 많은 값을 0으로 만들 수 있기 때문.</li>
<li>예시:</li>
<li><strong>정규분포:</strong> $$ w \sim N\left(0, \frac{2}{n_{in}}\right) $$</li>
<li><strong>균등분포:</strong> $$ w \sim U\left(-\sqrt{\frac{6}{n_{in}}}, \sqrt{\frac{6}{n_{in}}}\right) $$</li>
</ul>
<h5 id="4-bias">4. <strong>Bias(바이어스) 초기화</strong><a class="headerlink" href="#4-bias" title="Permanent link">&para;</a></h5>
<ul>
<li>바이어스는 보통 0으로 초기화. 특별한 경우를 제외하면 문제 발생이 적음.</li>
</ul>
<h4 id="_25">핵심 정리<a class="headerlink" href="#_25" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>가중치 초기화</strong>는 신경망 학습의 첫걸음이며, 성능과 수렴 속도에 직접적인 영향을 줍니다.</li>
<li>네트워크 구조와 활성화 함수에 맞는 초기화 방식을 선택해야 효과적입니다.</li>
<li>실제로는 딥러닝 프레임워크(파이토치, 텐서플로우 등)에서 함수로 쉽게 적용할 수 있습니다.</li>
</ul>
<p>잘 설계된 가중치 초기화가 학습의 시작과 성능을 좌우한다는 점이 가장 중요합니다.</p>
<p>참고 : <a href="https://yngie-c.github.io/deep%20learning/2020/03/17/parameter_init/">https://yngie-c.github.io/deep%20learning/2020/03/17/parameter_init/</a></p>
<h3 id="forward-propagation">순전파(Forward Propagation)<a class="headerlink" href="#forward-propagation" title="Permanent link">&para;</a></h3>
<ul>
<li>순전파는 신경망에 입력값이 들어와 각 층을 거쳐 출력층까지 전달되는 과정입니다.</li>
<li>입력 데이터에 가중치(weight)를 곱하고 편향(bias)을 더한 뒤 활성화 함수(activation function)를 거치면서 다음 층으로 신호를 전달합니다.</li>
<li>이 과정에서 모델이 예측하는 출력값이 계산됩니다.</li>
<li>순전파는 입력에서 출력으로의 데이터 흐름이며, 모델의 예측을 생성하는 본질적인 단계입니다.</li>
</ul>
<h3 id="activation-function">활성 함수(Activation Function)<a class="headerlink" href="#activation-function" title="Permanent link">&para;</a></h3>
<h4 id="step-function">Step Function을 활성화 함수로 사용하지 않는 이유<a class="headerlink" href="#step-function" title="Permanent link">&para;</a></h4>
<h5 id="1_1">1. 미분 불가능성과 기울기 소실 문제<a class="headerlink" href="#1_1" title="Permanent link">&para;</a></h5>
<ul>
<li>딥러닝 학습은 <strong>경사 하강법(gradient descent)</strong>과 <strong>역전파(backpropagation)</strong>에 기반함.</li>
<li>역전파를 위해선 활성화 함수가 <strong>미분 가능</strong>해야 함.</li>
<li>하지만 <strong>Heaviside step function</strong>은:</li>
<li><span class="arithmatex">\( x = 0 \)</span>에서 <strong>미분 불가능</strong></li>
<li>그 외의 구간에서는 도함수가 <strong>0</strong> → <strong>기울기 소실(vanishing gradient)</strong> 문제 발생</li>
<li>따라서 가중치를 효과적으로 <strong>업데이트할 수 없음</strong> → 학습이 불가능</li>
</ul>
<h5 id="2_1">2. 이산적인 출력으로 인한 최적화 어려움<a class="headerlink" href="#2_1" title="Permanent link">&para;</a></h5>
<ul>
<li>신경망은 <strong>출력이 실제값에 가까워지도록</strong> 가중치와 편향을 조정함.</li>
<li>이를 위해선 <strong>작은 가중치 변화</strong>가 출력에 <strong>연속적인 영향을 줘야</strong> 함.</li>
<li>그러나 step 함수는 출력이 <strong>0 또는 1</strong>로 이산적(discrete)이므로:</li>
<li>입력 변화에 따른 출력 변화가 <strong>불연속적</strong></li>
<li><strong>경사 기반 최적화</strong>(gradient-based optimization)에 부적합</li>
</ul>
<h5 id="3-relu">3. ReLU와의 비교: 부분 미분 가능성<a class="headerlink" href="#3-relu" title="Permanent link">&para;</a></h5>
<ul>
<li><strong>ReLU(Rectified Linear Unit)</strong>도 <span class="arithmatex">\( x = 0 \)</span>에서 미분 불가능하지만:</li>
<li>대부분 구간에서 <strong>미분 가능</strong></li>
<li><strong>부분 도함수(subderivative)</strong> 등을 통해 학습 가능</li>
<li>반면, step 함수는 거의 모든 구간에서 도함수가 0 → 실질적 학습 불가</li>
</ul>
<hr />
<h5 id="_26">✅ 결론<a class="headerlink" href="#_26" title="Permanent link">&para;</a></h5>
<blockquote>
<p><strong>Step function은 출력이 이산적이고 도함수가 대부분 0이므로, 딥러닝에서의 학습에 적합하지 않음.</strong><br />
따라서 <strong>ReLU, sigmoid, tanh</strong> 등 <strong>연속적이며 미분 가능한 함수</strong>들이 활성화 함수로 사용됨.</p>
</blockquote>
<h4 id="activation-function_1">주요 Activation Function(활성화 함수) 장단점 비교표<a class="headerlink" href="#activation-function_1" title="Permanent link">&para;</a></h4>
<p><img alt="" src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZafDv3VUm60Eh10OeJu1vw.png" /></p>
<table>
<thead>
<tr>
<th>함수명</th>
<th>수식/출력 범위</th>
<th>장점</th>
<th>단점</th>
<th>주요 사용처/특징</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Step</strong></td>
<td>0 또는 1</td>
<td>구현이 간단, 이진 분류에 직관적</td>
<td>미분 불가(학습 불가), 다중 클래스 불가, gradient=0으로 역전파 불가</td>
<td>고전적 퍼셉트론, 실전에서는 거의 사용 안 함</td>
</tr>
<tr>
<td><strong>Linear</strong></td>
<td>실수 전체</td>
<td>회귀 문제에 적합, 출력 해석 용이</td>
<td>비선형성 없음, 레이어 쌓아도 단일 선형함수와 동일, 학습력 제한</td>
<td>회귀 문제 출력층, 은닉층에는 사용 안 함</td>
</tr>
<tr>
<td><strong>Sigmoid</strong></td>
<td>(0, 1)</td>
<td>출력값을 확률처럼 해석 가능, 이진 분류 출력층</td>
<td>gradient vanishing(기울기 소실), 출력 분포가 0에 치우침, 느린 수렴</td>
<td>이진 분류 출력층, 은닉층에는 잘 안 씀</td>
</tr>
<tr>
<td><strong>Tanh</strong></td>
<td>(-1, 1)</td>
<td>출력 평균이 0에 가까워 학습 빠름, Sigmoid보다 gradient vanishing 덜함</td>
<td>여전히 gradient vanishing 있음, 출력 saturate 구간에서 학습 느림</td>
<td>RNN 등 일부 구조 은닉층</td>
</tr>
<tr>
<td><strong>ReLU</strong></td>
<td>[0, ∞)</td>
<td>계산 단순, 빠른 학습, gradient vanishing 적음, deep network에 효과적</td>
<td>입력&lt;0에서 gradient=0(죽은 뉴런 문제), 음수 입력 무시</td>
<td>CNN, MLP 등 대부분의 은닉층</td>
</tr>
<tr>
<td><strong>Leaky ReLU</strong></td>
<td>x&lt;0: αx, x≥0: x</td>
<td>ReLU의 죽은 뉴런 문제 완화, 음수 영역도 gradient 존재</td>
<td>α값 선정 필요, 여전히 완벽하지 않음</td>
<td>ReLU 대체, 죽은 뉴런 방지 목적</td>
</tr>
<tr>
<td><strong>Softmax</strong></td>
<td>(0, 1), 합=1</td>
<td>다중 클래스 확률 분포 출력, 각 클래스 확률 해석</td>
<td>gradient vanishing, 출력값이 0/1에 가까워질수록 학습 어려움</td>
<td>다중 클래스 분류 출력층</td>
</tr>
<tr>
<td><strong>Swish/GELU</strong></td>
<td>비선형, 실수 전체</td>
<td>ReLU보다 부드러운 비선형성, deep network에서 성능 우수</td>
<td>계산 복잡, 최신 네트워크에서 주로 사용</td>
<td>최신 deep network, BERT 등</td>
</tr>
</tbody>
</table>
<hr />
<h4 id="_27">참고 및 요약<a class="headerlink" href="#_27" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>ReLU</strong>: 현재 가장 널리 사용되는 은닉층 활성화 함수로, 계산이 간단하고 gradient vanishing 문제가 적어 deep learning에 최적[1][4][5][6].</li>
<li><strong>Sigmoid, Tanh</strong>: 과거에는 많이 썼으나, gradient vanishing 문제가 심해 최근에는 출력층에만 사용[1][4][5].</li>
<li><strong>Leaky ReLU, Swish, GELU 등</strong>: ReLU의 단점을 보완하거나 deep network에서 성능을 높이기 위해 고안된 함수들[5][6].</li>
<li><strong>Softmax</strong>: 다중 클래스 분류의 출력층에서 표준적으로 사용[1][5].</li>
<li><strong>Linear</strong>: 회귀 문제의 출력층에만 사용, 은닉층에는 사용하지 않음[1][5].</li>
</ul>
<blockquote>
<p>"ReLU is the top choice as it is simpler, faster, much lower run time, better convergence performance and does not suffer from vanishing gradient issues"[5].</p>
<p>"Sigmoid/Logistic and Tanh functions should not be used in hidden layers as they make the model more susceptible to problems during training (due to vanishing gradients)"[1].</p>
</blockquote>
<hr />
<h4 id="_28">상황별 권장 함수<a class="headerlink" href="#_28" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>은닉층</strong>: ReLU 계열(Leaky ReLU, PReLU 등), 일부 RNN은 Tanh/Sigmoid</li>
<li><strong>출력층</strong>: 회귀(Linear), 이진분류(Sigmoid), 다중분류(Softmax), 멀티라벨(Sigmoid)</li>
</ul>
<hr />
<p>출처
[1] Activation Functions in Neural Networks [12 Types &amp; Use Cases] <a href="https://www.v7labs.com/blog/neural-networks-activation-functions">https://www.v7labs.com/blog/neural-networks-activation-functions</a>
[2] [PyTorch] PyTorch가 제공하는 Activation function(활성화함수) 정리 <a href="https://sanghyu.tistory.com/182">https://sanghyu.tistory.com/182</a>
[3] [PDF] Review and Comparison of Commonly Used Activation Functions for ... <a href="https://arxiv.org/pdf/2010.09458.pdf">https://arxiv.org/pdf/2010.09458.pdf</a>
[4] Why is Relu considered superior compared to Tanh or sigmoid? <a href="https://www.reddit.com/r/learnmachinelearning/comments/ua6n6s/why_is_relu_considered_superior_compared_to_tanh/">https://www.reddit.com/r/learnmachinelearning/comments/ua6n6s/why_is_relu_considered_superior_compared_to_tanh/</a>
[5] What is an activation function? What are the different types of ... <a href="https://aiml.com/what-is-an-activation-function-what-are-the-different-types-of-activation-functions-discuss-their-pros-and-cons/">https://aiml.com/what-is-an-activation-function-what-are-the-different-types-of-activation-functions-discuss-their-pros-and-cons/</a>
[6] 활성화 함수(activation function) 종류와 정리 - PGNV 계단 - 티스토리 <a href="https://pgnv.tistory.com/17">https://pgnv.tistory.com/17</a>
[7] Activation functions in neural networks [Updated 2024] <a href="https://www.superannotate.com/blog/activation-functions-in-neural-networks">https://www.superannotate.com/blog/activation-functions-in-neural-networks</a>
[8] Top 10 Activation Function's Advantages &amp; Disadvantages - LinkedIn <a href="https://www.linkedin.com/pulse/top-10-activation-functions-advantages-disadvantages-dash">https://www.linkedin.com/pulse/top-10-activation-functions-advantages-disadvantages-dash</a>
[9] Activation Functions: Advantages &amp; Disadvantages - YouTube <a href="https://www.youtube.com/watch?v=uomZz2pckOA">https://www.youtube.com/watch?v=uomZz2pckOA</a>
[10] 7 Common Nonlinear Activation Functions (Advantage and ... - Kaggle <a href="https://www.kaggle.com/getting-started/157710">https://www.kaggle.com/getting-started/157710</a></p>
<h3 id="loss-function">손실함수 (Loss Function)<a class="headerlink" href="#loss-function" title="Permanent link">&para;</a></h3>
<p>손실함수는 머신러닝 및 딥러닝 모델 학습 과정에서, 모델이 예측한 값과 실제 정답 사이의 차이를 수치적으로 평가하는 함수입니다. 이 값이 작다는 것은 모델의 예측이 실제 값에 가깝다는 것을 의미하며, 이 함수를 최소화하는 방향으로 모델을 학습합니다. 손실함수는 최적화 과정에서 경사 하강법과 같은 알고리즘이 가중치 조정을 수행할 때 매우 중요한 역할을 합니다.</p>
<h4 id="_29">대표적인 손실함수 정리<a class="headerlink" href="#_29" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>손실함수명</th>
<th>적용 문제 유형</th>
<th>정의 및 설명</th>
<th>특징</th>
</tr>
</thead>
<tbody>
<tr>
<td>Mean Squared Error (MSE)</td>
<td>회귀</td>
<td>예측값과 실제값 차이의 제곱을 평균한 값</td>
<td>큰 오차에 더 큰 페널티를 부여, 이상치에 민감</td>
</tr>
<tr>
<td>Mean Absolute Error (MAE)</td>
<td>회귀</td>
<td>예측값과 실제값 절대 차이의 평균</td>
<td>이상치에 덜 민감, 더 안정적인 성능 제공</td>
</tr>
<tr>
<td>Binary Cross Entropy</td>
<td>이진 분류</td>
<td>예측 확률과 실제 이진 클래스 간의 차이를 측정 (로그 손실)</td>
<td>확률 기반 손실, 이진 분류에 적합</td>
</tr>
<tr>
<td>Categorical Cross Entropy</td>
<td>다중 클래스 분류</td>
<td>다중 클래스에 대해 예측 확률 분포와 실제 분포 간의 차이를 측정</td>
<td>softmax 출력 및 one-hot 라벨 대응에 적합</td>
</tr>
<tr>
<td>Huber Loss</td>
<td>회귀</td>
<td>MSE와 MAE의 장점을 결합, 작은 오차는 제곱, 큰 오차는 절대값으로 처리</td>
<td>이상치에 강인하며 안정적인 학습 가능</td>
</tr>
<tr>
<td>Hinge Loss</td>
<td>분류 (SVM 등)</td>
<td>올바른 클래스 점수와 최대 마진을 고려하는 손실 함수</td>
<td>마진 기반 분류에 적합, 서포트 벡터 머신에서 주로 사용</td>
</tr>
</tbody>
</table>
<p>손실 함수는 문제 유형과 데이터 특성에 따라 적합한 함수를 선택하는 것이 매우 중요하며, 이를 통해 모델의 학습 방향성과 최종 성능이 크게 좌우됩니다.</p>
<p>출처
[1] 딥러닝 개발자라면 꼭 알아야 할 손실 함수 의 개념과 종류 <a href="https://modulabs.co.kr/blog/machine_learning_loss_function">https://modulabs.co.kr/blog/machine_learning_loss_function</a>
[2] 손실함수 (Loss Function) | 블로그 - 모두의연구소 <a href="https://modulabs.co.kr/blog/loss-function-machinelearning">https://modulabs.co.kr/blog/loss-function-machinelearning</a>
[3] 손실 함수란 무엇인가요? - IBM <a href="https://www.ibm.com/kr-ko/think/topics/loss-function">https://www.ibm.com/kr-ko/think/topics/loss-function</a>
[4] [Machine Learning] 손실 함수 (loss function) <a href="https://insighted-h.tistory.com/7">https://insighted-h.tistory.com/7</a>
[5] 손실함수의 이해와 종류/파이썬으로 구현까지 <a href="https://my-inote.tistory.com/164">https://my-inote.tistory.com/164</a>
[6] loss function 종류 - velog <a href="https://velog.io/@jh_ds/loss-function-%EC%A2%85%EB%A5%98">https://velog.io/@jh_ds/loss-function-%EC%A2%85%EB%A5%98</a>
[7] 손실 함수(Loss Function) 개념 및 종류 - learningflix - 티스토리 <a href="https://learningflix.tistory.com/128">https://learningflix.tistory.com/128</a>
[8] 10. 손실 함수(Loss Function) - 소설처럼 읽는 딥러닝 - 위키독스 <a href="https://wikidocs.net/277027">https://wikidocs.net/277027</a>
[9] [Theory] 손실함수(Loss function)의 통계적 분석 <a href="https://hyewonleess.github.io/theory/loss-function/">https://hyewonleess.github.io/theory/loss-function/</a>
[10] [딥러닝] 손실함수 (loss function) 종류 및 간단 정리 (feat. keras ... <a href="https://didu-story.tistory.com/27">https://didu-story.tistory.com/27</a></p>
<h3 id="backpropagation">역전파 (Backpropagation)<a class="headerlink" href="#backpropagation" title="Permanent link">&para;</a></h3>
<ul>
<li>역전파는 모델의 예측값과 실제값 사이의 오차(loss)를 기반으로 가중치를 업데이트하는 과정입니다.</li>
<li>출력층에서부터 입력층 방향으로 오차를 거꾸로 전파하며, 각 가중치가 손실 함수에 어느 정도 기여하는지 미분(기울기)를 계산합니다.</li>
<li>이때 연쇄법칙(chain rule)을 활용해 미분값을 효율적으로 구하며, 이를 통해 가중치를 학습률(learning rate)에 따라 조정합니다.</li>
<li>역전파는 신경망을 학습시키는 핵심 알고리즘으로, 손실을 최소화하도록 모델을 최적화합니다.</li>
</ul>
<p>요약하면, 순전파는 데이터가 입력에서 출력으로 전달되어 예측을 계산하는 과정이고, 역전파는 예측 오차를 거꾸로 전파하여 가중치를 조정하는 학습 과정입니다. 두 과정이 반복되며 신경망은 점점 더 정확한 예측을 할 수 있도록 학습됩니다.</p>
<p>출처
[1] 순전파 &amp; 역전파 - velog <a href="https://velog.io/@tnsida315/%EC%88%9C%EC%A0%84%ED%8C%8C-%EC%97%AD%EC%A0%84%ED%8C%8C">https://velog.io/@tnsida315/%EC%88%9C%EC%A0%84%ED%8C%8C-%EC%97%AD%EC%A0%84%ED%8C%8C</a>
[2] 07-05 역전파(BackPropagation) 이해하기 - 위키독스 <a href="https://wikidocs.net/37406">https://wikidocs.net/37406</a>
[3] [딥러닝개론] 순전파와 역전파 - Jini Dev <a href="https://yujindevv.tistory.com/7">https://yujindevv.tistory.com/7</a>
[4] 신경망 (3) - 역전파 알고리즘(BackPropagation algorithm) <a href="https://yhyun225.tistory.com/22">https://yhyun225.tistory.com/22</a>
[5] 역전파란 무엇인가요? - IBM <a href="https://www.ibm.com/kr-ko/think/topics/backpropagation">https://www.ibm.com/kr-ko/think/topics/backpropagation</a>
[6] 딥러닝에 대한 이해와 순전파, 역전파 직접 구현 <a href="https://coco0414.tistory.com/44">https://coco0414.tistory.com/44</a></p>
<h3 id="optimization">최적화(Optimization)<a class="headerlink" href="#optimization" title="Permanent link">&para;</a></h3>
<h4 id="sgd-adam-rmsprop">SGD, Adam, RMSprop<a class="headerlink" href="#sgd-adam-rmsprop" title="Permanent link">&para;</a></h4>
<p><img alt="" src="https://statusneo.com/wp-content/uploads/2023/09/Credit-Analytics-Vidya.jpg" /></p>
<h4 id="batch-mini-batch-stochastic-gradient-descent">Batch, Mini-Batch, Stochastic Gradient Descent 비교표<a class="headerlink" href="#batch-mini-batch-stochastic-gradient-descent" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>구분</th>
<th>Batch Gradient Descent (배치)</th>
<th>Mini-Batch Gradient Descent (미니배치)</th>
<th>Stochastic Gradient Descent (확률적/SGD)</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Batch Size</strong></td>
<td>전체 데이터셋</td>
<td>1 &lt; 미니배치 크기 &lt; 전체 데이터셋</td>
<td>1</td>
</tr>
<tr>
<td><strong>업데이트 빈도</strong></td>
<td>에포크마다 1회 (전체 데이터로 1회)</td>
<td>미니배치마다 1회 (여러 번)</td>
<td>샘플마다 1회 (가장 빈번)</td>
</tr>
<tr>
<td><strong>속도</strong></td>
<td>느림</td>
<td>빠름 (GPU 등 벡터화 활용 가능)</td>
<td>빠름 (업데이트는 빠르나, 전체적으로 느릴 수 있음)</td>
</tr>
<tr>
<td><strong>노이즈/진동</strong></td>
<td>거의 없음 (매끄러운 경로)</td>
<td>중간 (적당한 진동, 노이즈는 일부 있음)</td>
<td>큼 (진동 심함, 경로가 불규칙)</td>
</tr>
<tr>
<td><strong>메모리 사용량</strong></td>
<td>큼</td>
<td>중간</td>
<td>적음</td>
</tr>
<tr>
<td><strong>장점</strong></td>
<td>정확한 gradient, 수렴 경로 안정적</td>
<td>속도와 안정성의 균형, 하드웨어 최적화, 실무에서 주로 사용</td>
<td>빠른 업데이트, local minima 탈출 가능성</td>
</tr>
<tr>
<td><strong>단점</strong></td>
<td>느린 학습, 대용량 데이터에 부적합</td>
<td>하이퍼파라미터(배치 크기) 조정 필요, local minima에 갇힐 수 있음</td>
<td>노이즈 큼, 수렴 경로 불안정, 벡터화 어려움</td>
</tr>
<tr>
<td><strong>사용 예시</strong></td>
<td>소규모 데이터, 이론적 분석</td>
<td>대부분의 딥러닝/머신러닝 실무</td>
<td>온라인 학습, 실시간 데이터 처리</td>
</tr>
</tbody>
</table>
<hr />
<h5 id="_30">추가 설명<a class="headerlink" href="#_30" title="Permanent link">&para;</a></h5>
<ul>
<li><strong>Batch Gradient Descent</strong>: 전체 데이터셋을 한 번에 사용해 gradient를 계산하고 파라미터를 업데이트합니다. 계산은 정확하지만, 데이터가 많아질수록 한 번의 업데이트에 시간이 오래 걸립니다[1][2][4][6].</li>
<li><strong>Stochastic Gradient Descent (SGD)</strong>: 매번 하나의 샘플만 사용해 파라미터를 업데이트합니다. 업데이트가 매우 빈번하고 빠르지만, 경로가 매우 불규칙하고 노이즈가 많아 최적점 근처에서 진동이 심할 수 있습니다[1][2][4][6].</li>
<li><strong>Mini-Batch Gradient Descent</strong>: 전체 데이터를 여러 개의 작은 배치로 나누어 각 배치마다 gradient를 계산해 파라미터를 업데이트합니다. 대부분의 딥러닝 프레임워크에서 기본적으로 사용하며, GPU 등 벡터 연산에 최적화되어 있고, 속도와 안정성의 균형이 좋아 실무에서 가장 많이 쓰입니다[1][2][3][4][6].</li>
</ul>
<hr />
<h5 id="_31">요약<a class="headerlink" href="#_31" title="Permanent link">&para;</a></h5>
<ul>
<li><strong>Batch</strong>: 전체 데이터로 한 번에 업데이트 → 느리지만 안정적</li>
<li><strong>SGD</strong>: 샘플 하나로 업데이트 → 빠르지만 불안정</li>
<li><strong>Mini-Batch</strong>: 일부 샘플(수십~수백 개)로 업데이트 → 빠르고 안정적, 실무 표준</li>
</ul>
<p>출처
[1] [Optimization Algorithms] Gradient Descent (1) Batch, Stochastic ... <a href="https://geniewishescometrue.tistory.com/entry/Gradient-Descent">https://geniewishescometrue.tistory.com/entry/Gradient-Descent</a>
[2] Batch vs Mini batch vs Stochastic - Woongjoon_AI - Choi Woongjoon <a href="https://woongjoonchoi.github.io/optimization/Batch-Size/">https://woongjoonchoi.github.io/optimization/Batch-Size/</a>
[3] Mini-batch Gradient Descent(미니배치 경사 하강법) - 정리 - 티스토리 <a href="https://better-tomorrow.tistory.com/entry/Mini-batch-Gradient-Descent%EB%AF%B8%EB%8B%88%EB%B0%B0%EC%B9%98-%EA%B2%BD%EC%82%AC-%ED%95%98%EA%B0%95%EB%B2%95">https://better-tomorrow.tistory.com/entry/Mini-batch-Gradient-Descent%EB%AF%B8%EB%8B%88%EB%B0%B0%EC%B9%98-%EA%B2%BD%EC%82%AC-%ED%95%98%EA%B0%95%EB%B2%95</a>
[4] Gradient Descent(경사하강법) 와 SGD( Stochastic ... - 매일매일 딥러닝 <a href="https://everyday-deeplearning.tistory.com/entry/SGD-Stochastic-Gradient-Descent-%ED%99%95%EB%A5%A0%EC%A0%81-%EA%B2%BD%EC%82%AC%ED%95%98%EA%B0%95%EB%B2%95">https://everyday-deeplearning.tistory.com/entry/SGD-Stochastic-Gradient-Descent-%ED%99%95%EB%A5%A0%EC%A0%81-%EA%B2%BD%EC%82%AC%ED%95%98%EA%B0%95%EB%B2%95</a>
[5] Batch gradient descent versus stochastic gradient descent <a href="https://stats.stackexchange.com/questions/49528/batch-gradient-descent-versus-stochastic-gradient-descent">https://stats.stackexchange.com/questions/49528/batch-gradient-descent-versus-stochastic-gradient-descent</a>
[6] Differences Between Gradient, Stochastic and Mini Batch Gradient ... <a href="https://www.baeldung.com/cs/gradient-stochastic-and-mini-batch">https://www.baeldung.com/cs/gradient-stochastic-and-mini-batch</a>
[7] [ML]Gradient Descent 의 세 종류(Batch, Stochastic, Mini-Batch) - velog <a href="https://velog.io/@crosstar1228/MLGradient-Descent-%EC%9D%98-%EC%84%B8-%EC%A2%85%EB%A5%98Batch-Stochastic-Mini-Batch">https://velog.io/@crosstar1228/MLGradient-Descent-%EC%9D%98-%EC%84%B8-%EC%A2%85%EB%A5%98Batch-Stochastic-Mini-Batch</a>
[8] 경사하강법 Batch/Stochastic/Mini-Batch Gradient Descent (BGD ... <a href="https://ju-blog.tistory.com/60">https://ju-blog.tistory.com/60</a>
[9] 딥러닝 용어정리, MGD(Mini-batch gradient descent), SGD ... - All about <a href="https://light-tree.tistory.com/133">https://light-tree.tistory.com/133</a>
[10] BGD: Batch Gradient Descent (배치 경사 하강법) - 위키독스 <a href="https://wikidocs.net/200934">https://wikidocs.net/200934</a></p>
<h3 id="learning-rate">학습률(Learning rate)<a class="headerlink" href="#learning-rate" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>학습률(learning rate)은 경사하강법에서 손실 함수의 기울기(gradient)를 따라 파라미터를 얼마나 이동시킬지를 결정하는 값</strong></li>
<li>너무 작으면 학습 속도가 느려지고, 너무 크면 발산하거나 최적값에 도달하지 못함</li>
</ul>
<h4 id="_32"><strong>수식 이해</strong><a class="headerlink" href="#_32" title="Permanent link">&para;</a></h4>
<p>딥러닝의 가중치 업데이트 수식:</p>
<pre><code>w = w - η * ∇L(w)
</code></pre>
<ul>
<li>w: 모델의 가중치</li>
<li>η (eta): <strong>learning rate</strong></li>
<li>∇L(w): 손실 함수 L에 대한 가중치의 기울기</li>
</ul>
<h4 id="learning-rate_1">learning Rate 의 영향<a class="headerlink" href="#learning-rate_1" title="Permanent link">&para;</a></h4>
<ul>
<li>너무 작을 때: 학습 속도가 매우 느리며, local minima에 갇힐 수 있음</li>
<li>너무 클 때: 손실 값이 발산하거나 최적점을 지나쳐 계속 진동하여 수렴 실패</li>
</ul>
<h4 id="_33">관련 기법<a class="headerlink" href="#_33" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>고정 학습률 (Fixed LR)</strong></li>
<li>일정한 값을 처음부터 끝까지 사용하는 방식</li>
<li><strong>Learning Rate Decay (감쇠)</strong></li>
<li>학습이 진행됨에 따라 학습률을 <strong>점점 줄이는</strong> 전략</li>
<li>예: Step decay, Exponential decay, Cosine annealing</li>
<li><strong>Warm-up</strong></li>
<li>초기 학습률을 매우 작게 시작하고, 일정 epoch 동안 점차 증가</li>
<li>안정적인 초기 학습에 도움</li>
<li><strong>Adaptive Methods</strong></li>
<li>학습률을 자동으로 조정하는 옵티마이저</li>
<li>예: Adam, RMSprop, Adagrad 등</li>
<li><strong>Cyclical Learning Rate</strong></li>
<li>일정 주기로 학습률을 증가·감소시키며 지역 최적점 탈출 유도</li>
</ul>
<p><a href="https://gaussian37.github.io/dl-pytorch-lr_scheduler/">Learning rate scheduler 정리</a></p>
<h3 id="normalization">정규화(Normalization)<a class="headerlink" href="#normalization" title="Permanent link">&para;</a></h3>
<h4 id="normalization_1">Normalization이란?<a class="headerlink" href="#normalization_1" title="Permanent link">&para;</a></h4>
<p>Normalization(정규화)은 <strong>뉴럴 네트워크의 학습과정에서 데이터의 분포를 일정하게 맞추는 과정</strong>입니다. 입력 값들의 스케일을 비슷하게 맞추어서 학습의 안정성, 속도를 높이고, 기울기 소실/폭주 문제를 완화하며 일반화 성능도 향상됩니다[1][2].</p>
<h4 id="_34">일반적인 목적과 효과<a class="headerlink" href="#_34" title="Permanent link">&para;</a></h4>
<ul>
<li>학습 속도 향상 및 안정화</li>
<li>내부 공변량 변화(Internal Covariate Shift) 완화</li>
<li>큰/작은 값에 의한 학습률 저하 방지</li>
<li>각 feature(특징)의 중요도 균형화</li>
</ul>
<h4 id="normalization_2">대표적인 Normalization 종류<a class="headerlink" href="#normalization_2" title="Permanent link">&para;</a></h4>
<p>아래 표는 신경망에서 자주 쓰이는 LayerNorm, BatchNorm 외 주요 Normalization 기법을 정리한 것입니다.</p>
<p><img width="960" height="252" alt="1*gat8a-TUnopoYN_veGEi0w" src="https://github.com/user-attachments/assets/1a653d45-9886-4868-9ebe-eeea6a45a55f" /></p>
<table>
<thead>
<tr>
<th>구분</th>
<th>정규화 방식</th>
<th>정규화 축</th>
<th>Batch 크기 의존성</th>
<th>적용 위치</th>
<th>대표 활용 분야</th>
<th>특징 및 장단점</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>BatchNorm</strong></td>
<td>(x-평균)/표준편차</td>
<td>Batch 차원(전체 샘플별 feature별)</td>
<td>O</td>
<td>Layer와 Activation 사이</td>
<td>CNN, 일반 MLP</td>
<td>빠른 수렴, 대형 Batch 필요, 추론 시 moving avg 사용[3][4][5]</td>
</tr>
<tr>
<td><strong>LayerNorm</strong></td>
<td>(x-평균)/표준편차</td>
<td>Feature 차원(샘플별 전체 feature)</td>
<td>X</td>
<td>주로 입력/출력(Transformer 등)</td>
<td>Transformer, RNN</td>
<td>Batch 크기 무관, 소규모 Batch, NLP/시계열, 실시간 추론 적합[6][7][4][8]</td>
</tr>
<tr>
<td><strong>InstanceNorm</strong></td>
<td>(x-평균)/표준편차</td>
<td>각 샘플, 각 채널 별</td>
<td>X</td>
<td>Conv layer 출력 등</td>
<td>스타일 트랜스퍼, 이미지</td>
<td>이미지별 채널 정규화, 스타일 변화에 유리</td>
</tr>
<tr>
<td><strong>GroupNorm</strong></td>
<td>(x-평균)/표준편차</td>
<td>채널을 여러 그룹으로 나눠 정규화</td>
<td>X</td>
<td>Conv layer 출력 등</td>
<td>컴퓨터 비전(CNN)</td>
<td>소규모-대규모 Batch 모두 유연, Group 수 조절로 Batch/LayerNorm 절충[4]</td>
</tr>
<tr>
<td><strong>RMSNorm</strong></td>
<td>RMS(제곱평균근)로만 정규화</td>
<td>Feature 차원(평균 미사용)</td>
<td>X</td>
<td>Transformer 등</td>
<td>대규모 LLM(언어모델)</td>
<td>계산 비용 낮고, Mean subtraction 없음(최근 논문에서 성능비교)[9]</td>
</tr>
</tbody>
</table>
<h4 id="batch-normalization-batchnorm">Batch Normalization (BatchNorm) 요약<a class="headerlink" href="#batch-normalization-batchnorm" title="Permanent link">&para;</a></h4>
<ul>
<li>미니배치 단위로 각 feature(채널)의 평균과 분산을 계산, 정규화 후 학습 가능 파라미터로 scale/shift를 적용.</li>
<li>학습 시 batch 통계 사용, 추론 시엔 전체 데이터 통계 적용.</li>
<li><strong>장점</strong>: 빠른 수렴, 일반화 성능 향상, 깊은 네트워크에 효과적[3][4][10].</li>
<li><strong>단점</strong>: Batch size에 민감, 순환/트랜스포머 모델/실시간 처리엔 적합하지 않음.</li>
</ul>
<h4 id="layer-normalization-layernorm">Layer Normalization (LayerNorm) 요약<a class="headerlink" href="#layer-normalization-layernorm" title="Permanent link">&para;</a></h4>
<ul>
<li>각 샘플 별 feature들을 정규화(즉, 한 샘플 벡터 전체의 평균, 분산 활용).</li>
<li><strong>Batch size의 영향 없이</strong> 사용 가능해 NLP, RNN, Transformer 등에 적합.</li>
<li><strong>장점</strong>: 작은 batch, 시퀀스 모델 등에서 우수, 실시간에 적합[6][7][8].</li>
<li><strong>단점</strong>: CNN에서 BatchNorm만큼 효과적이지 않을 때도 있음.</li>
</ul>
<h4 id="normalization_3">참고: 그 외 Normalization<a class="headerlink" href="#normalization_3" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>InstanceNorm</strong>: 주로 이미지 생성에 사용, 스타일 변화에 강건.</li>
<li><strong>GroupNorm</strong>: Group 단위로 정규화하여 Batch, LayerNorm의 단점을 보완.</li>
<li><strong>RMSNorm</strong>: Mean을 빼지 않고 RMS(Root Mean Square)만 사용해 계산 효율적[9].</li>
</ul>
<h4 id="_35">요약<a class="headerlink" href="#_35" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>Normalization은 신경망 학습의 핵심 도구</strong>로, 데이터의 분포를 통제하여 학습을 빠르고 안정적으로 만든다.</li>
<li><strong>BatchNorm</strong>은 대형 Batch에서 효과적이며, <strong>LayerNorm</strong>은 batch size와 무관하여 Transformer/RNN 등에서 주로 쓰인다.</li>
<li>모델 구조 및 목적에 따라서 적합한 Normalization 기법을 선택해야 한다[2][4][7].</li>
</ul>
<p><strong>참고 문헌:</strong>
내용은 최신 논문, PyTorch 공식문서, 학습 사이트 등 다양한 신뢰성 있는 자료 기반으로 작성되었습니다[1][2][6][7][3][4][8][10].</p>
<p>출처
[1] Normalizing Inputs for an Artificial Neural Network <a href="https://www.baeldung.com/cs/normalizing-inputs-artificial-neural-network">https://www.baeldung.com/cs/normalizing-inputs-artificial-neural-network</a>
[2] Using Normalization Layers to Improve Deep Learning ... <a href="https://www.machinelearningmastery.com/using-normalization-layers-to-improve-deep-learning-models/">https://www.machinelearningmastery.com/using-normalization-layers-to-improve-deep-learning-models/</a>
[3] Batch Normalization (BatchNorm) Explained | Ultralytics <a href="https://www.ultralytics.com/glossary/batch-normalization">https://www.ultralytics.com/glossary/batch-normalization</a>
[4] Build Better Deep Learning Models with Batch and Layer ... - Pinecone <a href="https://www.pinecone.io/learn/batch-layer-normalization/">https://www.pinecone.io/learn/batch-layer-normalization/</a>
[5] 8.5. Batch Normalization - Dive into Deep Learning <a href="http://d2l.ai/chapter_convolutional-modern/batch-norm.html">http://d2l.ai/chapter_convolutional-modern/batch-norm.html</a>
[6] LayerNorm — PyTorch 2.7 documentation <a href="https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html">https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html</a>
[7] Batch vs Layer Normalization - Zilliz Learn <a href="https://zilliz.com/learn/layer-vs-batch-normalization-unlocking-efficiency-in-neural-networks">https://zilliz.com/learn/layer-vs-batch-normalization-unlocking-efficiency-in-neural-networks</a>
[8] What is Layer Normalization? - GeeksforGeeks <a href="https://www.geeksforgeeks.org/deep-learning/what-is-layer-normalization/">https://www.geeksforgeeks.org/deep-learning/what-is-layer-normalization/</a>
[9] Re-Introducing LayerNorm: Geometric Meaning, Irreversibility and a ... <a href="https://arxiv.org/html/2409.12951v1">https://arxiv.org/html/2409.12951v1</a>
[10] Batch normalization - Wikipedia <a href="https://en.wikipedia.org/wiki/Batch_normalization">https://en.wikipedia.org/wiki/Batch_normalization</a>
[11] Normalization (machine learning) <a href="https://en.wikipedia.org/wiki/Normalization_(machine_learning">https://en.wikipedia.org/wiki/Normalization_(machine_learning</a>)
[12] Normalization Techniques in Training DNNs <a href="https://arxiv.org/pdf/2009.12836.pdf">https://arxiv.org/pdf/2009.12836.pdf</a>
[13] Normalizing data for better Neural Network performance <a href="https://www.youtube.com/watch?v=jL4cs5EZuO4">https://www.youtube.com/watch?v=jL4cs5EZuO4</a>
[14] Normalization effects on deep neural networks <a href="https://www.aimsciences.org/article/doi/10.3934/fods.2023004">https://www.aimsciences.org/article/doi/10.3934/fods.2023004</a>
[15] Batch Normalization VS Layer Normalization - 주홍색 코딩 - 티스토리 <a href="https://kwonkai.tistory.com/144">https://kwonkai.tistory.com/144</a>
[16] Neural Network 적용 전에 Input data를 Normalize 해야 하는 이유 <a href="https://goodtogreate.tistory.com/entry/Neural-Network-%EC%A0%81%EC%9A%A9-%EC%A0%84%EC%97%90-Input-data%EB%A5%BC-Normalize-%ED%95%B4%EC%95%BC-%ED%95%98%EB%8A%94-%EC%9D%B4%EC%9C%A0">https://goodtogreate.tistory.com/entry/Neural-Network-%EC%A0%81%EC%9A%A9-%EC%A0%84%EC%97%90-Input-data%EB%A5%BC-Normalize-%ED%95%B4%EC%95%BC-%ED%95%98%EB%8A%94-%EC%9D%B4%EC%9C%A0</a>
[17] Batch Norm Explained Visually - How it works, and why neural ... <a href="https://towardsdatascience.com/batch-norm-explained-visually-how-it-works-and-why-neural-networks-need-it-b18919692739/">https://towardsdatascience.com/batch-norm-explained-visually-how-it-works-and-why-neural-networks-need-it-b18919692739/</a>
[18] Why do transformers use layer norm instead of batch norm? <a href="https://stats.stackexchange.com/questions/474440/why-do-transformers-use-layer-norm-instead-of-batch-norm">https://stats.stackexchange.com/questions/474440/why-do-transformers-use-layer-norm-instead-of-batch-norm</a>
[19] Should I normalize all data prior feeding the neural network ... <a href="https://stats.stackexchange.com/questions/458579/should-i-normalize-all-data-prior-feeding-the-neural-network-models">https://stats.stackexchange.com/questions/458579/should-i-normalize-all-data-prior-feeding-the-neural-network-models</a>
[20] LayerNorm 과 BatchNorm 의 차이 - Embedded World - 티스토리 <a href="https://docon.tistory.com/22">https://docon.tistory.com/22</a></p>
<h3 id="regularization">Regularization<a class="headerlink" href="#regularization" title="Permanent link">&para;</a></h3>
<p>머신러닝에서 <strong>Regularization</strong>는 모델이 학습 데이터에 과도하게 적합하여 새로운 데이터에 대해 성능이 떨어지는 과적합(overfitting)을 방지하기 위해 모델의 복잡도를 제한하거나 패널티를 주는 기법입니다. 대표적인 규제 방법에는 <strong>Dropout</strong>과 <strong>L1/L2 정규화</strong>가 있습니다.</p>
<hr />
<h4 id="dropout">Dropout<a class="headerlink" href="#dropout" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>개념</strong>: 학습 과정에서 신경망의 일부 뉴런을 임의로 선택하여 일시적으로 비활성화(즉, 출력을 0으로 만듦)하는 방법입니다.</li>
<li><strong>목적</strong>: 뉴런 간 공동 적응(co-adaptation)을 방지하고, 다양한 뉴런 조합에서 학습하게 함으로써 모델이 보다 일반화할 수 있도록 돕습니다.</li>
<li><strong>작동 방식</strong>: 학습 시 각 층의 뉴런을 확률적으로 드롭(꺼짐)하며, 테스트 시에는 모든 뉴런을 활성화하고 출력에 확률값을 반영해 보정합니다.</li>
<li><strong>효과</strong>: 과적합 억제와 모델 일반화 성능 개선에 매우 효과적이며, 복잡한 신경망에서 자주 사용됩니다.</li>
</ul>
<hr />
<h4 id="l1l2">L1/L2 정규화<a class="headerlink" href="#l1l2" title="Permanent link">&para;</a></h4>
<ul>
<li>
<p><strong>개념</strong>: 손실 함수(loss function)에 모델 파라미터(가중치)에 대한 페널티 항을 추가하는 기법으로, 가중치가 너무 커지지 않도록 조절하여 과적합을 방지합니다.</p>
</li>
<li>
<p><strong>L1 정규화 (Lasso)</strong></p>
</li>
<li>가중치의 절대값 합을 페널티로 추가합니다.</li>
<li>특징: 일부 가중치를 정확히 0으로 만들어 희소한(sparse) 모델을 생성해 중요 특성만 선택하는 효과가 있습니다.</li>
<li>장점: 특성 선택(feature selection)에 적합하며, 이상치(outlier)에 대해 더 강건함(robust).</li>
<li>
<p>단점: 수학적으로 0에서 미분이 불가능해 최적화에 주의가 필요하며, 규제 강도가 약할 수 있음.</p>
</li>
<li>
<p><strong>L2 정규화 (Ridge)</strong></p>
</li>
<li>가중치의 제곱합을 페널티로 추가합니다.</li>
<li>특징: 모든 가중치를 균등하게 작게 유지하려 하며, 오버핏팅을 방지합니다.</li>
<li>장점: 미분 가능하고 계산이 안정적이며, 대부분의 경우 L1보다 더 좋은 예측 성능을 냅니다.</li>
<li>단점: 가중치를 완전히 0으로 만들지 않으므로 희소성은 제공하지 않음.</li>
<li>L2 정규화는 weight decay(가중치 감소)와 같은 효과를 가지며, 큰 가중치에 더 큰 패널티를 줍니다.</li>
</ul>
<hr />
<p>이 두 Regularization 기법은 종종 혼합하여 사용되기도 하며(예: Elastic Net), 모델의 성능과 일반화 능력을 크게 향상시키는 데 중요한 역할을 합니다. Dropout은 주로 딥러닝 모델에서 비선형 신경망의 과적합 방지에 활용되고, L1/L2 정규화는 전통적인 머신러닝과 딥러닝 모두 널리 적용됩니다.</p>
<p>출처
[1] [최적화] 가중치 규제 L1, L2 Regularization의 의미, 차이점 (Lasso ... <a href="https://seongyun-dev.tistory.com/52">https://seongyun-dev.tistory.com/52</a>
[2] L1 Regularization &amp; L2 Regularization - Everything - 티스토리 <a href="https://hyebiness.tistory.com/11">https://hyebiness.tistory.com/11</a>
[3] L1 정규화, L2 정규화 <a href="https://esj205.oopy.io/4b321662-5d02-4559-8677-7e974cf080a8">https://esj205.oopy.io/4b321662-5d02-4559-8677-7e974cf080a8</a>
[4] L1, L2 Norm, Loss, Regularization? - 생각정리 - 티스토리 <a href="https://junklee.tistory.com/29">https://junklee.tistory.com/29</a>
[5] L1 &amp; L2 loss/regularization - Seongkyun Han's blog <a href="https://seongkyun.github.io/study/2019/04/18/l1_l2/">https://seongkyun.github.io/study/2019/04/18/l1_l2/</a>
[6] L1 Loss &amp; L2 Loss - 효과는 굉장했다! - 티스토리 <a href="https://thflgg133.tistory.com/231">https://thflgg133.tistory.com/231</a>
[7] Regularization(정규화/규제화) 기법 - Ridge(L2 norm) / LASSO(L1 ... <a href="https://bigdaheta.tistory.com/104">https://bigdaheta.tistory.com/104</a>
[8] [기술면접] L1, L2 regularization, Ridge와 Lasso의 차이점 (201023) <a href="https://huidea.tistory.com/154">https://huidea.tistory.com/154</a>
[9] L1, L2 Regularization에 대하여 - Doby's Lab - 티스토리 <a href="https://draw-code-boy.tistory.com/502">https://draw-code-boy.tistory.com/502</a>
[10] [CS231n] 2강. L1 &amp; L2 distance - 룰루랄라 효니루 - 티스토리 <a href="https://bookandmed.tistory.com/27">https://bookandmed.tistory.com/27</a></p>
<h3 id="gradient-clipping"><a href="https://sanghyu.tistory.com/87">Gradient clipping</a><a class="headerlink" href="#gradient-clipping" title="Permanent link">&para;</a></h3>
<ul>
<li>Gradient Clipping은 기울기 폭주(gradient explosion) 현상을 방지하기 위한 기법</li>
<li>딥러닝 모델, 특히 RNN 또는 심층 네트워크 학습 시, 역전파 중 기울기가 지나치게 커지면 가중치가 발산하거나 NaN이 되는 문제가 발생</li>
<li>Gradient Clipping은 특정 임계값을 초과하는 기울기의 크기를 <strong>잘라내거나 재조정(clip)</strong>하여 안정적인 학습 도움</li>
<li>대표적인 방식: norm 기준으로 클리핑 (torch.nn.utils.clip_grad_norm_ 등)</li>
</ul>
<h2 id="4">4. 학습 모니터링 및 개선<a class="headerlink" href="#4" title="Permanent link">&para;</a></h2>
<h3 id="_36">손실 및 정확도 곡선 모니터링<a class="headerlink" href="#_36" title="Permanent link">&para;</a></h3>
<ul>
<li>모델 학습 동안 손실 함수 값(loss)과 정확도(accuracy)를 에포크(epoch)별로 추적, 시각화하는 기법입니다.</li>
<li>손실 곡선이 지속적으로 감소하고 정확도 곡선이 증가하면 학습이 잘 진행되고 있는 신호입니다.</li>
<li>곡선의 변화를 통해 학습 속도, 과적합(overfitting) 여부, 학습 정체 상태 등을 파악할 수 있습니다.</li>
<li>대표적인 도구로 TensorBoard, Weights &amp; Biases 등이 있으며, 실시간 모니터링 및 다양한 메트릭 시각화가 가능합니다.</li>
</ul>
<h3 id="early-stopping">조기 종료(Early Stopping)<a class="headerlink" href="#early-stopping" title="Permanent link">&para;</a></h3>
<ul>
<li>검증(validation) 데이터에서 성능 개선이 일정 에포크 이상 없을 경우 학습을 미리 종료하는 기법</li>
<li>과적합을 방지하며, 불필요한 학습 시간을 줄이고 자원 효율성을 개선합니다.</li>
<li>구현 방식은 주로 검증 손실(validation loss) 또는 검증 정확도(validation accuracy)를 기준으로 하며, 이 값이 개선되지 않으면 정지 신호를 보냅니다.</li>
</ul>
<h3 id="automl">자동화된 하이퍼파라미터 탐색 (AutoML)<a class="headerlink" href="#automl" title="Permanent link">&para;</a></h3>
<ul>
<li>모델 성능에 큰 영향을 미치는 하이퍼파라미터(학습률, 배치 크기, 네트워크 크기 등)를 자동으로 탐색, 최적화하는 기술</li>
<li>그리드 서치(Grid Search), 랜덤 서치(Random Search), 베이지안 최적화(Bayesian Optimization), 진화 알고리즘, 강화학습 등 다양한 방법이 활용됨</li>
<li>AutoML은 사람이 직접 조정하던 많은 반복적이고 시간 소모적인 작업을 자동화하여, 효율적이고 최적화된 모델을 빠르게 찾을 수 있도록 지원</li>
<li>대표적인 프레임워크로는 Google AutoML, Microsoft Azure AutoML, open-source인 Auto-sklearn, Optuna 등이 있음</li>
</ul>
<p>이처럼 학습 모니터링과 조기 종료, 그리고 AutoML은 모델 학습을 안정적으로 유지하고 최적 성능을 추구하기 위한 핵심 기술들입니다.</p>
<p>출처
[1] Azure Machine Learning 모니터링 <a href="https://learn.microsoft.com/ko-kr/azure/machine-learning/monitor-azure-machine-learning?view=azureml-api-2">https://learn.microsoft.com/ko-kr/azure/machine-learning/monitor-azure-machine-learning?view=azureml-api-2</a>
[2] 모니터링을 통한 머신러닝 모델 정확도 유지 <a href="https://dev.to/yoon/moniteoringeul-tonghan-meosinreoning-model-jeonghwagdo-yuji-1k86">https://dev.to/yoon/moniteoringeul-tonghan-meosinreoning-model-jeonghwagdo-yuji-1k86</a>
[3] 프로덕션 ML 시스템: 파이프라인 모니터링 | Machine Learning <a href="https://developers.google.com/machine-learning/crash-course/production-ml-systems/monitoring?hl=ko">https://developers.google.com/machine-learning/crash-course/production-ml-systems/monitoring?hl=ko</a>
[4] 내 딥러닝 모델(학습)이 지나치게 느릴 때 점검해야할 사항들 <a href="https://heesunpark26.tistory.com/15">https://heesunpark26.tistory.com/15</a>
[5] 딥러닝 파이토치 교과서: 2.2.7 훈련 과정 모니터링 <a href="https://thebook.io/080289/0053/">https://thebook.io/080289/0053/</a>
[6] 배포 후 컴퓨터 비전 모델 유지 관리 <a href="https://docs.ultralytics.com/ko/guides/model-monitoring-and-maintenance/">https://docs.ultralytics.com/ko/guides/model-monitoring-and-maintenance/</a>
[7] 봇 감지를 위한 머신 러닝 모델 모니터링 <a href="https://blog.cloudflare.com/ko-kr/monitoring-machine-learning-models-for-bot-detection/">https://blog.cloudflare.com/ko-kr/monitoring-machine-learning-models-for-bot-detection/</a>
[8] 모델 성능 모니터링을 통해 풀스택 옵저버빌리티를 머신 러닝 ... <a href="https://newrelic.com/kr/blog/nerdlog/ml-model-performance-monitoring">https://newrelic.com/kr/blog/nerdlog/ml-model-performance-monitoring</a>
[9] 머신러닝 학습 모니터링 데이터 버저닝을 위한 clearml <a href="https://www.youtube.com/watch?v=pIXxdFBdSEM">https://www.youtube.com/watch?v=pIXxdFBdSEM</a></p>
<h3 id="pflops-days">PFLOPS-days 절감 전략 선택을 위한 기본 개념 정리<a class="headerlink" href="#pflops-days" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>PFLOPS-days</strong>는 대규모 AI/머신러닝 트레이닝에서 사용하는 연산량 단위로, 1초에 1펫타플롭스(= <span class="arithmatex">\(<span class="arithmatex">\(10^{15}\)</span>\)</span> FLOPS)의 계산 속도를 하루(86,400초) 동안 수행했을 때의 총 연산량을 의미합니다.</li>
<li>예) “3,000 PFLOPS-days”는 1 PFLOPS의 컴퓨팅 자원으로 3,000일, 혹은 3,000 PFLOPS의 클러스터에서 1일간 작업한 총 연산량입니다[1][2][3].</li>
</ul>
<h4 id="pflops-days_1">총학습 비용(PFLOPS-days)에 영향을 주는 요소<a class="headerlink" href="#pflops-days_1" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>요소</th>
<th>설명</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>모델 크기</strong></td>
<td>파라미터·레이어 수가 클수록 연산량 폭증</td>
</tr>
<tr>
<td><strong>데이터셋 규모</strong></td>
<td>학습에 투입하는 데이터 양(토큰 수, 샘플 수)이 많을수록 연산 필요량 증가</td>
</tr>
<tr>
<td><strong>학습 반복(epoch)</strong></td>
<td>반복 학습 횟수가 늘수록 전연산량(PFLOPS-days) 증가</td>
</tr>
<tr>
<td><strong>하드웨어 성능 및 효율</strong></td>
<td>실제 GPU/TPU의 성능, 활용률(이론치 대비 실제 연산 효율; 통신, 자원 미활용 등으로 효율이 감소)</td>
</tr>
<tr>
<td><strong>알고리즘과 최적화</strong></td>
<td>연산 최소화 및 메모리 최적화가 이뤄질수록 총 연산 코스트 절감</td>
</tr>
</tbody>
</table>
<h4 id="pflops-days_2">대표적 PFLOPS-days 절감 전략<a class="headerlink" href="#pflops-days_2" title="Permanent link">&para;</a></h4>
<h5 id="1_2">1. 하드웨어 및 인프라 효율화<a class="headerlink" href="#1_2" title="Permanent link">&para;</a></h5>
<ul>
<li><strong>최적화된 인스턴스 타입/구성</strong>: 자체 하드웨어 외, GPU/TPU 선택, 클라우드의 최적 비용-성능 인스턴스 활용.</li>
<li><strong>병렬화·분산 학습</strong>: 여러 장비를 효율적으로 동원하고, 네트워크/데이터 병목 최소화[4][5][6].</li>
<li><strong>클러스터 자동 확장/축소</strong>: 필요 작업만 자원을 할당(오토스케일링); 유휴 리소스 감소.</li>
<li><strong>스팟/저가형 인스턴스 활용</strong>: 클라우드에서 스팟(Preemptible/Spot) 인스턴스를 사용하여 단가를 크게 낮춤[7].</li>
</ul>
<h5 id="2_2">2. 모델 구조 및 학습 알고리즘 개선<a class="headerlink" href="#2_2" title="Permanent link">&para;</a></h5>
<ul>
<li><strong>경량화 구조 설계</strong>: 모델 파라미터 수를 줄이는 경량 모델, 효율적 트랜스포머, Separable Convolution 등 연산량이 적은 구조 도입[8].</li>
<li><strong>모델 프루닝, 양자화</strong>: 불필요한 파라미터 제거, 저정밀 연산(half-precision/8bit 연산 등) 도입.</li>
<li><strong>Transfer/Adapter Learning</strong>: 사전학습(pre-trained) 모델을 전이학습 및 어댑터 등으로 활용, “from scratch” 전체 학습 비용 감소[5].</li>
</ul>
<h5 id="3_1">3. 학습 프로세스 최적화<a class="headerlink" href="#3_1" title="Permanent link">&para;</a></h5>
<ul>
<li><strong>데이터 최적화</strong>: 품질 높은 데이터 선별, 불필요 데이터 제거, 데이터 중복 배제.</li>
<li><strong>효율적 Hyperparameter 탐색</strong>: 서치 공간 축소, 초매개변수 자동 튜닝(HPO) 등으로 불필요한 반복 시도 절감[7].</li>
<li><strong>조기 종료(Early Stopping)</strong>: 성능 개선 효과가 없으면 학습 종료하여 불필요한 연산 방지.</li>
<li><strong>체계적 자원 할당 및 모니터링</strong>: 자원 낭비 방지, 실패 Job의 자동 종료 설정[6][5][7].</li>
</ul>
<h5 id="4-managedautoml">4. Managed/AutoML 서비스 활용<a class="headerlink" href="#4-managedautoml" title="Permanent link">&para;</a></h5>
<ul>
<li><strong>클라우드 관리형 학습 서비스</strong>: 인프라 자동 관리(AutoML, Vertex AI, Azure ML 등)로 오버프로비저닝·비효율 해소[5][6].</li>
<li><strong>리소스 예약·할당 자동화</strong>: 사용/비사용 시점에 자원 자동 할당-해제[6].</li>
</ul>
<h4 id="_37">주요 개념 요약<a class="headerlink" href="#_37" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>PFLOPS-days</strong>: 대규모 AI 학습의 자원소비량/비용 척도 단위(연산량=속도×시간).</li>
<li><strong>절감 전략</strong>: 하드웨어 효율화, 모델·알고리즘 개선, 프로세스/데이터 최적화, 관리형 서비스 활용 등.</li>
<li><strong>실전 적용</strong>: 필요한 비용 추산 → 병목/비효율 요인 파악 → 조합 전략 수립[4][5][7].</li>
</ul>
<p>이 개념과 전략을 기반으로 실제 AI/ML 프로젝트에서 PFLOPS-days 기반 총학습 비용을 합리적으로 절감할 수 있습니다.</p>
<p>출처
[1] What are petaFLOPS (PFLOPS)? - IONOS <a href="https://www.ionos.com/digitalguide/server/know-how/pflops/">https://www.ionos.com/digitalguide/server/know-how/pflops/</a>
[2] Computation used to train notable AI systems, by affiliation of ... <a href="https://ourworldindata.org/grapher/artificial-intelligence-training-computation-by-researcher-affiliation">https://ourworldindata.org/grapher/artificial-intelligence-training-computation-by-researcher-affiliation</a>
[3] AI and compute <a href="https://openai.com/index/ai-and-compute/">https://openai.com/index/ai-and-compute/</a>
[4] Transformer training costs | Continuum Labs <a href="https://training.continuumlabs.ai/infrastructure/data-and-memory/transformer-training-costs">https://training.continuumlabs.ai/infrastructure/data-and-memory/transformer-training-costs</a>
[5] AI and ML perspective: Cost optimization <a href="https://cloud.google.com/architecture/framework/perspectives/ai-ml/cost-optimization">https://cloud.google.com/architecture/framework/perspectives/ai-ml/cost-optimization</a>
[6] Manage and optimize Azure Machine Learning costs <a href="https://learn.microsoft.com/en-us/azure/machine-learning/how-to-manage-optimize-cost?view=azureml-api-2">https://learn.microsoft.com/en-us/azure/machine-learning/how-to-manage-optimize-cost?view=azureml-api-2</a>
[7] Cost optimization - Machine Learning Best Practices for ... <a href="https://docs.aws.amazon.com/whitepapers/latest/ml-best-practices-public-sector-organizations/cost-optimization.html">https://docs.aws.amazon.com/whitepapers/latest/ml-best-practices-public-sector-organizations/cost-optimization.html</a>
[8] How to Optimize a Deep Learning Model for faster Inference? <a href="https://www.thinkautonomous.ai/blog/deep-learning-optimization/">https://www.thinkautonomous.ai/blog/deep-learning-optimization/</a>
[9] [D] The cost of training GPT-3 : r/MachineLearning - Reddit <a href="https://www.reddit.com/r/MachineLearning/comments/hwfjej/d_the_cost_of_training_gpt3/">https://www.reddit.com/r/MachineLearning/comments/hwfjej/d_the_cost_of_training_gpt3/</a>
[10] Petaflops per second-days - DeepLearning.AI <a href="https://community.deeplearning.ai/t/petaflops-per-second-days/365974">https://community.deeplearning.ai/t/petaflops-per-second-days/365974</a>
[11] Estimating PaLM's training cost <a href="https://blog.heim.xyz/palm-training-cost/">https://blog.heim.xyz/palm-training-cost/</a>
[12] Disaggregating Power in Data Centers - Vicor Corporation <a href="https://www.vicorpower.com/resource-library/articles/high-performance-computing/disaggregating-power-in-data-centers">https://www.vicorpower.com/resource-library/articles/high-performance-computing/disaggregating-power-in-data-centers</a>
[13] What is the cost of training large language models? - CUDO Compute <a href="https://www.cudocompute.com/blog/what-is-the-cost-of-training-large-language-models">https://www.cudocompute.com/blog/what-is-the-cost-of-training-large-language-models</a>
[14] 8 ways to reduce cycle time with robots: Step-by-step guide <a href="https://standardbots.com/blog/reduce-cycle-time-guide">https://standardbots.com/blog/reduce-cycle-time-guide</a>
[15] FLOPS (Floating Point Operations Per Second) - Klu.ai <a href="https://klu.ai/glossary/flops">https://klu.ai/glossary/flops</a>
[16] Trends in the Dollar Training Cost of Machine Learning Systems <a href="https://epoch.ai/blog/trends-in-the-dollar-training-cost-of-machine-learning-systems">https://epoch.ai/blog/trends-in-the-dollar-training-cost-of-machine-learning-systems</a>
[17] 10 strategies for cycle time reduction <a href="https://about.gitlab.com/blog/strategies-to-reduce-cycle-times/">https://about.gitlab.com/blog/strategies-to-reduce-cycle-times/</a>
[18] DeepSeek V3 and the actual cost of training frontier AI models <a href="https://www.interconnects.ai/p/deepseek-v3-and-the-actual-cost-of">https://www.interconnects.ai/p/deepseek-v3-and-the-actual-cost-of</a>
[19] Saving cost on your machine learning training and inference ... <a href="https://pages.awscloud.com/EMEA-ML-Cost-Optimization.html">https://pages.awscloud.com/EMEA-ML-Cost-Optimization.html</a>
[20] Flops - Lark <a href="https://www.larksuite.com/en_us/topics/ai-glossary/flops">https://www.larksuite.com/en_us/topics/ai-glossary/flops</a></p>
<h2 id="5">5. 모델 평가 개요<a class="headerlink" href="#5" title="Permanent link">&para;</a></h2>
<ul>
<li>평가 지표의 필요성</li>
</ul>
<h3 id="classification">분류(Classification) 지표<a class="headerlink" href="#classification" title="Permanent link">&para;</a></h3>
<ul>
<li>정확도(Accuracy)</li>
<li>정밀도(Precision)</li>
<li>재현율(Recall)</li>
<li>F1-score</li>
<li>ROC-AUC, PR-AUC</li>
</ul>
<table>
<thead>
<tr>
<th>지표</th>
<th>정의 및 수식</th>
<th>의미 및 특징</th>
</tr>
</thead>
<tbody>
<tr>
<td>Accuracy (정확도)</td>
<td><span class="arithmatex">\(<span class="arithmatex">\(\frac{TP + TN}{TP + TN + FP + FN}\)</span>\)</span></td>
<td>전체 예측 중에서 맞게 예측한 비율. 클래스 불균형이 심할 때는 신뢰도가 떨어질 수 있음[1][2].</td>
</tr>
<tr>
<td>Precision (정밀도)</td>
<td><span class="arithmatex">\(<span class="arithmatex">\(\frac{TP}{TP + FP}\)</span>\)</span></td>
<td>양성으로 예측한 것 중 실제로 양성인 비율. False Positive(오탐)를 줄이는 데 중요[3][4].</td>
</tr>
<tr>
<td>Recall (재현율)</td>
<td><span class="arithmatex">\(<span class="arithmatex">\(\frac{TP}{TP + FN}\)</span>\)</span></td>
<td>실제 양성 중에서 모델이 맞게 예측한 비율. False Negative(누락)를 줄이는 데 중요[3][5][2].</td>
</tr>
<tr>
<td>F1-Score</td>
<td><span class="arithmatex">\(<span class="arithmatex">\(2 \times \frac{Precision \times Recall}{Precision + Recall}\)</span>\)</span></td>
<td>정밀도와 재현율의 조화평균. 두 지표의 균형이 필요할 때 사용. 1에 가까울수록 성능이 우수[6].</td>
</tr>
<tr>
<td>ROC AUC</td>
<td>ROC 곡선 아래 면적 (0~1)</td>
<td>임계값 변화에 따른 분류 성능을 종합적으로 평가. 1에 가까울수록 분류 성능 우수. 클래스 구분력 지표[7].</td>
</tr>
</tbody>
</table>
<p><strong>용어 설명</strong>
- TP: True Positive (실제 양성, 예측도 양성)
- TN: True Negative (실제 음성, 예측도 음성)
- FP: False Positive (실제 음성, 예측은 양성)
- FN: False Negative (실제 양성, 예측은 음성)</p>
<p><strong>요약</strong>
- <strong>Accuracy</strong>: 전체 예측 중 정답 비율. 클래스 불균형에 취약[1][2].
- <strong>Precision</strong>: 양성 예측 중 실제 양성 비율. 오탐이 중요한 분야에 적합[3][4].
- <strong>Recall</strong>: 실제 양성 중 맞춘 비율. 누락이 중요한 분야에 적합[3][5][2].
- <strong>F1-Score</strong>: Precision과 Recall의 조화평균(harmonic mean)으로, 두 지표를 균형 있게 반영하여 클래스 불균형 상황에서도 모델 성능을 종합적으로 평가할 수 있다.[6].
- <strong>ROC AUC</strong>: 임계값 변화 전반의 분류 성능. 1에 가까울수록 좋음[7].</p>
<p><strong>참고: roc-auc-precision-and-recall-visually-explained</strong>
<a href="https://paulvanderlaken.com/2019/08/16/roc-auc-precision-and-recall-visually-explained/">roc-auc-precision-and-recall-visually-explained</a></p>
<p><strong>참고: Confusion Matrix 계산기</strong>
<a href="https://www.omnicalculator.com/statistics/confusion-matrix">Confusion Matrix Calculator</a></p>
<p>출처
[1] Accuracy vs. precision vs. recall in machine learning - Evidently AI <a href="https://www.evidentlyai.com/classification-metrics/accuracy-precision-recall">https://www.evidentlyai.com/classification-metrics/accuracy-precision-recall</a>
[2] Accuracy vs. Precision vs. Recall in Machine Learning - Encord <a href="https://encord.com/blog/classification-metrics-accuracy-precision-recall/">https://encord.com/blog/classification-metrics-accuracy-precision-recall/</a>
[3] Classification: Accuracy, recall, precision, and related metrics <a href="https://developers.google.com/machine-learning/crash-course/classification/accuracy-precision-recall">https://developers.google.com/machine-learning/crash-course/classification/accuracy-precision-recall</a>
[4] Precision - Logicballs <a href="https://logicballs.com/glossary/precision/">https://logicballs.com/glossary/precision/</a>
[5] Recall - Logicballs <a href="https://logicballs.com/glossary/recall/">https://logicballs.com/glossary/recall/</a>
[6] An Introduction to the F1 Score in Machine Learning - Wandb <a href="https://wandb.ai/mostafaibrahim17/ml-articles/reports/An-Introduction-to-the-F1-Score-in-Machine-Learning--Vmlldzo2OTY0Mzg1">https://wandb.ai/mostafaibrahim17/ml-articles/reports/An-Introduction-to-the-F1-Score-in-Machine-Learning--Vmlldzo2OTY0Mzg1</a>
[7] What is Receiver Operating Characteristic Area Under Curve (ROC-AUC)? <a href="https://klu.ai/glossary/roc-auc">https://klu.ai/glossary/roc-auc</a>
[8] Accuracy - Cognilytica <a href="https://www.cognilytica.com/glossary/accuracy/">https://www.cognilytica.com/glossary/accuracy/</a>
[9] Accuracy (error rate) Definition - DeepAI <a href="https://deepai.org/machine-learning-glossary-and-terms/accuracy-error-rate">https://deepai.org/machine-learning-glossary-and-terms/accuracy-error-rate</a>
[10] What Is Accuracy In Machine Learning | Robots.net <a href="https://robots.net/fintech/what-is-accuracy-in-machine-learning/">https://robots.net/fintech/what-is-accuracy-in-machine-learning/</a></p>
<p><strong>평가지표 활용 예</strong>
| 상황                 | 중시 지표              | 이유                      |
| ------------------ | ------------------ | ----------------------- |
| 암 진단, 범죄 탐지, 공항 보안 | <strong>Recall</strong>         | 놓치면 큰 피해 (FN ↓)         |
| 스팸 분류, 광고 추천       | <strong>Precision</strong>      | 잘못 긍정하면 사용자가 불쾌 (FP ↓)  |
| 불균형 클래스 문제         | <strong>F1 Score</strong>       | Precision과 Recall 모두 중요 |
| 전체 모델 평가           | <strong>ROC Curve, AUC</strong> | 임계값 변화에 대한 전체 성향 시각화    |</p>
<h2 id="5_1">5. 모델 평가 개요<a class="headerlink" href="#5_1" title="Permanent link">&para;</a></h2>
<h3 id="regression">회귀(Regression) 지표<a class="headerlink" href="#regression" title="Permanent link">&para;</a></h3>
<p>회귀 문제에서는 실제 연속적인 값을 예측하므로, 예측값과 실제값 간의 오차를 기반으로 모델 성능을 평가합니다. 대표적인 지표들은 다음과 같습니다.</p>
<table>
<thead>
<tr>
<th>지표</th>
<th>정의 및 설명</th>
<th>특징</th>
</tr>
</thead>
<tbody>
<tr>
<td>MSE (Mean Squared Error, 평균 제곱 오차)</td>
<td>실제값과 예측값 차이를 제곱한 뒤 평균한 값<br> <span class="arithmatex">\(<span class="arithmatex">\(\text{MSE} = \frac{1}{n} \sum (y_i - \hat{y_i})^2\)</span>\)</span></td>
<td>큰 오차에 더 큰 패널티를 줌으로써, 큰 예측 오차가 중요시됨. 이상치에 민감함. 값이 작을수록 좋음.</td>
</tr>
<tr>
<td>RMSE (Root Mean Squared Error, 평균 제곱근 오차)</td>
<td>MSE의 제곱근을 취해 오차 단위를 실제 데이터 단위에 맞춤<br> <span class="arithmatex">\(<span class="arithmatex">\(\text{RMSE} = \sqrt{\text{MSE}}\)</span>\)</span></td>
<td>MSE보다 해석이 직관적이며, 큰 오차에 민감. 값이 작을수록 좋음.</td>
</tr>
<tr>
<td>MAE (Mean Absolute Error, 평균 절대 오차)</td>
<td>실제값과 예측값 차이의 절댓값 평균<br> $$\text{MAE} = \frac{1}{n} \sum</td>
<td>y_i - \hat{y_i}</td>
</tr>
<tr>
<td><span class="arithmatex">\(<span class="arithmatex">\(R^2\)</span>\)</span> (결정계수)</td>
<td>모델이 실제 데이터를 얼마나 잘 설명하는지 나타내는 지표로, 1에 가까울수록 좋은 모델<br> $$ R^2 = 1 - \frac{\sum (y_i - \hat{y_i})^2}{\sum (y_i - \bar{y})^2} $$</td>
<td>음수가 될 수도 있으나, 일반적으로 0~1 사이 값을 가지며 클수록 설명력이 높음.</td>
</tr>
</tbody>
</table>
<hr />
<h3 id="ranking">랭킹(Ranking) 지표<a class="headerlink" href="#ranking" title="Permanent link">&para;</a></h3>
<p>랭킹 문제는 검색, 추천시스템 등에서 결과 순서의 질을 평가할 때 사용되는 지표로, 대표적인 것들은 다음과 같습니다.</p>
<table>
<thead>
<tr>
<th>지표</th>
<th>정의 및 설명</th>
<th>특징</th>
</tr>
</thead>
<tbody>
<tr>
<td>NDCG (Normalized Discounted Cumulative Gain)</td>
<td>순위가 높을수록 더 큰 가중치를 주어 정답과 예측 랭킹의 품질을 평가하는 지표</td>
<td>랭킹의 위치와 정답의 중요도 모두 고려, 0~1 사이 값</td>
</tr>
<tr>
<td>MRR (Mean Reciprocal Rank)</td>
<td>정답이 처음 나타나는 위치의 역수를 평균한 값</td>
<td>첫 번째 정답에 초점, 값이 클수록 좋은 성능</td>
</tr>
</tbody>
</table>
<p>랭킹 지표는 결과 순서가 중요한 문제에서 단순 정확도보다 더 의미 있는 평가를 제공합니다.</p>
<hr />
<p>이처럼 회귀와 랭킹 문제는 목적에 맞는 적절한 지표로 모델 성능을 평가하고 해석해야 합니다.</p>
<p>출처
[1] 회귀 모델 성능 평가 지표(MAE, MSE, RMSE, MAPE 등) - Note <a href="https://white-joy.tistory.com/10">https://white-joy.tistory.com/10</a>
[2] 회귀모델 평가지표 - R2 score 결정계수, MAE, MSE, RMSE, MAPE ... <a href="https://sy-log.tistory.com/entry/%ED%9A%8C%EA%B7%80%EB%AA%A8%EB%8D%B8-%ED%8F%89%EA%B0%80%EC%A7%80%ED%91%9C-R2-score-%EA%B2%B0%EC%A0%95%EA%B3%84%EC%88%98-MAE-MSE-RMSE-MAPE-MPE">https://sy-log.tistory.com/entry/%ED%9A%8C%EA%B7%80%EB%AA%A8%EB%8D%B8-%ED%8F%89%EA%B0%80%EC%A7%80%ED%91%9C-R2-score-%EA%B2%B0%EC%A0%95%EA%B3%84%EC%88%98-MAE-MSE-RMSE-MAPE-MPE</a>
[3] [ML] 머신러닝 평가지표 - 회귀 모델 MSE, RMSE, MAE - DataPilots <a href="https://datapilots.tistory.com/42">https://datapilots.tistory.com/42</a>
[4] 회귀 평가지표 - MAE, MSE, R² - 퇴근 후 study with me - 티스토리 <a href="https://for-my-wealthy-life.tistory.com/68">https://for-my-wealthy-life.tistory.com/68</a>
[5] 회귀 모델 성능평가지표 : MAE, MSE, RMSE, R2 Score <a href="https://emjayahn.github.io/2022/02/10/Regression-Score/">https://emjayahn.github.io/2022/02/10/Regression-Score/</a>
[6] 머신러닝 - 17. 회귀 평가 지표 - 귀퉁이 서재 - 티스토리 <a href="https://bkshin.tistory.com/entry/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-17-%ED%9A%8C%EA%B7%80-%ED%8F%89%EA%B0%80-%EC%A7%80%ED%91%9C">https://bkshin.tistory.com/entry/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-17-%ED%9A%8C%EA%B7%80-%ED%8F%89%EA%B0%80-%EC%A7%80%ED%91%9C</a>
[7] 회귀 모델 평가 지표 정리 - 하미's 블로그 <a href="https://carpe08.tistory.com/501">https://carpe08.tistory.com/501</a>
[8] [D+25][ML] 지도학습 - 회귀 분석과 평가지표(MSE, RMSE, R²) <a href="https://soojung624.tistory.com/26">https://soojung624.tistory.com/26</a>
[9] 회귀분석(Regression)의 모델 평가 알아보기 - 브런치 <a href="https://brunch.co.kr/@26dbf56c3e594db/67">https://brunch.co.kr/@26dbf56c3e594db/67</a>
[10] [TIL] 다중선형회귀와 평가지표 - velog <a href="https://velog.io/@woooa/TIL-%EB%8B%A4%EC%A4%91%EC%84%A0%ED%98%95%ED%9A%8C%EA%B7%80%EC%99%80-%ED%8F%89%EA%B0%80%EC%A7%80%ED%91%9C">https://velog.io/@woooa/TIL-%EB%8B%A4%EC%A4%91%EC%84%A0%ED%98%95%ED%9A%8C%EA%B7%80%EC%99%80-%ED%8F%89%EA%B0%80%EC%A7%80%ED%91%9C</a></p>
<h3 id="_38">코사인 유사도<a class="headerlink" href="#_38" title="Permanent link">&para;</a></h3>
<h4 id="_39">코사인 유사도의 정의<a class="headerlink" href="#_39" title="Permanent link">&para;</a></h4>
<p>코사인 유사도(Cosine Similarity)는 두 벡터 간의 방향이 얼마나 유사한지를 측정하는 지표로, 벡터의 크기보다는 두 벡터가 이루는 각도(코사인 값)를 기반으로 유사도를 계산합니다. 값의 범위는 -1에서 1 사이이며, 1에 가까울수록 두 벡터가 같은 방향, 0에 가까울수록 두 벡터가 직각, -1은 완전히 반대 방향임을 의미합니다.</p>
<h4 id="_40">코사인 유사도 계산식<a class="headerlink" href="#_40" title="Permanent link">&para;</a></h4>
<p>코사인 유사도는 다음의 수식으로 계산됩니다.</p>
<div class="arithmatex">\[
\text{cosine similarity} = \frac{\mathbf{A} \cdot \mathbf{B}}{\|\mathbf{A}\| \cdot \|\mathbf{B}\|}
\]</div>
<p>여기서<br />
- <span class="arithmatex">\(<span class="arithmatex">\(\mathbf{A} \cdot \mathbf{B}\)</span>\)</span>: 벡터 A와 B의 내적<br />
- <span class="arithmatex">\(<span class="arithmatex">\(\|\mathbf{A}\|\)</span>\)</span>: 벡터 A의 크기(노름)<br />
- <span class="arithmatex">\(<span class="arithmatex">\(\|\mathbf{B}\|\)</span>\)</span>: 벡터 B의 크기(노름)</p>
<h4 id="_41">코사인 유사도 이용 사례<a class="headerlink" href="#_41" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>문서 유사도 측정</strong>: 자연어 처리(NLP) 분야에서 두 개의 문서가 얼마나 유사한지 평가할 때 문서별 단어 벡터(또는 TF-IDF 벡터) 간의 코사인 유사도를 활용.</li>
<li><strong>추천 시스템</strong>: 사용자와 아이템의 선호 벡터 간 유사도 계산을 통해 맞춤형 추천 제공.</li>
<li><strong>클러스터링</strong>: 유사도가 높은 데이터 포인트끼리 군집화하는 데 사용.</li>
</ul>
<h4 id="_42">코사인 유사도와 비슷한 다른 개념<a class="headerlink" href="#_42" title="Permanent link">&para;</a></h4>
<p>자연어처리(NLP)에서 텍스트의 유사성을 수치화할 때 널리 쓰이는 여러 <strong>유사도 평가 방법</strong>을 아래 표로 정리합니다.</p>
<table>
<thead>
<tr>
<th><strong>유사도 지표</strong></th>
<th><strong>계산 방식 및 특징</strong></th>
<th><strong>주요 활용 예</strong></th>
<th><strong>참고</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>코사인 유사도</td>
<td>두 벡터의 내적을 각각의 노름(크기)으로 나눈 값, -1~1 범위, <strong>방향성</strong>을 중시함. 크기에 영향을 받지 않음[1][3][7].</td>
<td>문서/문장 임베딩 비교, 추천 시스템, 검색 엔진</td>
<td>가장 널리 사용됨</td>
</tr>
<tr>
<td>유클리드 거리(유사도)</td>
<td>두 벡터 사이의 <strong>직선 거리</strong>(L2 norm)를 측정, 작을수록 유사, <strong>절대적 크기</strong> 반영. 단, 데이터가 표준화되어 있지 않으면 주의 필요[2][5].</td>
<td>이미지, 사운드 데이터 마이닝, 임베딩 비교</td>
<td>거리 기반 방식</td>
</tr>
<tr>
<td>맨허튼(맨해탄) 거리(유사도)</td>
<td>각 차원의 <strong>절대값 차이 합</strong>(L1 norm), 작을수록 유사, <strong>좌표(차원) 단위 절대 거리</strong> 측정[2][5].</td>
<td>문장/단어 임베딩 비교, 특이값 탐지</td>
<td>거리 기반 방식</td>
</tr>
<tr>
<td>자카드 유사도</td>
<td><strong>집합</strong>으로 변환 후, 교집합 크기 / 합집합 크기. 0~1 범위, <strong>단어의 중복 여부</strong>만 반영(순서 무시)[2][5][6].</td>
<td>짧은 텍스트, 이벤트/행동 기록 등 집합형 데이터</td>
<td>이진/집합 데이터에 적합</td>
</tr>
<tr>
<td>피어슨 상관계수</td>
<td>두 변수 간 <strong>선형 상관관계</strong>(가장 유사: +1, 반대 방향: -1, 무관계: 0), <strong>표준화된 민감도</strong> 측정, 평균, 분산을 고려함.</td>
<td>신호/시계열 데이터, 선형 회귀 텍스트 실험 등</td>
<td>통계적 유사도</td>
</tr>
<tr>
<td>n-gram 기반 유사도</td>
<td>n개의 연속 문자(혹은 단어)를 묶어, 겹치는 n-gram의 비율로 유사도 산출. <strong>국부적 패턴</strong>에 강점[4].</td>
<td>표절 탐지, 음성 인식, 맞춤법 검사</td>
<td>문자열/음성 데이터</td>
</tr>
</tbody>
</table>
<ul>
<li><strong>코사인 유사도</strong>: 문장 임베딩이나 문서 벡터화 후 방향성을 기반으로 유사도 산출, <strong>가장 대표적</strong>임[1][3][9].</li>
<li><strong>유클리드/맨해탄 거리</strong>: 벡터 간 거리(패턴, 위치) 차이에 주목, 데이터 정규화가 중요.</li>
<li><strong>자카드 유사도</strong>: 순서, 빈도, 문법적 구조를 무시하고 집합의 교집합/합집합 비율로 산출, <strong>짧은 텍스트/이진 데이터</strong>에 적합[2][5][6].</li>
<li><strong>피어슨 상관계수</strong>: 두 벡터의 <strong>선형 관계</strong> 측정, 사회과학/과학적 방법론에서도 활용.</li>
<li><strong>n-gram 유사도</strong>: 문자열/음성의 <strong>국소적 패턴</strong> 일치를 중시, 띄어쓰기/맞춤법 오류 탐지 등에 효과적[4].</li>
</ul>
<p>실제 NLP에서는 <strong>코사인 유사도</strong>와 <strong>임베딩 벡터화</strong>가 가장 보편적이며, <strong>자카드 유사도</strong>는 집합 기준 텍스트 간 중복 검출 등에, <strong>n-gram</strong>은 표절/음성 패턴 매칭 등에 적합합니다. <strong>거리 기반</strong> 방법(유클리드, 맨해탄)은 차원 축소나 비정형 데이터 탐색 시, <strong>피어슨</strong>은 통계적 분석 시 활용됩니다.</p>
<p>출처
[1] [python] 자연어처리(NLP) - 텍스트 유사도 <a href="https://wonhwa.tistory.com/26">https://wonhwa.tistory.com/26</a>
[2] 3장 자연어 처리 개요(3) : 텍스트 유사도 <a href="https://coshin.tistory.com/31">https://coshin.tistory.com/31</a>
[3] 05-01 코사인 유사도(Cosine Similarity) - 위키독스 <a href="https://wikidocs.net/24603">https://wikidocs.net/24603</a>
[4] [자연어처리] 중복 검출을 위한 텍스트 유사도 측정 - velog <a href="https://velog.io/@ykang5/%ED%85%8D%EC%8A%A4%ED%8A%B8-%EC%9C%A0%EC%82%AC%EB%8F%84-%EC%B8%A1%EC%A0%95">https://velog.io/@ykang5/%ED%85%8D%EC%8A%A4%ED%8A%B8-%EC%9C%A0%EC%82%AC%EB%8F%84-%EC%B8%A1%EC%A0%95</a>
[5] 자연어 처리 개요_텍스트 유사도 - 데이터 한 그릇 - 티스토리 <a href="https://kurt7191.tistory.com/117">https://kurt7191.tistory.com/117</a>
[6] [ NLP ] 텍스트 유사도 ( 자카드 유사도, 코사인 유사도 ) - 야누쓰 <a href="https://yanoo.tistory.com/21">https://yanoo.tistory.com/21</a>
[7] [자연어처리 입문] 4. 벡터의 유사도(Vector Similarity) <a href="https://codong.tistory.com/35">https://codong.tistory.com/35</a>
[8] 유사도 측정법 (Similarity Measure) - 도리의 디지털라이프 <a href="https://blog.skby.net/%EC%9C%A0%EC%82%AC%EB%8F%84-%EC%B8%A1%EC%A0%95%EB%B2%95-similarity-measure/">https://blog.skby.net/%EC%9C%A0%EC%82%AC%EB%8F%84-%EC%B8%A1%EC%A0%95%EB%B2%95-similarity-measure/</a>
[9] OpenAI 임베딩으로 유사한 문장 찾기 실습 - velog <a href="https://velog.io/@cha-suyeon/OpenAI-%EC%9E%84%EB%B2%A0%EB%94%A9%EC%9C%BC%EB%A1%9C-%EC%9C%A0%EC%82%AC%ED%95%9C-%EB%AC%B8%EC%9E%A5-%EC%B0%BE%EA%B8%B0-%EC%8B%A4%EC%8A%B5">https://velog.io/@cha-suyeon/OpenAI-%EC%9E%84%EB%B2%A0%EB%94%A9%EC%9C%BC%EB%A1%9C-%EC%9C%A0%EC%82%AC%ED%95%9C-%EB%AC%B8%EC%9E%A5-%EC%B0%BE%EA%B8%B0-%EC%8B%A4%EC%8A%B5</a></p>
<h2 id="6">6. 모델 평가 심화<a class="headerlink" href="#6" title="Permanent link">&para;</a></h2>
<h3 id="confusion-matrix">혼동 행렬(Confusion Matrix) 해석<a class="headerlink" href="#confusion-matrix" title="Permanent link">&para;</a></h3>
<p>혼동 행렬은 분류 모델 성능을 평가하는 데 많이 사용되는 도구로, 모델의 예측값과 실제값 간 관계를 표로 나타냅니다. 보통 다음 4가지 요소로 구성됩니다:</p>
<ul>
<li><strong>True Positive (TP)</strong>: 실제 Positive를 정확히 Positive로 예측</li>
<li><strong>True Negative (TN)</strong>: 실제 Negative를 정확히 Negative로 예측</li>
<li><strong>False Positive (FP)</strong>: 실제 Negative를 잘못 Positive로 예측 (오류 유형 1)</li>
<li><strong>False Negative (FN)</strong>: 실제 Positive를 잘못 Negative로 예측 (오류 유형 2)</li>
</ul>
<p>이 행렬을 바탕으로 정확도(Accuracy), 정밀도(Precision), 재현율(Recall), F1 점수 등 다양한 지표를 계산하며, 특히 불균형 데이터 상황에서 단순 정확도보다 더 의미 있는 성능 파악이 가능합니다. 다중 클래스 분류 문제도 각 클래스마다 혼동 행렬을 작성하여 어떤 클래스 간 혼동이 많은지 시각적으로 분석할 수 있습니다.</p>
<hr />
<h3 id="sample-efficiency">샘플 효율성(Sample Efficiency)<a class="headerlink" href="#sample-efficiency" title="Permanent link">&para;</a></h3>
<p>샘플 효율성은 모델이 주어진 데이터 샘플을 얼마나 효율적으로 활용하여 높은 성능을 낼 수 있는지를 나타냅니다. 적은 데이터에서도 빠르게 학습하여 효율적으로 일반화하는 능력을 의미합니다. 특히 강화학습이나 저자원 환경에서 중요한 평가 요소로 활용됩니다. 샘플 효율성이 높은 모델은 데이터 및 계산 비용을 줄이면서도 좋은 결과를 이끌어냅니다.</p>
<hr />
<h3 id="oodout-of-distribution">OOD(Out-of-Distribution) 데이터 평가<a class="headerlink" href="#oodout-of-distribution" title="Permanent link">&para;</a></h3>
<p>OOD 평가는 모델이 훈련 데이터 분포를 벗어난 새로운 데이터에 대해 얼마나 잘 대응하는지를 확인하는 과정입니다. 실제 활용 환경에서는 훈련 데이터와 분포가 다른 데이터가 자주 발생하기 때문에, 모델이 이러한 OOD 데이터를 인지하고 올바른 판단을 할 수 있어야 신뢰할 수 있습니다. OOD 탐지 기법으로는 Softmax 확률 분석, ODIN, Outlier Exposure 등이 있습니다.</p>
<hr />
<h3 id="llm">LLM·멀티모달 모델 평가 방법론<a class="headerlink" href="#llm" title="Permanent link">&para;</a></h3>
<p>대형 언어 모델(LLM)과 텍스트, 이미지, 음성 등 복수의 입력 모달리티를 처리하는 멀티모달 모델의 평가는 다음과 같은 방법을 사용합니다:</p>
<ul>
<li>작업별 벤치마크 데이터셋 활용 (예: Visual Question Answering, 이미지 캡션 등)</li>
<li>마이크로/Macro F1, 정확도 등 다양한 다면적 지표 도입</li>
<li>모달별 세밀한 평가를 통해 단일 모달 동작뿐 아니라 모달 간 상호작용 및 통합 능력 분석</li>
<li>사용자 피드백과 실제 사용 사례를 반영한 정성적 평가 병행</li>
</ul>
<p>이런 평가 방법을 통해 복잡한 모델이 다양한 환경과 태스크에서 안정적이고 일관된 성능을 발휘하는지 검증할 수 있습니다.</p>
<p>출처
[1] 혼동행렬(confusion matrix) - 의미를 이해하는 통계학과 데이터 분석 <a href="https://diseny.tistory.com/entry/%ED%98%BC%EB%8F%99%ED%96%89%EB%A0%ACconfusion-matrix">https://diseny.tistory.com/entry/%ED%98%BC%EB%8F%99%ED%96%89%EB%A0%ACconfusion-matrix</a>
[2] 혼동 행렬(Confusion Matrix) 해석 - 하미's 블로그 <a href="https://carpe08.tistory.com/500">https://carpe08.tistory.com/500</a>
[3] Confusion Matrix로 분류모델 성능평가 지표(precision, recall, f1 ... <a href="https://kyull-it.tistory.com/99">https://kyull-it.tistory.com/99</a>
[4] [파이썬 sklearn] 오차행렬(혼동행렬, confusion matrix) 공부하기 - 선비 <a href="https://spine-sunbi.tistory.com/entry/%ED%8C%8C%EC%9D%B4%EC%8D%AC-sklearn-%EC%98%A4%EC%B0%A8%ED%96%89%EB%A0%AC%ED%98%BC%EB%8F%99%ED%96%89%EB%A0%AC-confusion-matrix-%EA%B3%B5%EB%B6%80%ED%95%98%EA%B8%B0-%ED%8F%89%EA%B0%80-%EC%A7%80%ED%91%9C-%EC%9D%B4%ED%95%B41">https://spine-sunbi.tistory.com/entry/%ED%8C%8C%EC%9D%B4%EC%8D%AC-sklearn-%EC%98%A4%EC%B0%A8%ED%96%89%EB%A0%AC%ED%98%BC%EB%8F%99%ED%96%89%EB%A0%AC-confusion-matrix-%EA%B3%B5%EB%B6%80%ED%95%98%EA%B8%B0-%ED%8F%89%EA%B0%80-%EC%A7%80%ED%91%9C-%EC%9D%B4%ED%95%B41</a>
[5] [ML] 혼동 행렬(Confusion Matrix) / TP/TN/FP/FN - 홍시의 씽크탱크 <a href="https://kimhongsi.tistory.com/entry/ML-%ED%98%BC%EB%8F%99-%ED%96%89%EB%A0%ACConfusion-Matrix-TPTNFPFN">https://kimhongsi.tistory.com/entry/ML-%ED%98%BC%EB%8F%99-%ED%96%89%EB%A0%ACConfusion-Matrix-TPTNFPFN</a>
[6] 머신러닝 분류 - 혼동 행렬(Confusion matrix) - 세상탐험대 블로그 <a href="https://skillmemory.tistory.com/entry/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-%EB%B6%84%EB%A5%98-%ED%98%BC%EB%8F%99-%ED%96%89%EB%A0%ACConfusion-matrix">https://skillmemory.tistory.com/entry/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-%EB%B6%84%EB%A5%98-%ED%98%BC%EB%8F%99-%ED%96%89%EB%A0%ACConfusion-matrix</a>
[7] 혼동행렬 (Confusion Matrix) - 지식덤프 <a href="http://www.jidum.com/jidums/view.do?jidumId=1212">http://www.jidum.com/jidums/view.do?jidumId=1212</a>
[8] 혼동 행렬이란 무엇인가요? - IBM <a href="https://www.ibm.com/kr-ko/think/topics/confusion-matrix">https://www.ibm.com/kr-ko/think/topics/confusion-matrix</a></p>
<h2 id="7">7. 모델 성능 향상 기법<a class="headerlink" href="#7" title="Permanent link">&para;</a></h2>
<h3 id="transfer-learning">전이학습(Transfer Learning)<a class="headerlink" href="#transfer-learning" title="Permanent link">&para;</a></h3>
<p>전이학습은 이미 어떤 작업(Task)에 대해 학습된 모델이나 네트워크가 가진 지식을 새로운, 관련되었거나 유사한 작업에 재활용하여 학습 효율성과 성능을 개선하는 기술입니다. 예를 들어 대량의 이미지 데이터인 ImageNet으로 미리 학습된 CNN 모델을 가져와 고양이와 개를 분류하는 작은 데이터셋에 적용하는 것입니다. 이 경우 초기 레이어들은 이미지의 기본 특성(모서리, 패턴 등)을 추출하는 역할을 하므로 이 지식을 그대로 활용하고, 후반부 레이어만 새롭게 학습하거나 미세 조정하여 고양이 vs 개 문제에 맞게 최적화합니다.</p>
<ul>
<li><strong>장점</strong></li>
<li>대량 데이터와 긴 학습 시간을 절약할 수 있음</li>
<li>작은 데이터셋으로도 좋은 성능 달성 가능</li>
<li>사전 학습된 모델의 특징 추출 능력을 활용해 오버피팅 방지에 유리</li>
<li><strong>구성 요소</strong></li>
<li>소스 도메인/태스크: 최초 학습된 도메인과 작업</li>
<li>타겟 도메인/태스크: 새로운 도메인과 작업</li>
<li><strong>전이학습 방법</strong></li>
<li>고정된 피처 추출기 사용 후 분류기만 재학습</li>
<li>미세 조정(fine-tuning)으로 전체 또는 일부 네트워크 파라미터 수정</li>
</ul>
<p>사전학습된 모델을 재활용하므로, 전이학습은 사전학습(pre-training)과 미세 조정(fine-tuning)을 아우르는 개념으로도 볼 수 있습니다.</p>
<hr />
<h3 id="fine-tuning">파인튜닝(Fine-tuning)<a class="headerlink" href="#fine-tuning" title="Permanent link">&para;</a></h3>
<p>파인튜닝은 전이학습 중 사전학습된 모델을 특정한 작은 데이터셋이나 특화된 태스크에 맞추어 추가로 학습시키는 과정입니다. 주로 전체 모델 또는 일부 계층의 가중치를 업데이트하여 모델이 새로운 데이터 환경이나 요구사항에 적합하도록 조정합니다.</p>
<ul>
<li><strong>특징</strong></li>
<li>보통 낮은 학습률을 사용해 기존 가중치의 안정성을 유지하면서 점진적 적응</li>
<li>과적합 위험 감소를 위해 일부 레이어만 훈련하거나 드롭아웃, 조기 종료 같은 기법을 병행</li>
<li>특정 도메인(의료, 법률 등)이나 목적에 특화된 모델 개발에 강점</li>
</ul>
<hr />
<h3 id="lora-prefix-tuning">LoRA, Prefix Tuning 등 경량 학습 기법<a class="headerlink" href="#lora-prefix-tuning" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>LoRA (Low-Rank Adaptation)</strong></li>
<li>대체로 크고 복잡한 LLM에 적용</li>
<li>전체 모델 가중치를 업데이트하는 대신, 가중치 행렬을 저차원 행렬들의 곱으로 분해하고 이들 중 일부만 학습함</li>
<li>저장 공간과 계산량 크게 감소시키며 효율적인 미세 조정 가능</li>
<li>
<p>기존 사전학습 모델을 그대로 두면서 추가 학습만 수행, 확장성 좋음</p>
</li>
<li>
<p><strong>Prefix Tuning</strong></p>
</li>
<li>프롬프트 기반 미세 조정 기법</li>
<li>모델 입력 앞에 학습 가능한 "프리픽스"(특수 벡터 시퀀스)를 추가</li>
<li>모델 파라미터는 고정한 채 프리픽스 벡터만 업데이트</li>
<li>빠르고 자원 효율적이며, 대형 모델에 적합</li>
</ul>
<p>이들 경량 학습 기법은 급격한 모델 크기 증가와 이를 학습시키는 비용 부담 문제를 해결하며, 최근 대형 언어 모델 적용에 필수적인 기술로 주목받고 있습니다.</p>
<p>출처
[1] 전이 학습이란 무엇인가요? - IBM <a href="https://www.ibm.com/kr-ko/think/topics/transfer-learning">https://www.ibm.com/kr-ko/think/topics/transfer-learning</a>
[2] 전이학습(Transfer Learning)이란? - 데이콘 <a href="https://dacon.io/forum/405988">https://dacon.io/forum/405988</a>
[3] 딥러닝 기초 - 전이 학습 이해하기 - velog <a href="https://velog.io/@tjdtnsu/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B8%B0%EC%B4%88-%EC%A0%84%EC%9D%B4-%ED%95%99%EC%8A%B5-%EC%9D%B4%ED%95%B4%ED%95%98%EA%B8%B0">https://velog.io/@tjdtnsu/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B8%B0%EC%B4%88-%EC%A0%84%EC%9D%B4-%ED%95%99%EC%8A%B5-%EC%9D%B4%ED%95%B4%ED%95%98%EA%B8%B0</a>
[4] 전이 학습(Transfer learning)이란? 정의, 사용 방법, AI 구축 | appen 에펜 <a href="https://kr.appen.com/blog/transfer-learning/">https://kr.appen.com/blog/transfer-learning/</a>
[5] 전이 학습(Transfer Learning) 개념 및 활용 - learningflix - 티스토리 <a href="https://learningflix.tistory.com/138">https://learningflix.tistory.com/138</a>
[6] 05화 전이학습(Transfer Learning)이란? - 브런치 <a href="https://brunch.co.kr/@harryban0917/283">https://brunch.co.kr/@harryban0917/283</a>
[7] 전이학습: 사전 훈련된 모델 활용 전략 - 재능넷 <a href="https://www.jaenung.net/tree/14200">https://www.jaenung.net/tree/14200</a>
[8] 파이썬을 활용한 딥러닝 전이학습 - 위키북스 <a href="https://wikibook.co.kr/transfer-learning/">https://wikibook.co.kr/transfer-learning/</a>
[9] 전이 학습을 위한 기반 모델 결정 방법 및 그 방법을 지원하는 장치 <a href="https://patents.google.com/patent/KR102439606B1/ko">https://patents.google.com/patent/KR102439606B1/ko</a>
[10] 21.03.04. 딥러닝 - - 모도리는 공부중 - 티스토리 <a href="https://studying-modory.tistory.com/entry/210304-%EB%94%A5%EB%9F%AC%EB%8B%9D">https://studying-modory.tistory.com/entry/210304-%EB%94%A5%EB%9F%AC%EB%8B%9D</a></p>
<h3 id="ensemble"><a href="https://github.com/zoro0rkd/ai_study/wiki/AI-모델-아키텍처-설계-%E2%80%90-NEW#9-앙상블-구조">앙상블(Ensemble) 기법</a><a class="headerlink" href="#ensemble" title="Permanent link">&para;</a></h3>
<h3 id="_43">경량화 기법<a class="headerlink" href="#_43" title="Permanent link">&para;</a></h3>
<p>AI 모델 경량화 기법 중 <strong>Pruning(가지치기)</strong>은 모델의 불필요한 매개변수를 제거하여 크기와 계산 복잡도를 줄이는 기술입니다. 이 기법은 <strong>EfficientNet</strong>, <strong>MobileNet</strong>, <strong>SqueezeNet</strong> 등 모바일 친화적 아키텍처에서 핵심적으로 활용되며, 다음과 같은 방식으로 적용됩니다.</p>
<hr />
<h4 id="pruning">Pruning의 작동 원리<a class="headerlink" href="#pruning" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>Unstructured Pruning</strong>: 개별 가중치를 무작위로 제거하는 방식으로, 세밀한 압축이 가능하지만 하드웨어 가속에 비효율적[1][8].</li>
<li><strong>Structured Pruning</strong>: 뉴런/필터 단위로 제거하여 하드웨어 호환성을 높이며, MobileNet 등에서 채널 단위 절삭에 활용[3][8].</li>
<li><strong>Lottery Ticket Hypothesis</strong>: 초기 가중치를 유지한 서브네트워크가 원본 모델과 유사한 성능을 보인다는 이론으로, 반복적 가지치기 기반[1][7].</li>
</ul>
<table>
<thead>
<tr>
<th>기법</th>
<th>특징</th>
<th>적용 예시</th>
</tr>
</thead>
<tbody>
<tr>
<td>Unstructured</td>
<td>95% 가중치 제거 가능</td>
<td>VGG, ResNet[6]</td>
</tr>
<tr>
<td>Structured</td>
<td>GPU 가속 최적화</td>
<td>MobileNet, EfficientNet[5][9]</td>
</tr>
</tbody>
</table>
<hr />
<h4 id="_44">주요 모델별 적용 사례<a class="headerlink" href="#_44" title="Permanent link">&para;</a></h4>
<h5 id="1-squeezenet">1. SqueezeNet<a class="headerlink" href="#1-squeezenet" title="Permanent link">&para;</a></h5>
<ul>
<li><strong>1x1 컨볼루션</strong>으로 기본 구조를 경량화한 후, <strong>Deep Compression</strong>(가지치기 + 6비트 양자화) 적용 시 <strong>510x 크기 감소</strong> 달성[6].</li>
<li>원본 AlexNet 대비 50x 작은 4.8MB 모델로 동등한 정확도 유지[6].</li>
</ul>
<h5 id="2-mobilenet">2. MobileNet<a class="headerlink" href="#2-mobilenet" title="Permanent link">&para;</a></h5>
<ul>
<li><strong>Depthwise Separable Convolution</strong>으로 매개변수 90% 절감[5].</li>
<li>AutoML 기반 <strong>AMC(Automated Model Compression)</strong>로 레이어별 최적 압축 정책 탐색[9]:</li>
<li>GPU(Titan Xp)에서 1.53x, 픽셀 폰에서 1.95x 추론 속도 향상.</li>
</ul>
<h5 id="3-efficientnet">3. EfficientNet<a class="headerlink" href="#3-efficientnet" title="Permanent link">&para;</a></h5>
<ul>
<li><strong>Compound Scaling</strong>(깊이/너비/해상도 통합 최적화)으로 구조 효율화[9].</li>
<li>강화학습 기반 자동 압축(AMC)으로 리소스 제약 조건 하 최적 서브네트워크 탐색[9].</li>
</ul>
<hr />
<p>Pruning은 단독으로도 효과적이지만 <strong>양자화</strong>, <strong>지식 증류</strong>와 결합할 때 최대 효율을 발휘합니다. 모바일 디바이스의 실시간 AI 적용을 위해 MobileNet/EfficientNet은 구조 설계 단계부터 압축을 고려하며, SqueezeNet은 사후 압축의 극단적 사례로 자리잡았습니다[5][6][9].</p>
<p>출처
[1] 모델 경량화 1 - Pruning(가지치기) <a href="https://blogik.netlify.app/boostcamp/u_stage/45_pruning/">https://blogik.netlify.app/boostcamp/u_stage/45_pruning/</a>
[2] Model Pruning: Keeping the Essentials - Unify AI <a href="https://unify.ai/blog/compression-pruning">https://unify.ai/blog/compression-pruning</a>
[3] Pruning (artificial neural network) - Wikipedia <a href="https://en.wikipedia.org/wiki/Pruning_(artificial_neural_network">https://en.wikipedia.org/wiki/Pruning_(artificial_neural_network</a>)
[4] Edge AI: Evaluation of Model Compression Techniques for ... - arXiv <a href="https://arxiv.org/html/2409.02134v1">https://arxiv.org/html/2409.02134v1</a>
[5] Deep Learning on Mobile Devices: Strategies for Model ... - Zetic.ai <a href="https://zetic.ai/ko/blog/deep-learning-on-mobile-devices-strategies-for-model-compression-and-optimization">https://zetic.ai/ko/blog/deep-learning-on-mobile-devices-strategies-for-model-compression-and-optimization</a>
[6] [PDF] SQUEEZENET: ALEXNET-LEVEL ACCURACY WITH 50X FEWER ... <a href="https://openreview.net/pdf?id=S1xh5sYgx">https://openreview.net/pdf?id=S1xh5sYgx</a>
[7] 딥러닝 모델 경량화를 위한 Pruning 기본 개념 정리 - Seanpark <a href="https://seanpark11.tistory.com/190">https://seanpark11.tistory.com/190</a>
[8] AI Model Compression Techniques - Sogeti Labs <a href="https://labs.sogeti.com/ai-model-compression-techniques/">https://labs.sogeti.com/ai-model-compression-techniques/</a>
[9] [PDF] AutoML for Model Compression and Acceleration on Mobile Devices <a href="https://openaccess.thecvf.com/content_ECCV_2018/papers/Yihui_He_AMC_Automated_Model_ECCV_2018_paper.pdf">https://openaccess.thecvf.com/content_ECCV_2018/papers/Yihui_He_AMC_Automated_Model_ECCV_2018_paper.pdf</a>
[10] 모델 경량화 방법 - 인공지능 공부 - 티스토리 <a href="https://ai0-0jiyun.tistory.com/5">https://ai0-0jiyun.tistory.com/5</a>
[11] 경량화 기법 정리: Pruning, Quantization, Knowledge Distillation - velog <a href="https://velog.io/@mmodestaa/%EA%B2%BD%EB%9F%89%ED%99%94-%EA%B8%B0%EB%B2%95-%EC%A0%95%EB%A6%AC-Pruning-Quantization-Knowledge-Distillation">https://velog.io/@mmodestaa/%EA%B2%BD%EB%9F%89%ED%99%94-%EA%B8%B0%EB%B2%95-%EC%A0%95%EB%A6%AC-Pruning-Quantization-Knowledge-Distillation</a>
[12] [최적화] 모델 경량화 , AutoML , Pruning , Knowledge Distillation ... <a href="https://amber-chaeeunk.tistory.com/110">https://amber-chaeeunk.tistory.com/110</a>
[13] 딥러닝 모델 최적화 방법: 모델 경량화와 모델 추론 속도 가속화 <a href="https://blog-ko.superb-ai.com/how-to-optimize-deep-learning-models/">https://blog-ko.superb-ai.com/how-to-optimize-deep-learning-models/</a>
[14] [2412.02328] Efficient Model Compression Techniques with FishLeg <a href="https://arxiv.org/abs/2412.02328">https://arxiv.org/abs/2412.02328</a>
[15] A Comparative Study of Preprocessing and Model Compression ... <a href="https://www.mdpi.com/1424-8220/24/4/1149">https://www.mdpi.com/1424-8220/24/4/1149</a>
[16] [PDF] Lecture 9: Models Compression Techniques - GitHub Pages <a href="https://harvard-iacs.github.io/2023-AC215/assets/lectures/lecture9/05_model2_compression_techniques.pdf">https://harvard-iacs.github.io/2023-AC215/assets/lectures/lecture9/05_model2_compression_techniques.pdf</a>
[17] [PDF] Efficient Model Compression Techniques with FishLeg - OpenReview <a href="https://openreview.net/pdf?id=0PnN3hKYL7">https://openreview.net/pdf?id=0PnN3hKYL7</a>
[18] Model Compression - an overview | ScienceDirect Topics <a href="https://www.sciencedirect.com/topics/computer-science/model-compression">https://www.sciencedirect.com/topics/computer-science/model-compression</a>
[19] [PDF] What is the State of Neural Network Pruning? - arXiv <a href="https://arxiv.org/pdf/2003.03033.pdf">https://arxiv.org/pdf/2003.03033.pdf</a>
[20] Neural Network Pruning - Nathan Hubens <a href="https://nathanhubens.github.io/posts/deep%20learning/2020/05/22/pruning.html">https://nathanhubens.github.io/posts/deep%20learning/2020/05/22/pruning.html</a></p>
<h2 id="8">8. 모델 학습과 평가의 한계 및 고려사항<a class="headerlink" href="#8" title="Permanent link">&para;</a></h2>
<h3 id="bias-variance">Bias &amp; Variance<a class="headerlink" href="#bias-variance" title="Permanent link">&para;</a></h3>
<p><img alt="스크린샷 2025-07-07 오후 1 48 02" src="https://github.com/user-attachments/assets/0dd50ce8-07e2-433b-ae0f-a6fe44a4dd70" /></p>
<p>AI를 학습하면 4가지 케이스 중 하나.</p>
<h4 id="bias-variance-tradeoff">Bias &amp; Variance tradeoff<a class="headerlink" href="#bias-variance-tradeoff" title="Permanent link">&para;</a></h4>
<p><img alt="스크린샷 2025-07-07 오후 1 53 08" src="https://github.com/user-attachments/assets/e41164be-bee2-4278-a8bd-c616a9e63680" /></p>
<p><img alt="스크린샷 2025-07-07 오후 1 53 24" src="https://github.com/user-attachments/assets/694b5679-5487-440a-8c8e-bcfdf3e353f8" /></p>
<p><strong>Bias-Variance tradeoff(편향-분산 트레이드오프)</strong>는 머신러닝과 통계학에서 모델의 성능과 일반화 능력을 이해하는 데 핵심적인 개념입니다. 이는 <strong>모델의 복잡성에 따라 발생하는 두 가지 주요 오차(편향, 분산)</strong> 사이의 균형을 의미하며, 이 두 오차를 동시에 최소화하는 것이 불가능하다는 딜레마를 설명합니다[1][3][5].</p>
<h3 id="tradeoff">왜 Tradeoff(트레이드오프)인가?<a class="headerlink" href="#tradeoff" title="Permanent link">&para;</a></h3>
<ul>
<li>
<p><strong>편향과 분산은 반비례 관계</strong>에 있습니다.<br />
  모델이 단순할수록(복잡성이 낮을수록) 편향은 커지고 분산은 작아집니다.<br />
  반대로 모델이 복잡할수록 편향은 줄어들지만 분산이 커집니다[1][8].</p>
</li>
<li>
<p><strong>이상적인 모델</strong>은 편향과 분산이 모두 낮아야 하지만, 실제로는 둘을 동시에 최소화할 수 없습니다.<br />
  따라서 <strong>적절한 균형점(Optimal Point)</strong>을 찾는 것이 중요합니다[1][5].</p>
</li>
</ul>
<h4 id="_45">수식과 시각적 이해<a class="headerlink" href="#_45" title="Permanent link">&para;</a></h4>
<ul>
<li>
<p><strong>예측 오차(Mean Squared Error, MSE)는 다음과 같이 분해할 수 있습니다:</strong><br />
  $$
  \text{MSE} = \text{Bias}^2 + \text{Variance} + \text{Irreducible Error}
  $$
  여기서 Irreducible Error(줄일 수 없는 오차)는 데이터 자체의 노이즈 등 모델로는 줄일 수 없는 부분입니다[4][5].</p>
</li>
<li>
<p><strong>그래프 상에서</strong>  </p>
</li>
<li>모델 복잡성이 증가할수록 편향은 감소, 분산은 증가  </li>
<li>테스트 에러는 처음에는 감소하다가(편향 감소 효과), 이후 다시 증가(분산 증가 효과)  </li>
<li><strong>최적점</strong>은 테스트 에러가 최소가 되는 지점, 즉 적절한 복잡성을 가진 모델입니다[5].</li>
</ul>
<h4 id="_46">실전에서의 활용<a class="headerlink" href="#_46" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>Underfitting(과소적합)</strong>: 높은 편향, 낮은 분산 → 모델이 너무 단순  </li>
<li><strong>Overfitting(과대적합)</strong>: 낮은 편향, 높은 분산 → 모델이 너무 복잡  </li>
<li><strong>좋은 모델</strong>: 적절한 편향과 분산의 균형을 이루어, 훈련 데이터뿐 아니라 새로운 데이터(테스트 데이터)에서도 좋은 성능을 보임[5][7].</li>
</ul>
<p><strong>요약</strong><br />
Bias-Variance tradeoff는 머신러닝 모델이 훈련 데이터와 새로운 데이터 모두에서 잘 작동하도록 하기 위해, 모델의 복잡성과 두 오차(편향, 분산) 사이의 균형을 어떻게 맞출 것인지를 설명하는 핵심 원리입니다[1][3][5].</p>
<p>출처
[1] Bias-Variance trade-off - 편향-분산 트레이드 <a href="https://velog.io/@lolhi/Bias-Variance-trade-off">https://velog.io/@lolhi/Bias-Variance-trade-off</a>
[2] 편향-분산 트레이드오프 (Bias-Variance Tradeoff)와 L2 규제 ... <a href="https://untitledtblog.tistory.com/143">https://untitledtblog.tistory.com/143</a>
[3] 편향-분산 트레이드오프 <a href="https://ko.wikipedia.org/wiki/%ED%8E%B8%ED%96%A5-%EB%B6%84%EC%82%B0_%ED%8A%B8%EB%A0%88%EC%9D%B4%EB%93%9C%EC%98%A4%ED%94%84">https://ko.wikipedia.org/wiki/%ED%8E%B8%ED%96%A5-%EB%B6%84%EC%82%B0_%ED%8A%B8%EB%A0%88%EC%9D%B4%EB%93%9C%EC%98%A4%ED%94%84</a>
[4] 쉽게 이해해보는 bias-variance tradeoff - 건빵의 블로그 <a href="https://bywords.tistory.com/entry/%EB%B2%88%EC%97%AD-%EC%9C%A0%EC%B9%98%EC%9B%90%EC%83%9D%EB%8F%84-%EC%9D%B4%ED%95%B4%ED%95%A0-%EC%88%98-%EC%9E%88%EB%8A%94-biasvariance-tradeoff">https://bywords.tistory.com/entry/%EB%B2%88%EC%97%AD-%EC%9C%A0%EC%B9%98%EC%9B%90%EC%83%9D%EB%8F%84-%EC%9D%B4%ED%95%B4%ED%95%A0-%EC%88%98-%EC%9E%88%EB%8A%94-biasvariance-tradeoff</a>
[5] 머신러닝 - Bias and Variance trade-off - 태호의 개발 블로그 <a href="https://dailytaeho.tistory.com/79">https://dailytaeho.tistory.com/79</a>
[6] 편향-분산 상충관계 - 위키피디아 <a href="https://translate.google.com/translate?u=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FBias%25E2%2580%2593variance_tradeoff&amp;sl=en&amp;tl=ko&amp;client=srp">https://translate.google.com/translate?u=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FBias%25E2%2580%2593variance_tradeoff&amp;sl=en&amp;tl=ko&amp;client=srp</a>
[7] [인사이드 머신러닝] Bias-Variance Trade-Off <a href="https://velog.io/@cleansky/%EC%9D%B8%EC%82%AC%EC%9D%B4%EB%93%9C-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-Bias-Variance-Trade-Off">https://velog.io/@cleansky/%EC%9D%B8%EC%82%AC%EC%9D%B4%EB%93%9C-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-Bias-Variance-Trade-Off</a>
[8] Bias-Variance Trade Off 이란? - Attention, Please!!! - 티스토리 <a href="https://g3lu.tistory.com/9">https://g3lu.tistory.com/9</a></p>
<h3 id="overfitting-underfitting">과적합(Overfitting) 및 과소적합(Underfitting)<a class="headerlink" href="#overfitting-underfitting" title="Permanent link">&para;</a></h3>
<ul>
<li>언더피팅(Underfitting)과 오버피팅(Overfitting)은 딥러닝과 머신러닝에서 모델 성능에 영향을 미치는 대표적인 두 가지 학습 문제</li>
<li>모델의 학습 부족과 과잉 학습을 의미하며, 서로 반대 개념</li>
</ul>
<table>
<thead>
<tr>
<th>구분</th>
<th>언더피팅 (Underfitting)</th>
<th>오버피팅 (Overfitting)</th>
</tr>
</thead>
<tbody>
<tr>
<td>🔍 정의</td>
<td>모델이 충분히 학습하지 못해 성능이 낮은 상태<br/>모델이 <strong>너무 단순</strong>해서 훈련 데이터도 제대로 학습 못함</td>
<td>모델이 훈련 데이터에 과하게 맞춰져, 일반화가 안 되는 상태<br/>모델이 <strong>너무 복잡</strong>해서 훈련 데이터에 과하게 맞춤</td>
</tr>
<tr>
<td>🎯 원인</td>
<td>모델이 너무 단순함<br>훈련 부족<br>입력 정보 부족</td>
<td>모델이 너무 복잡함<br>에폭 과다<br>노이즈까지 학습</td>
</tr>
<tr>
<td>📊 특징</td>
<td>훈련 정확도 낮음<br>검증 정확도 낮음</td>
<td>훈련 정확도 매우 높음<br>검증 정확도 낮음</td>
</tr>
<tr>
<td>📈 그래프 형태</td>
<td>훈련/검증 오류 모두 높음 (→ 모델이 무지함)</td>
<td>훈련 오류는 낮지만, 검증 오류는 높음 (→ 모델이 외움)</td>
</tr>
<tr>
<td>🛠 해결책</td>
<td>모델 복잡도 증가<br>더 많이 학습<br>더 많은 특징 사용</td>
<td>정규화 사용 (Dropout, L2 등)<br>단순한 모델 사용<br>조기 종료(Early Stopping)</td>
</tr>
</tbody>
</table>
<h4 id="vs">간단한 예: 고양이 vs 강아지 구분<a class="headerlink" href="#vs" title="Permanent link">&para;</a></h4>
<ul>
<li>언더피팅:</li>
<li>모델이 너무 단순해서 고양이 귀와 강아지 귀의 차이를 구분 못함 </li>
<li>모두 그냥 "동물"이라고 예측 </li>
<li>오버피팅:</li>
<li>모델이 훈련 데이터에 나온 특정 고양이 사진만 기억 </li>
<li>새로운 고양이 사진이 나오면 예측 실패</li>
</ul>
<h4 id="learning-curve">확인 방법: 학습 곡선(Learning Curve)<a class="headerlink" href="#learning-curve" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>항목</th>
<th>훈련 정확도</th>
<th>검증 정확도</th>
</tr>
</thead>
<tbody>
<tr>
<td>언더피팅</td>
<td>낮음</td>
<td>낮음</td>
</tr>
<tr>
<td>적절 학습</td>
<td>높음</td>
<td>높음</td>
</tr>
<tr>
<td>오버피팅</td>
<td>매우 높음</td>
<td>낮음 (떨어짐)</td>
</tr>
</tbody>
</table>
<ul>
<li>참고</li>
<li><a href="https://kh-kim.github.io/nlp_with_deep_learning_blog/docs/1-12-how-to-prevent-overfitting/02-overfitting/">https://kh-kim.github.io/nlp_with_deep_learning_blog/docs/1-12-how-to-prevent-overfitting/02-overfitting/</a></li>
</ul>
<h3 id="_47">평가 지표의 해석 한계<a class="headerlink" href="#_47" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>단일 지표의 한계:</strong> 정확도, 정밀도, 재현율, F1 점수 등 각 평가지표는 특정 문제 유형이나 목표에 따라 강점과 약점이 존재합니다. 예를 들어, 정확도는 클래스가 불균형일 때 모델 성능을 과대평가할 수 있습니다. 정밀도와 재현율이 높다고 해도 현실에서는 한쪽만 높을 수 있고, F1 점수 역시 실제 응용 상황이나 비즈니스 목표를 반영하지 못할 수 있습니다[1][2].</li>
<li><strong>다양성 미반영:</strong> 기존 평가지표는 데이터 내의 서브그룹별 성능을 잘 포착하지 못하며, 신뢰도(Confidence), 불확실성(Entropy) 등 미묘한 요소를 간접적으로만 반영합니다.</li>
<li><strong>주관성 및 적용 제한:</strong> 생성형 LLM, 이미지 생성 등에서는 정답이 명확하지 않아서 BLEU, FID, 정확도 등 기존 지표의 단순한 수치만으로 평가가 어려우며, 평가자 개인의 판단, 맥락, 상황에 따라 결과가 달라질 수 있습니다[3][4][5].</li>
<li><strong>모델 블랙박스 문제:</strong> 성능 지표는 모델의 결정 과정을 완전히 설명하지 못하며, 특히 복잡한 딥러닝 모델의 경우 예측 근거를 해석하기 어려운 한계가 있습니다[6][7].</li>
</ul>
<hr />
<h3 id="_48">사회적·윤리적 영향 고려<a class="headerlink" href="#_48" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>공정성과 편향:</strong> AI 시스템은 데이터에 내재된 편견을 재생산하거나 강화할 수 있으므로, 사회적 공정성(공평 배분, 차별 방지) 확보가 필수적입니다. 데이터셋의 다양성, 알고리즘의 투명성, 설명가능성(Explainability) 등도 중요한 윤리적 고려 사항입니다[8][7][9].</li>
<li><strong>개인정보 보호:</strong> 모델 학습 및 운영 과정에서 사용자의 개인 정보를 안전하게 보호하고, 관련 법/규정을 준수해야 합니다. GDPR 등 강화된 글로벌 기준이 점차 확대되고 있습니다.</li>
<li><strong>책임성과 투명성:</strong> AI 결정 과정의 투명성 확보와 그 결과에 대한 책임 부여가 중요합니다. 이는 신뢰성 확보와 함께, AI의 사회적 수용도를 높이는 데 필요합니다.</li>
<li><strong>사회적 가치 및 영향:</strong> 자동화로 인한 고용 변화, 신뢰성/안전성 문제, 불평등 심화 등 다양한 사회적 영향이 존재하며, 이에 대한 정책적 대응이나 교육, 기술 개발 방향의 설정이 필수적입니다.</li>
</ul>
<hr />
<p>결론적으로, 모델 평가 지표의 해석은 항상 그 한계를 인식하고, 다양한 사회적·윤리적 시각을 함께 반영하여, AI 기술이 더 안전하고 신뢰할 수 있으며 포용적인 방향으로 발전할 수 있도록 하는 것이 매우 중요합니다.</p>
<p>출처
[1] 머신러닝 성능 지표란? | 퓨어스토리지 - Pure Storage <a href="https://www.purestorage.com/kr/knowledge/machine-learning-performance-metrics.html">https://www.purestorage.com/kr/knowledge/machine-learning-performance-metrics.html</a>
[2] 머신러닝 모델 평가 방법 (예: 정확도, F1 스코어) - LearnCodeEasy <a href="https://thebasics.tistory.com/105">https://thebasics.tistory.com/105</a>
[3] LLM 성능평가를 위한 지표들 - 슈퍼브 블로그 - Superb AI <a href="https://blog-ko.superb-ai.com/llm-evaluation-metrics/">https://blog-ko.superb-ai.com/llm-evaluation-metrics/</a>
[4] AI로 생성한 이미지는 어떻게 평가할까요? (기본편) <a href="https://techblog.lycorp.co.jp/ko/how-to-evaluate-ai-generated-images-1">https://techblog.lycorp.co.jp/ko/how-to-evaluate-ai-generated-images-1</a>
[5] [LLM Evaluation] LLM 성능 평가 방법 - 가디의 tech 스터디 <a href="https://gagadi.tistory.com/58">https://gagadi.tistory.com/58</a>
[6] AI 기반 분석 프로젝트는 왜 실패하는가? AI 분석모델에 대한 오해와 진실 <a href="https://www.samsungsds.com/kr/insights/ai_analytics_model.html">https://www.samsungsds.com/kr/insights/ai_analytics_model.html</a>
[7] AI 윤리와 사회적 영향의 현주소 <a href="https://seo.goover.ai/report/202412/go-public-report-ko-63153ce6-da9e-40b5-ba98-e5876f4d696e-0-0.html">https://seo.goover.ai/report/202412/go-public-report-ko-63153ce6-da9e-40b5-ba98-e5876f4d696e-0-0.html</a>
[8] 07 인공지능의 윤리와 사회적 영향 <a href="https://wikidocs.net/240300">https://wikidocs.net/240300</a>
[9] AI 시대의 윤리적 책임: 지속 가능한 발전을 위한 접근 방법 <a href="https://seo.goover.ai/report/202503/go-public-report-ko-f8a5db8a-7ffb-4ae8-ab1b-3f7744c19812-0-0.html">https://seo.goover.ai/report/202503/go-public-report-ko-f8a5db8a-7ffb-4ae8-ab1b-3f7744c19812-0-0.html</a>
[10] LLM 성능, 어떻게 평가하는 것일까? (feat. lm-eval-harness) - DevOcean <a href="https://devocean.sk.com/blog/techBoardDetail.do?ID=166716&amp;boardType=techBlog">https://devocean.sk.com/blog/techBoardDetail.do?ID=166716&amp;boardType=techBlog</a></p>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "..", "features": ["navigation.instant", "navigation.tracking", "search.suggest", "search.highlight", "content.code.copy"], "search": "../assets/javascripts/workers/search.973d3a69.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../assets/javascripts/bundle.92b07e13.min.js"></script>
      
        <script src="../javascripts/katex.js"></script>
      
        <script src="https://unpkg.com/katex@0/dist/katex.min.js"></script>
      
        <script src="https://unpkg.com/katex@0/dist/contrib/auto-render.min.js"></script>
      
    
  </body>
</html>